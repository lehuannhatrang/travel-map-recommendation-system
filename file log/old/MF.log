
################################################################################
user-based = 1 ( ~ user-based)
################################################################################

K = 10, lam = .1, learning_rate = 0.75, max_iter = 500, user_based = 1)

iter = 10 , loss = 11.035673753706833 , RMSE train = 1.31341077071076
iter = 20 , loss = 5.37314338840249 , RMSE train = 1.1911622475347006
iter = 30 , loss = 2.8428618012365434 , RMSE train = 1.186774244160729
iter = 40 , loss = 1.6851096813319186 , RMSE train = 1.1868934114059204
iter = 50 , loss = 1.1542229610836914 , RMSE train = 1.1869799860939898
iter = 60 , loss = 0.9107464968436222 , RMSE train = 1.1870042322967942
iter = 70 , loss = 0.7990846654339439 , RMSE train = 1.1870103081600836
iter = 80 , loss = 0.7478759484311706 , RMSE train = 1.1870117814820431
iter = 90 , loss = 0.7243916122586725 , RMSE train = 1.187012132775251
iter = 100 , loss = 0.7136217374662518 , RMSE train = 1.1870122155659981
iter = 110 , loss = 0.7086826969591864 , RMSE train = 1.1870122349029002
iter = 120 , loss = 0.7064176616150156 , RMSE train = 1.1870122393867562
iter = 130 , loss = 0.7053789180686472 , RMSE train = 1.1870122404203265
iter = 140 , loss = 0.7049025496742313 , RMSE train = 1.1870122406574215
iter = 150 , loss = 0.7046840861367905 , RMSE train = 1.1870122407115744
iter = 160 , loss = 0.7045838979483012 , RMSE train = 1.1870122407239052
iter = 170 , loss = 0.7045379511005588 , RMSE train = 1.1870122407267143
iter = 180 , loss = 0.704516879543067 , RMSE train = 1.187012240727338
iter = 190 , loss = 0.7045072159339548 , RMSE train = 1.187012240727461
iter = 200 , loss = 0.7045027840951401 , RMSE train = 1.1870122407274768
iter = 210 , loss = 0.7045007515952574 , RMSE train = 1.1870122407274908
iter = 220 , loss = 0.7044998194594788 , RMSE train = 1.187012240727492
iter = 230 , loss = 0.7044993919655034 , RMSE train = 1.1870122407274923
iter = 240 , loss = 0.704499195908163 , RMSE train = 1.1870122407274923
iter = 250 , loss = 0.7044991059918226 , RMSE train = 1.1870122407274923
iter = 260 , loss = 0.7044990647539097 , RMSE train = 1.1870122407274923
iter = 270 , loss = 0.7044990458410406 , RMSE train = 1.1870122407274923
iter = 280 , loss = 0.7044990371670091 , RMSE train = 1.1870122407274923
iter = 290 , loss = 0.7044990331888005 , RMSE train = 1.1870122407274923
iter = 300 , loss = 0.7044990313642439 , RMSE train = 1.1870122407274923
iter = 310 , loss = 0.7044990305274269 , RMSE train = 1.1870122407274923
iter = 320 , loss = 0.7044990301436249 , RMSE train = 1.1870122407274923
iter = 330 , loss = 0.7044990299675945 , RMSE train = 1.1870122407274923
iter = 340 , loss = 0.7044990298868575 , RMSE train = 1.1870122407274923
iter = 350 , loss = 0.7044990298498268 , RMSE train = 1.1870122407274923
iter = 360 , loss = 0.7044990298328423 , RMSE train = 1.1870122407274923
iter = 370 , loss = 0.704499029825052 , RMSE train = 1.1870122407274923
iter = 380 , loss = 0.7044990298214788 , RMSE train = 1.1870122407274923
iter = 390 , loss = 0.7044990298198398 , RMSE train = 1.1870122407274923
iter = 400 , loss = 0.7044990298190881 , RMSE train = 1.1870122407274923
iter = 410 , loss = 0.7044990298187432 , RMSE train = 1.1870122407274923
iter = 420 , loss = 0.704499029818585 , RMSE train = 1.1870122407274923
iter = 430 , loss = 0.7044990298185125 , RMSE train = 1.1870122407274923
iter = 440 , loss = 0.7044990298184792 , RMSE train = 1.1870122407274923
iter = 450 , loss = 0.704499029818464 , RMSE train = 1.1870122407274923
iter = 460 , loss = 0.7044990298184569 , RMSE train = 1.1870122407274923
iter = 470 , loss = 0.7044990298184538 , RMSE train = 1.1870122407274923
iter = 480 , loss = 0.7044990298184522 , RMSE train = 1.1870122407274923
iter = 490 , loss = 0.7044990298184516 , RMSE train = 1.1870122407274923
iter = 500 , loss = 0.7044990298184513 , RMSE train = 1.1870122407274923

User-based MF, RMSE = 1.2309334491382473

################################################################################

K = 10, lam = .1, learning_rate = 0.1, max_iter = 500, user_based = 1)

iter = 10 , loss = 23.864623168807064 , RMSE train = 2.442035728975284
iter = 20 , loss = 20.796605778246484 , RMSE train = 2.1504851840064956
iter = 30 , loss = 18.354342160057666 , RMSE train = 1.9080397503428657
iter = 40 , loss = 16.340776408728082 , RMSE train = 1.7122280452687344
iter = 50 , loss = 14.63830289493886 , RMSE train = 1.561628752228466
iter = 60 , loss = 13.172391724585951 , RMSE train = 1.449216513809776
iter = 70 , loss = 11.893366354435088 , RMSE train = 1.3679007206781504
iter = 80 , loss = 10.76661060230512 , RMSE train = 1.3103538893162034
iter = 90 , loss = 9.767011958510786 , RMSE train = 1.270616629602805
iter = 100 , loss = 8.875671305515834 , RMSE train = 1.2434593835716228
iter = 110 , loss = 8.077886600310064 , RMSE train = 1.2250000711221039
iter = 120 , loss = 7.3618812595216285 , RMSE train = 1.2125332295393674
iter = 130 , loss = 6.717981708997531 , RMSE train = 1.20413557378355
iter = 140 , loss = 6.138072835021899 , RMSE train = 1.1984844335753566
iter = 150 , loss = 5.6152290805214715 , RMSE train = 1.1946823107735796
iter = 160 , loss = 5.143458635476277 , RMSE train = 1.1921262788056513
iter = 170 , loss = 4.7175216926597585 , RMSE train = 1.1904090666659886
iter = 180 , loss = 4.332798009849152 , RMSE train = 1.1892560750680412
iter = 190 , loss = 3.985187847095145 , RMSE train = 1.188482726906743
iter = 200 , loss = 3.6710358966416368 , RMSE train = 1.1879649318846535
iter = 210 , loss = 3.387071358209096 , RMSE train = 1.1876191911996574
iter = 220 , loss = 3.1303595902409467 , RMSE train = 1.1873892887016648
iter = 230 , loss = 2.898262250145179 , RMSE train = 1.1872373619109104
iter = 240 , loss = 2.6884038100627414 , RMSE train = 1.1871378624406683
iter = 250 , loss = 2.498642979497938 , RMSE train = 1.1870735443640863
iter = 260 , loss = 2.327047996934744 , RMSE train = 1.187032764677385
iter = 270 , loss = 2.1718750430053437 , RMSE train = 1.1870076617656053
iter = 280 , loss = 2.031549225579881 , RMSE train = 1.1869929289832455
iter = 290 , loss = 1.9046477234112642 , RMSE train = 1.1869849878390402
iter = 300 , loss = 1.7898847700985057 , RMSE train = 1.1869814298124965
iter = 310 , loss = 1.6860982275701124 , RMSE train = 1.1869806389869806
iter = 320 , loss = 1.5922375469523895 , RMSE train = 1.1869815365612102
iter = 330 , loss = 1.5073529505016472 , RMSE train = 1.1869834076346595
iter = 340 , loss = 1.4305856952127667 , RMSE train = 1.1869857836191613
iter = 350 , loss = 1.361159299435933 , RMSE train = 1.1869883623221884
iter = 360 , loss = 1.298371630139386 , RMSE train = 1.1869909535853884
iter = 370 , loss = 1.2415877615765927 , RMSE train = 1.1869934422884945
iter = 380 , loss = 1.1902335268931274 , RMSE train = 1.1869957631734636
iter = 390 , loss = 1.1437896932228944 , RMSE train = 1.1869978837267194
iter = 400 , loss = 1.1017866984862277 , RMSE train = 1.1869997925631162
iter = 410 , loss = 1.063799894700372 , RMSE train = 1.187001491569834
iter = 420 , loss = 1.0294452483595473 , RMSE train = 1.187002990622368
iter = 430 , loss = 0.9983754534868701 , RMSE train = 1.1870043040598022
iter = 440 , loss = 0.9702764174225693 , RMSE train = 1.187005448362508
iter = 450 , loss = 0.9448640833800485 , RMSE train = 1.1870064406505685
iter = 460 , loss = 0.9218815573413153 , RMSE train = 1.1870072977400996
iter = 470 , loss = 0.9010965100340222 , RMSE train = 1.1870080355769155
iter = 480 , loss = 0.8822988275775516 , RMSE train = 1.1870086689230306
iter = 490 , loss = 0.8652984869446949 , RMSE train = 1.1870092112106638
iter = 500 , loss = 0.8499236346890869 , RMSE train = 1.1870096745050538

User-based MF, RMSE = 1.2309316198391613

################################################################################

K = 20, lam = .1, learning_rate = 0.75, max_iter = 500, user_based = 1)

iter = 10 , loss = 15.399726295604914 , RMSE train = 1.4242360573153654
iter = 20 , loss = 7.304805526959858 , RMSE train = 1.1935973925092873
iter = 30 , loss = 3.7265638316778147 , RMSE train = 1.186116458751093
iter = 40 , loss = 2.0904722847681687 , RMSE train = 1.186678034557869
iter = 50 , loss = 1.340174530699298 , RMSE train = 1.1869258318601348
iter = 60 , loss = 0.9960381618955936 , RMSE train = 1.1869912772287252
iter = 70 , loss = 0.8382031257532545 , RMSE train = 1.187007260280495
iter = 80 , loss = 0.7658166987173062 , RMSE train = 1.1870110710184383
iter = 90 , loss = 0.732619567739921 , RMSE train = 1.1870119682706837
iter = 100 , loss = 0.7173952039308864 , RMSE train = 1.1870121776778937
iter = 110 , loss = 0.7104132655362169 , RMSE train = 1.1870122262145748
iter = 120 , loss = 0.707211328478812 , RMSE train = 1.1870122374015515
iter = 130 , loss = 0.7057429081546147 , RMSE train = 1.1870122399681018
iter = 140 , loss = 0.7050694830532935 , RMSE train = 1.1870122405546542
iter = 150 , loss = 0.704760645698932 , RMSE train = 1.1870122406882704
iter = 160 , loss = 0.7046190101902348 , RMSE train = 1.187012240718632
iter = 170 , loss = 0.7045540546181379 , RMSE train = 1.1870122407255224
iter = 180 , loss = 0.7045242651504854 , RMSE train = 1.1870122407270556
iter = 190 , loss = 0.7045106032457636 , RMSE train = 1.1870122407273984
iter = 200 , loss = 0.7045043376544794 , RMSE train = 1.1870122407274657
iter = 210 , loss = 0.7045014641274101 , RMSE train = 1.1870122407274857
iter = 220 , loss = 0.7045001462619308 , RMSE train = 1.1870122407274912
iter = 230 , loss = 0.7044995418548076 , RMSE train = 1.1870122407274921
iter = 240 , loss = 0.704499264656277 , RMSE train = 1.1870122407274923
iter = 250 , loss = 0.7044991375241535 , RMSE train = 1.1870122407274923
iter = 260 , loss = 0.7044990792168591 , RMSE train = 1.1870122407274923
iter = 270 , loss = 0.7044990524748587 , RMSE train = 1.1870122407274923
iter = 280 , loss = 0.7044990402098315 , RMSE train = 1.1870122407274923
iter = 290 , loss = 0.7044990345845143 , RMSE train = 1.1870122407274923
iter = 300 , loss = 0.7044990320044554 , RMSE train = 1.1870122407274923
iter = 310 , loss = 0.7044990308210961 , RMSE train = 1.1870122407274923
iter = 320 , loss = 0.7044990302783355 , RMSE train = 1.1870122407274923
iter = 330 , loss = 0.7044990300293896 , RMSE train = 1.1870122407274923
iter = 340 , loss = 0.7044990299152052 , RMSE train = 1.1870122407274923
iter = 350 , loss = 0.7044990298628312 , RMSE train = 1.1870122407274923
iter = 360 , loss = 0.7044990298388082 , RMSE train = 1.1870122407274923
iter = 370 , loss = 0.704499029827789 , RMSE train = 1.1870122407274923
iter = 380 , loss = 0.7044990298227345 , RMSE train = 1.1870122407274923
iter = 390 , loss = 0.7044990298204159 , RMSE train = 1.1870122407274923
iter = 400 , loss = 0.7044990298193524 , RMSE train = 1.1870122407274923
iter = 410 , loss = 0.7044990298188645 , RMSE train = 1.1870122407274923
iter = 420 , loss = 0.7044990298186408 , RMSE train = 1.1870122407274923
iter = 430 , loss = 0.7044990298185381 , RMSE train = 1.1870122407274923
iter = 440 , loss = 0.704499029818491 , RMSE train = 1.1870122407274923
iter = 450 , loss = 0.7044990298184693 , RMSE train = 1.1870122407274923
iter = 460 , loss = 0.7044990298184595 , RMSE train = 1.1870122407274923
iter = 470 , loss = 0.7044990298184549 , RMSE train = 1.1870122407274923
iter = 480 , loss = 0.7044990298184528 , RMSE train = 1.1870122407274923
iter = 490 , loss = 0.7044990298184518 , RMSE train = 1.1870122407274923
iter = 500 , loss = 0.7044990298184514 , RMSE train = 1.1870122407274923

User-based MF, RMSE = 1.2309334491382473

################################################################################

K = 20, lam = .1, learning_rate = 0.1, max_iter = 500, user_based = 1)

iter = 10 , loss = 35.109327007557724 , RMSE train = 2.943147911084127
iter = 20 , loss = 30.169764906496145 , RMSE train = 2.588434064155943
iter = 30 , loss = 26.340259520770687 , RMSE train = 2.2751501049439473
iter = 40 , loss = 23.25433047632366 , RMSE train = 2.0143401175879787
iter = 50 , loss = 20.693967753756542 , RMSE train = 1.8027547657379428
iter = 60 , loss = 18.522335144549015 , RMSE train = 1.6352333843200009
iter = 70 , loss = 16.64964533420692 , RMSE train = 1.5046330603158142
iter = 80 , loss = 15.01461896711668 , RMSE train = 1.4071806787608478
iter = 90 , loss = 13.573878775785717 , RMSE train = 1.3368013249608848
iter = 100 , loss = 12.295639589781562 , RMSE train = 1.2874440868536694
iter = 110 , loss = 11.155836139149761 , RMSE train = 1.2536975031621165
iter = 120 , loss = 10.135685695417354 , RMSE train = 1.230911661250589
iter = 130 , loss = 9.220119867447979 , RMSE train = 1.215662091730417
iter = 140 , loss = 8.39675519776177 , RMSE train = 1.2055236523168904
iter = 150 , loss = 7.655204206887324 , RMSE train = 1.198812929788705
iter = 160 , loss = 6.98660510328957 , RMSE train = 1.1944032958646822
iter = 170 , loss = 6.383294017460513 , RMSE train = 1.1915269654791505
iter = 180 , loss = 5.838571432693766 , RMSE train = 1.1896671779704153
iter = 190 , loss = 5.346531744097809 , RMSE train = 1.1884781852641464
iter = 200 , loss = 4.901935747211856 , RMSE train = 1.1877302109823662
iter = 210 , loss = 4.500112788424107 , RMSE train = 1.187270278997451
iter = 220 , loss = 4.136883774880146 , RMSE train = 1.1869969641152092
iter = 230 , loss = 3.808499144593405 , RMSE train = 1.1868433110444012
iter = 240 , loss = 3.511587799875177 , RMSE train = 1.1867652901854921
iter = 250 , loss = 3.2431142631636924 , RMSE train = 1.1867340844021448
iter = 260 , loss = 3.000342149534528 , RMSE train = 1.186730966671858
iter = 270 , loss = 2.780802609734536 , RMSE train = 1.1867439237518238
iter = 280 , loss = 2.582266775505903 , RMSE train = 1.186765308349313
iter = 290 , loss = 2.4027214965592214 , RMSE train = 1.186790354423424
iter = 300 , loss = 2.2403478359945774 , RMSE train = 1.1868161727135933
iter = 310 , loss = 2.0935019147355742 , RMSE train = 1.1868410857100504
iter = 320 , loss = 1.9606977831375834 , RMSE train = 1.1868641841583236
iter = 330 , loss = 1.8405920609974795 , RMSE train = 1.186885045215495
iter = 340 , loss = 1.7319701334788014 , RMSE train = 1.1869035571345725
iter = 350 , loss = 1.6337337251699708 , RMSE train = 1.1869197708219217
iter = 360 , loss = 1.5448897011045482 , RMSE train = 1.1869338340323212
iter = 370 , loss = 1.4645399644553914 , RMSE train = 1.1869459419751336
iter = 380 , loss = 1.391872337370148 , RMSE train = 1.186956306819963
iter = 390 , loss = 1.3261523251493093 , RMSE train = 1.1869651394902943
iter = 400 , loss = 1.266715675434411 , RMSE train = 1.1869726393672497
iter = 410 , loss = 1.2129616538095618 , RMSE train = 1.1869789890160758
iter = 420 , loss = 1.1643469655964562 , RMSE train = 1.1869843520393102
iter = 430 , loss = 1.1203802609144955 , RMSE train = 1.1869888728200404
iter = 440 , loss = 1.0806171664799127 , RMSE train = 1.1869926773554866
iter = 450 , loss = 1.04465579328062 , RMSE train = 1.1869958746703761
iter = 460 , loss = 1.0121326742988177 , RMSE train = 1.1869985584899068
iter = 470 , loss = 0.9827190909490295 , RMSE train = 1.1870008089759698
iter = 480 , loss = 0.9561177509282206 , RMSE train = 1.187002694412008
iter = 490 , loss = 0.9320597837908262 , RMSE train = 1.1870042727730228
iter = 500 , loss = 0.910302023816074 , RMSE train = 1.1870055931504646

User-based MF, RMSE = 1.230928286935163

################################################################################

K = 50, lam = .1, learning_rate = 0.75, max_iter = 500, user_based = 1)

iter = 10 , loss = 24.228040255795666 , RMSE train = 1.7085814464545146
iter = 20 , loss = 11.148818290112757 , RMSE train = 1.2113240492450341
iter = 30 , loss = 5.480246443980696 , RMSE train = 1.186744787767033
iter = 40 , loss = 2.8942493255078787 , RMSE train = 1.1866126714013419
iter = 50 , loss = 1.7087639032715962 , RMSE train = 1.186890736904207
iter = 60 , loss = 1.1650722480570543 , RMSE train = 1.1869802718333533
iter = 70 , loss = 0.9157215281411635 , RMSE train = 1.1870042147253106
iter = 80 , loss = 0.8013660711943056 , RMSE train = 1.1870102762770434
iter = 90 , loss = 0.7489221615345076 , RMSE train = 1.1870117681884589
iter = 100 , loss = 0.7248713950839745 , RMSE train = 1.1870121285283681
iter = 110 , loss = 0.7138417639156089 , RMSE train = 1.187012214355277
iter = 120 , loss = 0.7087836015038775 , RMSE train = 1.187012234578525
iter = 130 , loss = 0.7064639372742676 , RMSE train = 1.187012239303137
iter = 140 , loss = 0.7054001407898634 , RMSE train = 1.187012240399328
iter = 150 , loss = 0.7049122828963889 , RMSE train = 1.187012240652239
iter = 160 , loss = 0.7046885500925874 , RMSE train = 1.1870122407103125
iter = 170 , loss = 0.7045859452951605 , RMSE train = 1.1870122407236108
iter = 180 , loss = 0.704538890114203 , RMSE train = 1.187012240726624
iter = 190 , loss = 0.7045173102303495 , RMSE train = 1.187012240727297
iter = 200 , loss = 0.7045074134774268 , RMSE train = 1.1870122407274408
iter = 210 , loss = 0.7045028747048563 , RMSE train = 1.187012240727481
iter = 220 , loss = 0.7045007931575122 , RMSE train = 1.1870122407274895
iter = 230 , loss = 0.7044998385244846 , RMSE train = 1.1870122407274917
iter = 240 , loss = 0.7044994007110997 , RMSE train = 1.1870122407274923
iter = 250 , loss = 0.704499199920135 , RMSE train = 1.1870122407274923
iter = 260 , loss = 0.7044991078323565 , RMSE train = 1.1870122407274923
iter = 270 , loss = 0.7044990655983107 , RMSE train = 1.1870122407274923
iter = 280 , loss = 0.7044990462284538 , RMSE train = 1.1870122407274923
iter = 290 , loss = 0.7044990373447646 , RMSE train = 1.1870122407274923
iter = 300 , loss = 0.704499033270364 , RMSE train = 1.1870122407274923
iter = 310 , loss = 0.7044990314016718 , RMSE train = 1.1870122407274923
iter = 320 , loss = 0.704499030544603 , RMSE train = 1.1870122407274923
iter = 330 , loss = 0.7044990301515078 , RMSE train = 1.1870122407274923
iter = 340 , loss = 0.7044990299712126 , RMSE train = 1.1870122407274923
iter = 350 , loss = 0.7044990298885183 , RMSE train = 1.1870122407274923
iter = 360 , loss = 0.7044990298505893 , RMSE train = 1.1870122407274923
iter = 370 , loss = 0.7044990298331923 , RMSE train = 1.1870122407274923
iter = 380 , loss = 0.7044990298252127 , RMSE train = 1.1870122407274923
iter = 390 , loss = 0.7044990298215527 , RMSE train = 1.1870122407274923
iter = 400 , loss = 0.7044990298198738 , RMSE train = 1.1870122407274923
iter = 410 , loss = 0.7044990298191036 , RMSE train = 1.1870122407274923
iter = 420 , loss = 0.7044990298187505 , RMSE train = 1.1870122407274923
iter = 430 , loss = 0.7044990298185884 , RMSE train = 1.1870122407274923
iter = 440 , loss = 0.704499029818514 , RMSE train = 1.1870122407274923
iter = 450 , loss = 0.7044990298184799 , RMSE train = 1.1870122407274923
iter = 460 , loss = 0.7044990298184642 , RMSE train = 1.1870122407274923
iter = 470 , loss = 0.7044990298184571 , RMSE train = 1.1870122407274923
iter = 480 , loss = 0.7044990298184538 , RMSE train = 1.1870122407274923
iter = 490 , loss = 0.7044990298184524 , RMSE train = 1.1870122407274923
iter = 500 , loss = 0.7044990298184516 , RMSE train = 1.1870122407274923

User-based MF, RMSE = 1.2309334491382473

################################################################################

K = 50, lam = .1, learning_rate = 0.1, max_iter = 500, user_based = 1)

iter = 10 , loss = 60.36460298204469 , RMSE train = 3.6170273138039315
iter = 20 , loss = 50.52315047142472 , RMSE train = 3.2722250375344735
iter = 30 , loss = 43.266003076687625 , RMSE train = 2.9204947284702674
iter = 40 , loss = 37.649492539681916 , RMSE train = 2.586187003713931
iter = 50 , loss = 33.138935613089785 , RMSE train = 2.2876176319428154
iter = 60 , loss = 29.411104550222362 , RMSE train = 2.032994773753899
iter = 70 , loss = 26.261003251087182 , RMSE train = 1.8202257365346504
iter = 80 , loss = 23.55337281310221 , RMSE train = 1.6488370764389213
iter = 90 , loss = 21.195748856852532 , RMSE train = 1.5139752875215355
iter = 100 , loss = 19.122772006755298 , RMSE train = 1.412494032086434
iter = 110 , loss = 17.286724722505816 , RMSE train = 1.3393221146609984
iter = 120 , loss = 15.651664788810624 , RMSE train = 1.2883329800454413
iter = 130 , loss = 14.189705737554224 , RMSE train = 1.2534913658647158
iter = 140 , loss = 12.878612077834028 , RMSE train = 1.2299975848725586
iter = 150 , loss = 11.70021642864458 , RMSE train = 1.21433584769051
iter = 160 , loss = 10.63935922791279 , RMSE train = 1.2039942662067118
iter = 170 , loss = 9.683165587221264 , RMSE train = 1.1972345345265063
iter = 180 , loss = 8.82054254182208 , RMSE train = 1.1928642818586155
iter = 190 , loss = 8.0418221927515 , RMSE train = 1.1900788960505917
iter = 200 , loss = 7.338502650136434 , RMSE train = 1.1883381287734265
iter = 210 , loss = 6.703055420817226 , RMSE train = 1.18728015358564
iter = 220 , loss = 6.128778603671542 , RMSE train = 1.186664189412967
iter = 230 , loss = 5.609682188262242 , RMSE train = 1.1863311278666633
iter = 240 , loss = 5.140396271552008 , RMSE train = 1.1861760618220023
iter = 250 , loss = 4.716095974793899 , RMSE train = 1.1861302281595687
iter = 260 , loss = 4.332438804294836 , RMSE train = 1.1861489667655787
iter = 270 , loss = 3.9855115050322265 , RMSE train = 1.186203574846858
iter = 280 , loss = 3.6717843306043276 , RMSE train = 1.1862759001366672
iter = 290 , loss = 3.3880712432386257 , RMSE train = 1.1863546972499786
iter = 300 , loss = 3.1314949593325108 , RMSE train = 1.1864332162471143
iter = 310 , loss = 2.899456032144994 , RMSE train = 1.1865076133953023
iter = 320 , loss = 2.6896053552825134 , RMSE train = 1.1865758965636277
iter = 330 , loss = 2.4998196059957154 , RMSE train = 1.1866372315016631
iter = 340 , loss = 2.3281792442867975 , RMSE train = 1.1866914881531896
iter = 350 , loss = 2.172948754596552 , RMSE train = 1.1867389458332454
iter = 360 , loss = 2.0325588695521035 , RMSE train = 1.1867801049486684
iter = 370 , loss = 1.9055905553937134 , RMSE train = 1.1868155679770704
iter = 380 , loss = 1.7907605699642506 , RMSE train = 1.186845966145408
iter = 390 , loss = 1.6869084290570093 , RMSE train = 1.1868719159154812
iter = 400 , loss = 1.5929846371929104 , RMSE train = 1.1868939946991082
iter = 410 , loss = 1.5080400557299478 , RMSE train = 1.1869127288051422
iter = 420 , loss = 1.4312162954162662 , RMSE train = 1.1869285890074162
iter = 430 , loss = 1.3617370326732843 , RMSE train = 1.1869419907159364
iter = 440 , loss = 1.2989001594497822 , RMSE train = 1.186953296792502
iter = 450 , loss = 1.242070685728054 , RMSE train = 1.1869628217545953
iter = 460 , loss = 1.1906743219171543 , RMSE train = 1.186970836577489
iter = 470 , loss = 1.1441916756010764 , RMSE train = 1.1869775736082064
iter = 480 , loss = 1.1021530035628504 , RMSE train = 1.1869832313062427
iter = 490 , loss = 1.064133465776055 , RMSE train = 1.1869879786534903
iter = 500 , loss = 1.0297488332348004 , RMSE train = 1.1869919591586064

User-based MF, RMSE = 1.230922152703888

################################################################################

K = 100, lam = .1, learning_rate = 0.75, max_iter = 1000, user_based = 1)

iter = 10 , loss = 34.32995861719904 , RMSE train = 1.9783210815426246
iter = 20 , loss = 15.498169498831523 , RMSE train = 1.2285590065210086
iter = 30 , loss = 7.463970166815973 , RMSE train = 1.1858072800281234
iter = 40 , loss = 3.8038567105889647 , RMSE train = 1.1860988237015717
iter = 50 , loss = 2.126006579343628 , RMSE train = 1.1867448646830758
iter = 60 , loss = 1.3564510610582365 , RMSE train = 1.1869428257022254
iter = 70 , loss = 1.0034958952561674 , RMSE train = 1.1869949456646034
iter = 80 , loss = 0.8416213083495068 , RMSE train = 1.1870080294182406
iter = 90 , loss = 0.7673836543316266 , RMSE train = 1.1870112315303785
iter = 100 , loss = 0.7333379432781829 , RMSE train = 1.1870120017855321
iter = 110 , loss = 0.7177245564059085 , RMSE train = 1.187012184687575
iter = 120 , loss = 0.7105642652547163 , RMSE train = 1.187012227683565
iter = 130 , loss = 0.7072805582751264 , RMSE train = 1.1870122377100618
iter = 140 , loss = 0.7057746483565531 , RMSE train = 1.1870122400330245
iter = 150 , loss = 0.7050840351304626 , RMSE train = 1.1870122405683503
iter = 160 , loss = 0.7047673174271243 , RMSE train = 1.1870122406911645
iter = 170 , loss = 0.7046220689778255 , RMSE train = 1.1870122407192487
iter = 180 , loss = 0.7045554569717971 , RMSE train = 1.1870122407256465
iter = 190 , loss = 0.7045249080790339 , RMSE train = 1.1870122407270727
iter = 200 , loss = 0.7045108980029934 , RMSE train = 1.1870122407273944
iter = 210 , loss = 0.7045044727878168 , RMSE train = 1.1870122407274657
iter = 220 , loss = 0.7045015260795704 , RMSE train = 1.1870122407274883
iter = 230 , loss = 0.7045001746637329 , RMSE train = 1.1870122407274917
iter = 240 , loss = 0.7044995548753923 , RMSE train = 1.1870122407274923
iter = 250 , loss = 0.7044992706253896 , RMSE train = 1.1870122407274923
iter = 260 , loss = 0.704499140260577 , RMSE train = 1.1870122407274923
iter = 270 , loss = 0.7044990804713012 , RMSE train = 1.1870122407274923
iter = 280 , loss = 0.7044990530499161 , RMSE train = 1.1870122407274923
iter = 290 , loss = 0.704499040473443 , RMSE train = 1.1870122407274923
iter = 300 , loss = 0.704499034705354 , RMSE train = 1.1870122407274923
iter = 310 , loss = 0.7044990320598474 , RMSE train = 1.1870122407274923
iter = 320 , loss = 0.7044990308464868 , RMSE train = 1.1870122407274923
iter = 330 , loss = 0.704499030289974 , RMSE train = 1.1870122407274923
iter = 340 , loss = 0.7044990300347242 , RMSE train = 1.1870122407274923
iter = 350 , loss = 0.7044990299176502 , RMSE train = 1.1870122407274923
iter = 360 , loss = 0.704499029863952 , RMSE train = 1.1870122407274923
iter = 370 , loss = 0.7044990298393218 , RMSE train = 1.1870122407274923
iter = 380 , loss = 0.7044990298280244 , RMSE train = 1.1870122407274923
iter = 390 , loss = 0.7044990298228424 , RMSE train = 1.1870122407274923
iter = 400 , loss = 0.7044990298204654 , RMSE train = 1.1870122407274923
iter = 410 , loss = 0.7044990298193751 , RMSE train = 1.1870122407274923
iter = 420 , loss = 0.7044990298188749 , RMSE train = 1.1870122407274923
iter = 430 , loss = 0.7044990298186454 , RMSE train = 1.1870122407274923
iter = 440 , loss = 0.7044990298185402 , RMSE train = 1.1870122407274923
iter = 450 , loss = 0.704499029818492 , RMSE train = 1.1870122407274923
iter = 460 , loss = 0.7044990298184698 , RMSE train = 1.1870122407274923
iter = 470 , loss = 0.7044990298184597 , RMSE train = 1.1870122407274923
iter = 480 , loss = 0.704499029818455 , RMSE train = 1.1870122407274923
iter = 490 , loss = 0.7044990298184528 , RMSE train = 1.1870122407274923
iter = 500 , loss = 0.7044990298184518 , RMSE train = 1.1870122407274923
iter = 510 , loss = 0.7044990298184514 , RMSE train = 1.1870122407274923
iter = 520 , loss = 0.7044990298184513 , RMSE train = 1.1870122407274923
iter = 530 , loss = 0.7044990298184511 , RMSE train = 1.1870122407274923
iter = 540 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 550 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 560 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 570 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 580 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 590 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 600 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 610 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 620 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 630 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 640 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 650 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 660 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 670 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 680 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 690 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 700 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 710 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 720 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 730 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 740 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 750 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 760 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 770 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 780 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 790 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 800 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 810 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 820 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 830 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 840 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 850 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 860 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 870 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 880 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 890 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 900 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 910 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 920 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 930 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 940 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 950 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 960 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 970 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 980 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 990 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923
iter = 1000 , loss = 0.704499029818451 , RMSE train = 1.1870122407274923

User-based MF, RMSE = 1.2309334491382473

################################################################################

K = 100, lam = .1, learning_rate = 0.1, max_iter = 1000, user_based = 1)

iter = 10 , loss = 94.88410875303305 , RMSE train = 4.049630250175836
iter = 20 , loss = 76.993268061736 , RMSE train = 3.7654230876784927
iter = 30 , loss = 64.49566037477582 , RMSE train = 3.439315835398633
iter = 40 , loss = 55.22555223618508 , RMSE train = 3.100323648420511
iter = 50 , loss = 48.03202571665491 , RMSE train = 2.767610954872995
iter = 60 , loss = 42.24896560890825 , RMSE train = 2.455594516233535
iter = 70 , loss = 37.46835016580468 , RMSE train = 2.174324337179035
iter = 90 , loss = 29.958448176138532 , RMSE train = 1.7403023135110376
iter = 100 , loss = 26.937309952346947 , RMSE train = 1.5845191328393664
iter = 110 , loss = 24.2817172802233 , RMSE train = 1.4646005594749352
iter = 120 , loss = 21.93023349057728 , RMSE train = 1.3761801725227718
iter = 130 , loss = 19.836580294077805 , RMSE train = 1.3132122782361169
iter = 140 , loss = 17.964867148664275 , RMSE train = 1.2694871520717486
iter = 150 , loss = 16.286496530738265 , RMSE train = 1.2397471636010373
iter = 160 , loss = 14.77812204586764 , RMSE train = 1.2199387706066909
iter = 170 , loss = 13.420278467357731 , RMSE train = 1.2069070815149365
iter = 180 , loss = 12.1964465056797 , RMSE train = 1.1984471009904576
iter = 190 , loss = 11.092402364361515 , RMSE train = 1.1930400284197678
iter = 200 , loss = 10.09575609745436 , RMSE train = 1.1896552996539254
iter = 210 , loss = 9.19561666108937 , RMSE train = 1.1875959938961564
iter = 220 , loss = 8.382343087252975 , RMSE train = 1.1863957040250828
iter = 230 , loss = 7.647355040848761 , RMSE train = 1.185744824809854
iter = 240 , loss = 6.982984985017428 , RMSE train = 1.1854392171629116
iter = 250 , loss = 6.382360032998932 , RMSE train = 1.1853459347906457
iter = 260 , loss = 5.839305413267908 , RMSE train = 1.1853782159191277
iter = 270 , loss = 5.34826402125517 , RMSE train = 1.1854803415550834
iter = 280 , loss = 4.904228226766564 , RMSE train = 1.1856170504799488
iter = 290 , loss = 4.502681242902178 , RMSE train = 1.185766512717997
iter = 300 , loss = 4.1395461296906015 , RMSE train = 1.1859156446379369
iter = 310 , loss = 3.811141028002747 , RMSE train = 1.1860570029921862
iter = 320 , loss = 3.51413957827506 , RMSE train = 1.186186732988407
iter = 330 , loss = 3.2455357280676265 , RMSE train = 1.1863032193710445
iter = 340 , loss = 3.002612308249166 , RMSE train = 1.1864062068872707
iter = 350 , loss = 2.7829128833924566 , RMSE train = 1.1864962306536735
iter = 360 , loss = 2.584216473647217 , RMSE train = 1.1865742510068296
iter = 370 , loss = 2.4045148135471206 , RMSE train = 1.1866414236100329
iter = 380 , loss = 2.2419918650501436 , RMSE train = 1.1866989582040015
iter = 390 , loss = 2.09500534243059 , RMSE train = 1.186748035025345
iter = 400 , loss = 1.9620700387138053 , RMSE train = 1.1867897583572367
iter = 410 , loss = 1.8418427694128374 , RMSE train = 1.1868251336442597
iter = 420 , loss = 1.7331087709346713 , RMSE train = 1.1868550592487104
iter = 430 , loss = 1.634769409247809 , RMSE train = 1.1868803270182706
iter = 440 , loss = 1.5458310700004723 , RMSE train = 1.186901627889255
iter = 450 , loss = 1.4653951147940991 , RMSE train = 1.1869195601118825
iter = 460 , loss = 1.392648800140507 , RMSE train = 1.1869346385814465
iter = 470 , loss = 1.3268570660594239 , RMSE train = 1.1869473043492191
iter = 480 , loss = 1.2673551105249339 , RMSE train = 1.1869579337706906
iter = 490 , loss = 1.2135416742166123 , RMSE train = 1.1869668469956678
iter = 500 , loss = 1.1648729674101725 , RMSE train = 1.1869743156618269
iter = 510 , loss = 1.1208571774615443 , RMSE train = 1.18698056975007
iter = 520 , loss = 1.0810495012896908 , RMSE train = 1.1869858036187124
iter = 530 , loss = 1.0450476526205168 , RMSE train = 1.1869901812656232
iter = 540 , loss = 1.012487798584697 , RMSE train = 1.1869938408845941
iter = 550 , loss = 0.9830408846197709 , RMSE train = 1.1869968987886474
iter = 560 , loss = 0.9564093105606973 , RMSE train = 1.18699945277299
iter = 570 , loss = 0.9323239243570448 , RMSE train = 1.187001584987314
iter = 580 , loss = 0.9105413030656767 , RMSE train = 1.18700336438158
iter = 590 , loss = 0.8908412936704219 , RMSE train = 1.1870048487835498
iter = 600 , loss = 0.8730247889035917 , RMSE train = 1.1870060866596097
iter = 610 , loss = 0.8569117156171772 , RMSE train = 1.187007118604783
iter = 620 , loss = 0.8423392153959044 , RMSE train = 1.1870079786012813
iter = 630 , loss = 0.8291599990454086 , RMSE train = 1.1870086950804857
iter = 640 , loss = 0.8172408583420999 , RMSE train = 1.187009291817516
iter = 650 , loss = 0.8064613200196863 , RMSE train = 1.1870097886845397
iter = 660 , loss = 0.7967124284010865 , RMSE train = 1.1870102022840507
iter = 670 , loss = 0.7878956443837433 , RMSE train = 1.187010546481031
iter = 680 , loss = 0.7799218496599947 , RMSE train = 1.1870108328496862
iter = 690 , loss = 0.772710446116603 , RMSE train = 1.1870110710483532
iter = 700 , loss = 0.7661885413176499 , RMSE train = 1.1870112691336887
iter = 710 , loss = 0.7602902118444529 , RMSE train = 1.1870114338239448
iter = 720 , loss = 0.7549558370519858 , RMSE train = 1.1870115707195659
iter = 730 , loss = 0.75013149651162 , RMSE train = 1.1870116844876002
iter = 740 , loss = 0.7457684250539817 , RMSE train = 1.1870117790161898
iter = 750 , loss = 0.741822519906227 , RMSE train = 1.1870118575435804
iter = 760 , loss = 0.738253894944815 , RMSE train = 1.187011922765954
iter = 770 , loss = 0.735026477560598 , RMSE train = 1.1870119769276921
iter = 780 , loss = 0.7321076440626001 , RMSE train = 1.1870120218964537
iter = 790 , loss = 0.7294678899374757 , RMSE train = 1.1870120592261766
iter = 800 , loss = 0.7270805316320746 , RMSE train = 1.1870120902093475
iter = 810 , loss = 0.7249214368464351 , RMSE train = 1.1870121159207967
iter = 820 , loss = 0.7229687806114763 , RMSE train = 1.187012137254139
iter = 830 , loss = 0.7212028246866389 , RMSE train = 1.1870121549521526
iter = 840 , loss = 0.7196057180482445 , RMSE train = 1.1870121696321412
iter = 850 , loss = 0.7181613164522449 , RMSE train = 1.1870121818070132
iter = 860 , loss = 0.7168550192479842 , RMSE train = 1.187012191902878
iter = 870 , loss = 0.7156736217935659 , RMSE train = 1.187012200273549
iter = 880 , loss = 0.7146051819818736 , RMSE train = 1.187012207212952
iter = 890 , loss = 0.7136388995273617 , RMSE train = 1.1870122129650742
iter = 900 , loss = 0.7127650067943738 , RMSE train = 1.187012217732442
iter = 910 , loss = 0.7119746700632859 , RMSE train = 1.1870122216831602
iter = 920 , loss = 0.7112599002365639 , RMSE train = 1.1870122249567372
iter = 930 , loss = 0.710613472082225 , RMSE train = 1.1870122276689115
iter = 940 , loss = 0.7100288511985832 , RMSE train = 1.18701222991573
iter = 950 , loss = 0.7095001279618142 , RMSE train = 1.1870122317768044
iter = 960 , loss = 0.709021957789134 , RMSE train = 1.1870122333182078
iter = 970 , loss = 0.7085895071133289 , RMSE train = 1.187012234594731
iter = 980 , loss = 0.7081984045227921 , RMSE train = 1.187012235651764
iter = 990 , loss = 0.7078446965733264 , RMSE train = 1.1870122365269888
iter = 1000 , loss = 0.707524807824647 , RMSE train = 1.187012237251574

User-based MF, RMSE = 1.2309334464822106

################################################################################
user-based = 0 ( ~ item-based)
################################################################################

K = 100, lam = .1, learning_rate = 0.1, max_iter = 500, user_based = 0)

iter = 10 , loss = 93.17391014360814 , RMSE train = 3.9622538210586375
iter = 20 , loss = 76.00808466777018 , RMSE train = 3.673532166576498
iter = 30 , loss = 63.850079173662465 , RMSE train = 3.3435697438337795
iter = 40 , loss = 54.75521923503075 , RMSE train = 2.9970338842088995
iter = 50 , loss = 47.657236485153796 , RMSE train = 2.6522853198694256
iter = 60 , loss = 41.92790944765151 , RMSE train = 2.3212115641295963
iter = 70 , loss = 37.17793419857025 , RMSE train = 2.028248452401482
iter = 80 , loss = 33.1562352148681 , RMSE train = 1.7791106978820967
iter = 90 , loss = 29.69517748749746 , RMSE train = 1.5718598860941335
iter = 100 , loss = 26.67917283099114 , RMSE train = 1.4045800347283501
iter = 110 , loss = 24.025992849814877 , RMSE train = 1.2734004519110018
iter = 120 , loss = 21.67531600205198 , RMSE train = 1.1735663047276021
iter = 130 , loss = 19.581542451710636 , RMSE train = 1.1003748034957705
iter = 140 , loss = 17.709197076478794 , RMSE train = 1.0481508737842942
iter = 150 , loss = 16.029936670906828 , RMSE train = 1.0116292555107806
iter = 160 , loss = 14.520569273883531 , RMSE train = 0.9865342559287865
iter = 170 , loss = 13.16172166421489 , RMSE train = 0.9694623709515773
iter = 180 , loss = 11.936927397302675 , RMSE train = 0.9579595233407209
iter = 190 , loss = 10.831990986490359 , RMSE train = 0.9502672126171245
iter = 200 , loss = 9.834535535415682 , RMSE train = 0.9451606362624282
iter = 210 , loss = 8.933673705542622 , RMSE train = 0.9417942503346465
iter = 220 , loss = 8.119762673100904 , RMSE train = 0.9395917480012701
iter = 230 , loss = 7.384217102571514 , RMSE train = 0.9381639231046464
iter = 240 , loss = 6.719362846469495 , RMSE train = 0.9372488072574714
iter = 250 , loss = 6.118319759841206 , RMSE train = 0.9366717053876938
iter = 260 , loss = 5.574905756445607 , RMSE train = 0.9363160233723169
iter = 270 , loss = 5.083556710200835 , RMSE train = 0.9361042317000163
iter = 280 , loss = 4.639258456264882 , RMSE train = 0.9359849831771037
iter = 290 , loss = 4.2374882535917004 , RMSE train = 0.9359244095140432
iter = 300 , loss = 3.8741638190989516 , RMSE train = 0.9359002872036776
iter = 310 , loss = 3.545598553343607 , RMSE train = 0.9358981332642216
iter = 320 , loss = 3.2484619282265683 , RMSE train = 0.935908596054828
iter = 330 , loss = 2.979744251198747 , RMSE train = 0.9359257138636671
iter = 340 , loss = 2.7367251925065386 , RMSE train = 0.9359457543352805
iter = 350 , loss = 2.51694558533907 , RMSE train = 0.9359664423535933
iter = 360 , loss = 2.3181820987734074 , RMSE train = 0.9359864475137457
iter = 370 , loss = 2.138424450512078 , RMSE train = 0.9360050449180309
iter = 380 , loss = 1.9758548775265825 , RMSE train = 0.9360218915735644
iter = 390 , loss = 1.8288296225716878 , RMSE train = 0.9360368797858016
iter = 400 , loss = 1.6958622263077368 , RMSE train = 0.9360500417370269
iter = 410 , loss = 1.5756084406530937 , RMSE train = 0.9360614880052767
iter = 420 , loss = 1.4668526004886446 , RMSE train = 0.9360713685099393
iter = 430 , loss = 1.3684953090048904 , RMSE train = 0.9360798482066162
iter = 440 , loss = 1.2795423075574341 , RMSE train = 0.936087092419273
iter = 450 , loss = 1.1990944144097544 , RMSE train = 0.93609325841336
iter = 460 , loss = 1.1263384285795193 , RMSE train = 0.9360984909605083
iter = 470 , loss = 1.0605389054524728 , RMSE train = 0.9361029204110081
iter = 480 , loss = 1.0010307201037718 , RMSE train = 0.9361066623006432
iter = 490 , loss = 0.9472123425403158 , RMSE train = 0.9361098178587416
iter = 500 , loss = 0.8985397564815312 , RMSE train = 0.9361124750093895

User-based MF, RMSE = 1.8203556259295375

################################################################################

K = 100, lam = .1, learning_rate = 0.1, max_iter = 1000, user_based = 0)

iter = 10 , loss = 93.13489483528265 , RMSE train = 3.937991198149626
iter = 20 , loss = 75.96949241072167 , RMSE train = 3.6421692580295626
iter = 30 , loss = 63.821345079949126 , RMSE train = 3.3113687655680435
iter = 40 , loss = 54.735167723901185 , RMSE train = 2.9696297549268134
iter = 50 , loss = 47.64351384619022 , RMSE train = 2.6350790174431067
iter = 60 , loss = 41.91862317671532 , RMSE train = 2.311997443892219
iter = 70 , loss = 37.17174523684426 , RMSE train = 2.0219450495285995
iter = 80 , loss = 33.152219682167164 , RMSE train = 1.7753185718627171
iter = 90 , loss = 29.692696573178765 , RMSE train = 1.5698316585602938
iter = 100 , loss = 26.67778019674355 , RMSE train = 1.4032729249772287
iter = 110 , loss = 24.025372725411938 , RMSE train = 1.2717671993038322
iter = 120 , loss = 21.675242272977197 , RMSE train = 1.1719712417997847
iter = 130 , loss = 19.58185147414414 , RMSE train = 1.0986079318724373
iter = 140 , loss = 17.709769398478937 , RMSE train = 1.046378589904996
iter = 150 , loss = 16.030684561988743 , RMSE train = 1.010019690894864
iter = 160 , loss = 14.521428082927802 , RMSE train = 0.9851138857315834
iter = 170 , loss = 13.162643733950906 , RMSE train = 0.9682620221513051
iter = 180 , loss = 11.937877711099867 , RMSE train = 0.956958368999318
iter = 190 , loss = 10.832944007209235 , RMSE train = 0.9494445585863184
iter = 200 , loss = 9.835472877749122 , RMSE train = 0.9444886279057341
iter = 210 , loss = 8.934582400588623 , RMSE train = 0.9412463860339035
iter = 220 , loss = 8.120633860450866 , RMSE train = 0.9391446988881698
iter = 230 , loss = 7.385045035710991 , RMSE train = 0.9377990119378203
iter = 240 , loss = 6.720144131039561 , RMSE train = 0.9369513309679753
iter = 250 , loss = 6.119052767165138 , RMSE train = 0.9364293952563844
iter = 260 , loss = 5.575590170429928 , RMSE train = 0.936118649718767
iter = 270 , loss = 5.084193176677346 , RMSE train = 0.9359435297222265
iter = 280 , loss = 4.639848311608555 , RMSE train = 0.9358541831956562
iter = 290 , loss = 4.238033315047075 , RMSE train = 0.9358179759912558
iter = 300 , loss = 3.8746662233450113 , RMSE train = 0.9358136995133401
iter = 310 , loss = 3.546060632872871 , RMSE train = 0.9358277031637287
iter = 320 , loss = 3.24888611740923 , RMSE train = 0.9358513166417807
iter = 330 , loss = 2.980133015654908 , RMSE train = 0.9358791351232159
iter = 340 , loss = 2.737080976760755 , RMSE train = 0.9359078809135535
iter = 350 , loss = 2.517270774783522 , RMSE train = 0.9359356497778841
iter = 360 , loss = 2.318478992792267 , RMSE train = 0.9359614136475358
iter = 370 , loss = 2.138695244269828 , RMSE train = 0.9359846939523512
iter = 380 , loss = 1.9761016504405897 , RMSE train = 0.9360053483080926
iter = 390 , loss = 1.8290543318991426 , RMSE train = 0.9360234323383226
iter = 400 , loss = 1.6960667046052087 , RMSE train = 0.9360391111468103
iter = 410 , loss = 1.5757943961278973 , RMSE train = 0.9360526034682305
iter = 420 , loss = 1.4670216194703638 , RMSE train = 0.9360641472104887
iter = 430 , loss = 1.3686488599323652 , RMSE train = 0.9360739788987896
iter = 440 , loss = 1.2796817460092622 , RMSE train = 0.9360823220635082
iter = 450 , loss = 1.19922098881323 , RMSE train = 0.9360893813031296
iter = 460 , loss = 1.1264532863169543 , RMSE train = 0.9360953398763798
iter = 470 , loss = 1.0606430991513651 , RMSE train = 0.9361003594253781
iter = 480 , loss = 1.0011252139503886 , RMSE train = 0.9361045809259582
iter = 490 , loss = 0.9472980184980715 , RMSE train = 0.9361081262887455
iter = 500 , loss = 0.8986174203289088 , RMSE train = 0.9361111002498311
iter = 510 , loss = 0.8545913470689244 , RMSE train = 0.9361135923302618
iter = 520 , loss = 0.8147747727735908 , RMSE train = 0.9361156787352933
iter = 530 , loss = 0.7787652198927906 , RMSE train = 0.936117424122439
iter = 540 , loss = 0.7461986913387832 , RMSE train = 0.9361188832047179
iter = 550 , loss = 0.7167459915045727 , RMSE train = 0.936120102177822
iter = 560 , loss = 0.6901093990278055 , RMSE train = 0.9361211199742052
iter = 570 , loss = 0.6660196576598739 , RMSE train = 0.9361219693539986
iter = 580 , loss = 0.6442332548215746 , RMSE train = 0.936122677846871
iter = 590 , loss = 0.624529960338372 , RMSE train = 0.936123268560712
iter = 600 , loss = 0.6067106004794085 , RMSE train = 0.9361237608724178
iter = 610 , loss = 0.5905950448045052 , RMSE train = 0.9361241710160324
iter = 620 , loss = 0.5760203854742512 , RMSE train = 0.9361245125818974
iter = 630 , loss = 0.5628392906237676 , RMSE train = 0.9361247969391854
iter = 640 , loss = 0.5509185151599202 , RMSE train = 0.9361250335928816
iter = 650 , loss = 0.5401375539325874 , RMSE train = 0.9361252304848776
iter = 660 , loss = 0.5303874236692455 , RMSE train = 0.9361253942475317
iter = 670 , loss = 0.521569561363159 , RMSE train = 0.936125530416816
iter = 680 , loss = 0.5135948279824853 , RMSE train = 0.936125643611453
iter = 690 , loss = 0.50638260743159 , RMSE train = 0.9361257376833086
iter = 700 , loss = 0.49985999165822415 , RMSE train = 0.936125815843352
iter = 710 , loss = 0.4939610436711595 , RMSE train = 0.9361258807673372
iter = 720 , loss = 0.4886261310195896 , RMSE train = 0.9361259346841427
iter = 730 , loss = 0.4838013229982162 , RMSE train = 0.9361259794498441
iter = 740 , loss = 0.4794378454853356 , RMSE train = 0.9361260166095484
iter = 750 , loss = 0.47549158790398405 , RMSE train = 0.9361260474490266
iter = 760 , loss = 0.47192265732309246 , RMSE train = 0.9361260730380073
iter = 770 , loss = 0.4686949751914781 , RMSE train = 0.9361260942662012
iter = 780 , loss = 0.4657759126287835 , RMSE train = 0.9361261118733618
iter = 790 , loss = 0.4631359605870887 , RMSE train = 0.9361261264744178
iter = 800 , loss = 0.46074843154916595 , RMSE train = 0.93612613858041
iter = 810 , loss = 0.4585891897481949 , RMSE train = 0.9361261486159124
iter = 820 , loss = 0.4566364071820946 , RMSE train = 0.9361261569336204
iter = 830 , loss = 0.45487034295609907 , RMSE train = 0.9361261638264158
iter = 840 , loss = 0.45327314372329003 , RMSE train = 0.9361261695374837
iter = 850 , loss = 0.45182866320573783 , RMSE train = 0.9361261742686526
iter = 860 , loss = 0.4505222989721277 , RMSE train = 0.9361261781874616
iter = 870 , loss = 0.4493408448216463 , RMSE train = 0.9361261814328734
iter = 880 , loss = 0.44827235728235976 , RMSE train = 0.9361261841202442
iter = 890 , loss = 0.44730603487407133 , RMSE train = 0.9361261863451853
iter = 900 , loss = 0.44643210891572593 , RMSE train = 0.9361261881870292
iter = 910 , loss = 0.44564174477309265 , RMSE train = 0.9361261897115106
iter = 920 , loss = 0.44492695254885395 , RMSE train = 0.9361261909731503
iter = 930 , loss = 0.4442805063120132 , RMSE train = 0.9361261920171524
iter = 940 , loss = 0.44369587105018343 , RMSE train = 0.9361261928809078
iter = 950 , loss = 0.443167136606619 , RMSE train = 0.9361261935954867
iter = 960 , loss = 0.44268895793360624 , RMSE train = 0.9361261941865576
iter = 970 , loss = 0.44225650105886055 , RMSE train = 0.9361261946754302
iter = 980 , loss = 0.44186539421819543 , RMSE train = 0.9361261950797052
iter = 990 , loss = 0.44151168366085825 , RMSE train = 0.9361261954139802
iter = 1000 , loss = 0.44119179368053313 , RMSE train = 0.9361261956903739

User-based MF, RMSE = 1.8203572552753913

################################################################################

K = 500, lam = .1, learning_rate = 0.1, max_iter = 1000, user_based = 0)

iter = 10 , loss = 270.31749321519663 , RMSE train = 4.491019992955309
iter = 20 , loss = 201.86543412562582 , RMSE train = 4.2675484882699335
iter = 30 , loss = 160.38338988714779 , RMSE train = 4.027343616410865
iter = 40 , loss = 132.51182284182335 , RMSE train = 3.7611550344687372
iter = 50 , loss = 112.42555125753707 , RMSE train = 3.469014193969428
iter = 60 , loss = 97.16093841318006 , RMSE train = 3.162299960508953
iter = 70 , loss = 85.06946052631703 , RMSE train = 2.8494702092360935
iter = 80 , loss = 75.17593836009193 , RMSE train = 2.540250480452281
iter = 90 , loss = 66.87523896275397 , RMSE train = 2.24114644004281
iter = 100 , loss = 59.77622288368247 , RMSE train = 1.9691443041676666
iter = 110 , loss = 53.61647928799862 , RMSE train = 1.7335818926127382
iter = 120 , loss = 48.213600739646054 , RMSE train = 1.5370112906663602
iter = 130 , loss = 43.43634778460523 , RMSE train = 1.3778421352732295
iter = 140 , loss = 39.18709362625809 , RMSE train = 1.2524949664780236
iter = 150 , loss = 35.39087766908358 , RMSE train = 1.1568206382256703
iter = 160 , loss = 31.988434669325265 , RMSE train = 1.0867089730044674
iter = 170 , loss = 28.93166831635538 , RMSE train = 1.036927890660353
iter = 180 , loss = 26.180655945883657 , RMSE train = 1.002288294526373
iter = 190 , loss = 23.70162799600629 , RMSE train = 0.978700331933997
iter = 200 , loss = 21.46557718350286 , RMSE train = 0.9628799340556997
iter = 210 , loss = 19.447280196104426 , RMSE train = 0.9524175690520277
iter = 220 , loss = 17.624593365191537 , RMSE train = 0.9455895049610785
iter = 230 , loss = 15.977932929482721 , RMSE train = 0.9412002629321573
iter = 240 , loss = 14.489881595491607 , RMSE train = 0.9384320705278286
iter = 250 , loss = 13.14488299087322 , RMSE train = 0.9367320418031969
iter = 260 , loss = 11.928998451691466 , RMSE train = 0.9357285593596321
iter = 270 , loss = 10.829708950570136 , RMSE train = 0.9351740547073057
iter = 280 , loss = 9.835750464300087 , RMSE train = 0.9349041765018143
iter = 290 , loss = 8.936974711493587 , RMSE train = 0.9348106892176195
iter = 300 , loss = 8.124229611234824 , RMSE train = 0.9348228900789587
iter = 310 , loss = 7.389255439522001 , RMSE train = 0.9348950735443233
iter = 320 , loss = 6.724593762115417 , RMSE train = 0.9349980915437497
iter = 330 , loss = 6.123506976653104 , RMSE train = 0.9351137092283761
iter = 340 , loss = 5.579906819308351 , RMSE train = 0.9352308395558492
iter = 350 , loss = 5.088290558110958 , RMSE train = 0.9353430377626967
iter = 360 , loss = 4.6436838568221415 , RMSE train = 0.9354468392634162
iter = 370 , loss = 4.241589483499989 , RMSE train = 0.9355406613690385
iter = 380 , loss = 3.877941179015092 , RMSE train = 0.9356240813541744
iter = 390 , loss = 3.549062107799142 , RMSE train = 0.9356973652924488
iter = 400 , loss = 3.2516273961470192 , RMSE train = 0.9357611635965217
iter = 410 , loss = 2.982630329310665 , RMSE train = 0.9358163170311059
iter = 420 , loss = 2.7393518320966224 , RMSE train = 0.9358637356171781
iter = 430 , loss = 2.519332901931653 , RMSE train = 0.9359043253405738
iter = 440 , loss = 2.3203497006305778 , RMSE train = 0.935938945948909
iter = 450 , loss = 2.1403910429606885 , RMSE train = 0.9359683887252918
iter = 460 , loss = 1.9776380476744715 , RMSE train = 0.9359933668801349
iter = 470 , loss = 1.830445740791085 , RMSE train = 0.9360145137114185
iter = 480 , loss = 1.697326422153023 , RMSE train = 0.9360323853606193
iter = 490 , loss = 1.5769346251225984 , RMSE train = 0.9360474661083579
iter = 500 , loss = 1.468053516068823 , RMSE train = 0.9360601748978731
iter = 510 , loss = 1.369582595304309 , RMSE train = 0.9360708722650979
iter = 520 , loss = 1.2805265745932435 , RMSE train = 0.9360798671788593
iter = 530 , loss = 1.1999853184471896 , RMSE train = 0.9360874235054831
iter = 540 , loss = 1.1271447473135185 , RMSE train = 0.936093765948498
iter = 550 , loss = 1.0612686105731146 , RMSE train = 0.9360990854007492
iter = 560 , loss = 1.0016910461139636 , RMSE train = 0.9361035436994132
iter = 570 , loss = 0.9478098512353357 , RMSE train = 0.9361072778060996
iter = 580 , loss = 0.8990803968511359 , RMSE train = 0.9361104034519923
iter = 590 , loss = 0.8550101234778473 , RMSE train = 0.9361130182963854
iter = 600 , loss = 0.8151535633812186 , RMSE train = 0.9361152046493298
iter = 610 , loss = 0.7791078385796079 , RMSE train = 0.9361170318093702
iter = 620 , loss = 0.7465085892127201 , RMSE train = 0.9361185580634098
iter = 630 , loss = 0.7170262911359826 , RMSE train = 0.9361198323929465
iter = 640 , loss = 0.6903629255340621 , RMSE train = 0.9361208959262062
iter = 650 , loss = 0.6662489669041641 , RMSE train = 0.9361217831712697
iter = 660 , loss = 0.6444406589768448 , RMSE train = 0.9361225230615502
iter = 670 , loss = 0.6247175510507481 , RMSE train = 0.9361231398406002
iter = 680 , loss = 0.6068802698488049 , RMSE train = 0.9361236538100148
iter = 690 , loss = 0.5907485043824119 , RMSE train = 0.9361240819605884
iter = 700 , loss = 0.5761591834625042 , RMSE train = 0.9361244385046755
iter = 710 , loss = 0.562964827441922 , RMSE train = 0.936124735324424
iter = 720 , loss = 0.5510320575342261 , RMSE train = 0.9361249823490843
iter = 730 , loss = 0.5402402476455401 , RMSE train = 0.9361251878721255
iter = 740 , loss = 0.5304803050961581 , RMSE train = 0.9361253588177099
iter = 750 , loss = 0.5216535679105495 , RMSE train = 0.936125500964361
iter = 760 , loss = 0.5136708075322779 , RMSE train = 0.9361256191326222
iter = 770 , loss = 0.5064513268853108 , RMSE train = 0.9361257173422768
iter = 780 , loss = 0.4999221446668957 , RMSE train = 0.9361257989441615
iter = 790 , loss = 0.4940172576281379 , RMSE train = 0.9361258667305233
iter = 800 , loss = 0.4886769733865961 , RMSE train = 0.9361259230273632
iter = 810 , loss = 0.4838473070277761 , RMSE train = 0.9361259697715845
iter = 820 , loss = 0.4794794353973657 , RMSE train = 0.9361260085756515
iter = 830 , loss = 0.4755292035684572 , RMSE train = 0.9361260407815087
iter = 840 , loss = 0.47195667849569145 , RMSE train = 0.9361260675056214
iter = 850 , loss = 0.46872574534500705 , RMSE train = 0.9361260896766137
iter = 860 , loss = 0.4658037424188532 , RMSE train = 0.9361261080666516
iter = 870 , loss = 0.4631611309869104 , RMSE train = 0.9361261233176414
iter = 880 , loss = 0.46077119668508193 , RMSE train = 0.9361261359631191
iter = 890 , loss = 0.4586097794643268 , RMSE train = 0.9361261464462941
iter = 900 , loss = 0.45665502936023983 , RMSE train = 0.9361261551354445
iter = 910 , loss = 0.45488718561392033 , RMSE train = 0.9361261623363429
iter = 920 , loss = 0.4532883769122011 , RMSE train = 0.9361261683029158
iter = 930 , loss = 0.45184244072753166 , RMSE train = 0.936126173245967
iter = 940 , loss = 0.4505347599315052 , RMSE train = 0.936126177340411
iter = 950 , loss = 0.4493521150306162 , RMSE train = 0.9361261807314364
iter = 970 , loss = 0.44731525407651385 , RMSE train = 0.9361261858643869
iter = 980 , loss = 0.4464404471535111 , RMSE train = 0.9361261877890488
iter = 990 , loss = 0.44564928623209904 , RMSE train = 0.9361261893821482
iter = 1000 , loss = 0.44493377336994877 , RMSE train = 0.9361261907006105

User-based MF, RMSE = 1.8203572536042316

################################################################################

K = 500, lam = .1, learning_rate = 0.75, max_iter = 1000, user_based = 0)

iter = 10 , loss = 34.08342340708427 , RMSE train = 1.8510052827449368
iter = 20 , loss = 15.226198139765074 , RMSE train = 0.9960483265949153
iter = 30 , loss = 7.19379952726204 , RMSE train = 0.9372761494225124
iter = 40 , loss = 3.535333278078763 , RMSE train = 0.935762216845799
iter = 50 , loss = 1.858520032590268 , RMSE train = 0.9360048571147075
iter = 60 , loss = 1.0895418827771854 , RMSE train = 0.9360948981985552
iter = 70 , loss = 0.736883916154391 , RMSE train = 0.9361185583662873
iter = 80 , loss = 0.5751555031174662 , RMSE train = 0.9361243750510939
iter = 90 , loss = 0.500987985044017 , RMSE train = 0.9361257687117617
iter = 100 , loss = 0.4669755022288227 , RMSE train = 0.9361260974276665
iter = 110 , loss = 0.45137776562750365 , RMSE train = 0.9361261740584517
iter = 120 , loss = 0.44422482986685086 , RMSE train = 0.9361261917562321
iter = 130 , loss = 0.44094457932162023 , RMSE train = 0.9361261958120344
iter = 140 , loss = 0.4394402951123796 , RMSE train = 0.936126196735481
iter = 150 , loss = 0.4387504476187742 , RMSE train = 0.9361261969445823
iter = 160 , loss = 0.43843409119055965 , RMSE train = 0.9361261969916945
iter = 170 , loss = 0.438289013494843 , RMSE train = 0.93612619700227
iter = 180 , loss = 0.4382224823394708 , RMSE train = 0.9361261970046332
iter = 190 , loss = 0.4381919717974492 , RMSE train = 0.9361261970051641
iter = 200 , loss = 0.43817797994461916 , RMSE train = 0.9361261970052775
iter = 210 , loss = 0.43817156340343977 , RMSE train = 0.9361261970053075
iter = 220 , loss = 0.43816862083074926 , RMSE train = 0.9361261970053119
iter = 230 , loss = 0.438167271389836 , RMSE train = 0.936126197005313
iter = 240 , loss = 0.4381666525460849 , RMSE train = 0.9361261970053132
iter = 250 , loss = 0.438166368748554 , RMSE train = 0.9361261970053134
iter = 260 , loss = 0.4381662386007975 , RMSE train = 0.9361261970053134
iter = 270 , loss = 0.43816617891579274 , RMSE train = 0.9361261970053134
iter = 280 , loss = 0.43816615154456623 , RMSE train = 0.9361261970053134
iter = 290 , loss = 0.4381661389922531 , RMSE train = 0.9361261970053134
iter = 300 , loss = 0.43816613323581605 , RMSE train = 0.9361261970053134
iter = 310 , loss = 0.43816613059593573 , RMSE train = 0.9361261970053134
iter = 320 , loss = 0.43816612938529514 , RMSE train = 0.9361261970053134
iter = 330 , loss = 0.4381661288300987 , RMSE train = 0.9361261970053134
iter = 340 , loss = 0.43816612857548676 , RMSE train = 0.9361261970053134
iter = 350 , loss = 0.43816612845872216 , RMSE train = 0.9361261970053134
iter = 360 , loss = 0.43816612840517405 , RMSE train = 0.9361261970053134
iter = 370 , loss = 0.4381661283806169 , RMSE train = 0.9361261970053134
iter = 380 , loss = 0.43816612836935503 , RMSE train = 0.9361261970053134
iter = 390 , loss = 0.43816612836419033 , RMSE train = 0.9361261970053134
iter = 400 , loss = 0.4381661283618218 , RMSE train = 0.9361261970053134
iter = 410 , loss = 0.43816612836073554 , RMSE train = 0.9361261970053134
iter = 420 , loss = 0.4381661283602374 , RMSE train = 0.9361261970053134
iter = 430 , loss = 0.43816612836000896 , RMSE train = 0.9361261970053134
iter = 440 , loss = 0.4381661283599042 , RMSE train = 0.9361261970053134
iter = 450 , loss = 0.43816612835985613 , RMSE train = 0.9361261970053134
iter = 460 , loss = 0.4381661283598341 , RMSE train = 0.9361261970053134
iter = 470 , loss = 0.438166128359824 , RMSE train = 0.9361261970053134
iter = 480 , loss = 0.4381661283598194 , RMSE train = 0.9361261970053134
iter = 490 , loss = 0.4381661283598172 , RMSE train = 0.9361261970053134
iter = 500 , loss = 0.4381661283598163 , RMSE train = 0.9361261970053134
iter = 510 , loss = 0.43816612835981583 , RMSE train = 0.9361261970053134
iter = 520 , loss = 0.4381661283598156 , RMSE train = 0.9361261970053134
iter = 530 , loss = 0.4381661283598155 , RMSE train = 0.9361261970053134
iter = 540 , loss = 0.4381661283598155 , RMSE train = 0.9361261970053134
iter = 550 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 560 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 570 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 580 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 590 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 600 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 610 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 620 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 630 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 640 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 650 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 660 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 670 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 680 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 690 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 700 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 710 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 720 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 730 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 740 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 750 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 760 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 770 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 780 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 790 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 800 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 810 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 820 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 830 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 840 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 850 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 860 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 870 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 880 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 890 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 900 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 910 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 920 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 930 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 940 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 950 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 960 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 970 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 980 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 990 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134
iter = 1000 , loss = 0.43816612835981544 , RMSE train = 0.9361261970053134

User-based MF, aMSE = 1.8203572558685246

################################################################################

