
################################################################################
user-based = 1 ( ~ user-based)
################################################################################

K = 50, lam = .1, learning_rate = 0.75, max_iter = 350, user_based = 1

iter = 10 , loss = 24.482614722434132 , RMSE train = 1.8620923460835566
iter = 20 , loss = 11.42085493540561 , RMSE train = 1.42314542738421
iter = 30 , loss = 5.756484683988467 , RMSE train = 1.4025775406699572
iter = 40 , loss = 3.172449489537195 , RMSE train = 1.4026291881075477
iter = 50 , loss = 1.987853856221522 , RMSE train = 1.4029330623835528
iter = 60 , loss = 1.4445562580876388 , RMSE train = 1.4030311752836884
iter = 70 , loss = 1.1953800178866953 , RMSE train = 1.4030578798747348
iter = 80 , loss = 1.081102302285156 , RMSE train = 1.403064746572798
iter = 90 , loss = 1.0286932237959878 , RMSE train = 1.403066458525812
iter = 100 , loss = 1.0046581118442999 , RMSE train = 1.4030668765906722
iter = 110 , loss = 0.9936355225814518 , RMSE train = 1.4030669771537778
iter = 120 , loss = 0.9885805253068183 , RMSE train = 1.4030670010678876
iter = 130 , loss = 0.9862622808662787 , RMSE train = 1.403067006704199
iter = 140 , loss = 0.9851991194034326 , RMSE train = 1.4030670080232728
iter = 150 , loss = 0.9847115444849668 , RMSE train = 1.4030670083302386
iter = 160 , loss = 0.9844879372174224 , RMSE train = 1.4030670084013739
iter = 170 , loss = 0.9843853878162968 , RMSE train = 1.4030670084177677
iter = 180 , loss = 0.9843383569261309 , RMSE train = 1.4030670084216115
iter = 190 , loss = 0.9843167876124502 , RMSE train = 1.4030670084224475
iter = 200 , loss = 0.9843068954164838 , RMSE train = 1.4030670084226295
iter = 210 , loss = 0.9843023585858329 , RMSE train = 1.40306700842271
iter = 220 , loss = 0.9843002778537934 , RMSE train = 1.4030670084227144
iter = 230 , loss = 0.9842993235564541 , RMSE train = 1.4030670084227153
iter = 240 , loss = 0.9842988858776256 , RMSE train = 1.4030670084227153
iter = 250 , loss = 0.9842986851385356 , RMSE train = 1.4030670084227153
iter = 260 , loss = 0.9842985930695626 , RMSE train = 1.4030670084227153
iter = 270 , loss = 0.9842985508416157 , RMSE train = 1.4030670084227153
iter = 280 , loss = 0.9842985314732764 , RMSE train = 1.4030670084227153
iter = 290 , loss = 0.9842985225896352 , RMSE train = 1.4030670084227153
iter = 300 , loss = 0.9842985185149283 , RMSE train = 1.4030670084227153
iter = 310 , loss = 0.9842985166459296 , RMSE train = 1.4030670084227153
iter = 320 , loss = 0.9842985157886359 , RMSE train = 1.4030670084227153
iter = 330 , loss = 0.984298515395395 , RMSE train = 1.4030670084227153
iter = 340 , loss = 0.9842985152150113 , RMSE train = 1.4030670084227153
iter = 350 , loss = 0.9842985151322655 , RMSE train = 1.4030670084227153

User-based MF, RMSE = 1.4621422080122954


iter = 10 , loss = 24.244447754210213 , RMSE train = 1.726586675152502
iter = 20 , loss = 11.216305725091445 , RMSE train = 1.2733405303768888
iter = 30 , loss = 5.555837459600835 , RMSE train = 1.2523447668995618
iter = 40 , loss = 2.9726760252038797 , RMSE train = 1.2525733373086685
iter = 50 , loss = 1.78838125905733 , RMSE train = 1.2529128167739199
iter = 60 , loss = 1.2452127347633832 , RMSE train = 1.253015234248123
iter = 70 , loss = 0.9960962858504989 , RMSE train = 1.2530420075279194
iter = 80 , loss = 0.8818467684970557 , RMSE train = 1.2530486952046682
iter = 90 , loss = 0.8294510049965182 , RMSE train = 1.2530503255994643
iter = 100 , loss = 0.805422172487104 , RMSE train = 1.2530507165494595
iter = 110 , loss = 0.7944025444030788 , RMSE train = 1.253050809143112
iter = 120 , loss = 0.789348945560642 , RMSE train = 1.2530508308637167
iter = 130 , loss = 0.7870313632621949 , RMSE train = 1.2530508359202612
iter = 140 , loss = 0.7859685163561669 , RMSE train = 1.2530508370902091
iter = 150 , loss = 0.7854810914211084 , RMSE train = 1.253050837359545
iter = 160 , loss = 0.7852575559402952 , RMSE train = 1.253050837421305
iter = 170 , loss = 0.7851550410299172 , RMSE train = 1.2530508374354186
iter = 180 , loss = 0.7851080267729824 , RMSE train = 1.2530508374386164
iter = 190 , loss = 0.7850864655096484 , RMSE train = 1.2530508374392775
iter = 200 , loss = 0.7850765772233704 , RMSE train = 1.2530508374394305
iter = 210 , loss = 0.7850720422975871 , RMSE train = 1.2530508374394804
iter = 220 , loss = 0.7850699624965026 , RMSE train = 1.2530508374394873
iter = 230 , loss = 0.7850690086554244 , RMSE train = 1.2530508374394842
iter = 240 , loss = 0.7850685712008034 , RMSE train = 1.253050837439481
iter = 250 , loss = 0.7850683705721649 , RMSE train = 1.2530508374394813
iter = 260 , loss = 0.7850682785577264 , RMSE train = 1.2530508374394813
iter = 270 , loss = 0.7850682363567622 , RMSE train = 1.2530508374394813
iter = 280 , loss = 0.7850682170017993 , RMSE train = 1.2530508374394813
iter = 290 , loss = 0.785068208124801 , RMSE train = 1.2530508374394813
iter = 300 , loss = 0.7850682040533987 , RMSE train = 1.2530508374394813
iter = 310 , loss = 0.7850682021860461 , RMSE train = 1.2530508374394813
iter = 320 , loss = 0.7850682013295737 , RMSE train = 1.2530508374394813
iter = 330 , loss = 0.7850682009367429 , RMSE train = 1.2530508374394813
iter = 340 , loss = 0.7850682007565644 , RMSE train = 1.2530508374394813
iter = 350 , loss = 0.7850682006739211 , RMSE train = 1.2530508374394813

User-based MF, RMSE = 1.3178213441952316


iter = 10 , loss = 24.637791671512105 , RMSE train = 1.9403636494033503
iter = 20 , loss = 11.608728349922984 , RMSE train = 1.5525275457058694
iter = 30 , loss = 5.951730648690637 , RMSE train = 1.536866295337684
iter = 40 , loss = 3.3699337650642605 , RMSE train = 1.5376144920971682
iter = 50 , loss = 2.1861108378138514 , RMSE train = 1.5380517294432479
iter = 60 , loss = 1.6431077809713368 , RMSE train = 1.538177828426117
iter = 70 , loss = 1.3940520253732012 , RMSE train = 1.5382105541823228
iter = 80 , loss = 1.2798257827661548 , RMSE train = 1.5382187293086105
iter = 90 , loss = 1.2274392246975454 , RMSE train = 1.5382207275456177
iter = 100 , loss = 1.2034140826899766 , RMSE train = 1.5382212086371967
iter = 110 , loss = 1.1923959290841086 , RMSE train = 1.5382213231586488
iter = 120 , loss = 1.1873429075063766 , RMSE train = 1.5382213501819797
iter = 130 , loss = 1.185025542150543 , RMSE train = 1.5382213565147433
iter = 140 , loss = 1.1839627708885991 , RMSE train = 1.5382213579906465
iter = 150 , loss = 1.1834753685715826 , RMSE train = 1.5382213583331188
iter = 160 , loss = 1.1832518373177592 , RMSE train = 1.5382213584122955
iter = 170 , loss = 1.1831493212130675 , RMSE train = 1.538221358430547
iter = 180 , loss = 1.1831023048120366 , RMSE train = 1.538221358434768
iter = 190 , loss = 1.1830807417527378 , RMSE train = 1.538221358435747
iter = 200 , loss = 1.183070852229473 , RMSE train = 1.5382213584359945
iter = 210 , loss = 1.1830663165263418 , RMSE train = 1.5382213584360525
iter = 220 , loss = 1.18306423626214 , RMSE train = 1.53822135843607
iter = 230 , loss = 1.183063282154557 , RMSE train = 1.5382213584360676
iter = 240 , loss = 1.1830628445502753 , RMSE train = 1.538221358436067
iter = 250 , loss = 1.1830626438390839 , RMSE train = 1.5382213584360664
iter = 260 , loss = 1.183062551779731 , RMSE train = 1.5382213584360664
iter = 270 , loss = 1.183062509554592 , RMSE train = 1.5382213584360664
iter = 280 , loss = 1.1830624901867293 , RMSE train = 1.5382213584360664
iter = 290 , loss = 1.1830624813028956 , RMSE train = 1.5382213584360664
iter = 300 , loss = 1.1830624772278922 , RMSE train = 1.5382213584360664
iter = 310 , loss = 1.1830624753586516 , RMSE train = 1.5382213584360664
iter = 320 , loss = 1.1830624745011933 , RMSE train = 1.5382213584360664
iter = 330 , loss = 1.1830624741078495 , RMSE train = 1.5382213584360664
iter = 340 , loss = 1.1830624739274047 , RMSE train = 1.5382213584360664
iter = 350 , loss = 1.1830624738446238 , RMSE train = 1.5382213584360664

User-based MF, RMSE = 1.6035522323941516


iter = 10 , loss = 24.64918715056405 , RMSE train = 1.936895568508696
iter = 20 , loss = 11.579541234682146 , RMSE train = 1.5236455216967928
iter = 30 , loss = 5.9138264624406816 , RMSE train = 1.5073729525275084
iter = 40 , loss = 3.3283022967872924 , RMSE train = 1.5081725085901259
iter = 50 , loss = 2.1427620281650466 , RMSE train = 1.508630045547344
iter = 60 , loss = 1.5989669457435522 , RMSE train = 1.508761184290893
iter = 70 , loss = 1.3495468831565118 , RMSE train = 1.5087950790511062
iter = 80 , loss = 1.235153319880804 , RMSE train = 1.5088035212855007
iter = 90 , loss = 1.1826899673903255 , RMSE train = 1.508805580335433
iter = 100 , loss = 1.1586295904946726 , RMSE train = 1.508806075272042
iter = 110 , loss = 1.1475952726926382 , RMSE train = 1.5088061929508803
iter = 120 , loss = 1.1425348361788674 , RMSE train = 1.5088062206962474
iter = 130 , loss = 1.1402140694897476 , RMSE train = 1.5088062271947118
iter = 140 , loss = 1.1391497379981939 , RMSE train = 1.5088062287088218
iter = 150 , loss = 1.1386616199799637 , RMSE train = 1.5088062290601276
iter = 160 , loss = 1.1384377604158455 , RMSE train = 1.5088062291413558
iter = 170 , loss = 1.1383350937015213 , RMSE train = 1.5088062291600646
iter = 180 , loss = 1.1382880082060876 , RMSE train = 1.5088062291643942
iter = 190 , loss = 1.1382664134464409 , RMSE train = 1.5088062291653348
iter = 200 , loss = 1.138256509377918 , RMSE train = 1.5088062291655804
iter = 210 , loss = 1.138251967000149 , RMSE train = 1.508806229165644
iter = 220 , loss = 1.1382498836726151 , RMSE train = 1.5088062291656712
iter = 230 , loss = 1.1382489281588668 , RMSE train = 1.508806229165672
iter = 240 , loss = 1.1382484899089769 , RMSE train = 1.5088062291656723
iter = 250 , loss = 1.1382482889012964 , RMSE train = 1.5088062291656728
iter = 260 , loss = 1.1382481967057432 , RMSE train = 1.5088062291656728
iter = 270 , loss = 1.1382481544180152 , RMSE train = 1.5088062291656728
iter = 280 , loss = 1.1382481350213787 , RMSE train = 1.5088062291656728
iter = 290 , loss = 1.1382481261243107 , RMSE train = 1.5088062291656728
iter = 300 , loss = 1.1382481220432168 , RMSE train = 1.5088062291656728
iter = 310 , loss = 1.1382481201711718 , RMSE train = 1.5088062291656728
iter = 320 , loss = 1.138248119312421 , RMSE train = 1.5088062291656728
iter = 330 , loss = 1.1382481189184812 , RMSE train = 1.5088062291656728
iter = 340 , loss = 1.1382481187377613 , RMSE train = 1.5088062291656728
iter = 350 , loss = 1.1382481186548532 , RMSE train = 1.5088062291656728

User-based MF, RMSE = 1.5927882830492057


iter = 10 , loss = 24.487024930378176 , RMSE train = 1.8445517793880388
iter = 20 , loss = 11.40102290007777 , RMSE train = 1.3992538694160024
iter = 30 , loss = 5.729558052382462 , RMSE train = 1.3786629579587784
iter = 40 , loss = 3.142206959161146 , RMSE train = 1.378795915418503
iter = 50 , loss = 1.9560540093950618 , RMSE train = 1.3791153240745315
iter = 60 , loss = 1.4120359511655562 , RMSE train = 1.379215407778521
iter = 70 , loss = 1.1625286297663986 , RMSE train = 1.3792422090788954
iter = 80 , loss = 1.04809911653447 , RMSE train = 1.3792490269371542
iter = 90 , loss = 0.99562048845303 , RMSE train = 1.379250713799168
iter = 100 , loss = 0.9715535179061983 , RMSE train = 1.3792511233990288
iter = 110 , loss = 0.9605163372184042 , RMSE train = 1.3792512214902155
iter = 120 , loss = 0.9554546580943115 , RMSE train = 1.3792512447330385
iter = 130 , loss = 0.9531333544850852 , RMSE train = 1.3792512501947662
iter = 140 , loss = 0.9520687927927531 , RMSE train = 1.3792512514696729
iter = 150 , loss = 0.9515805771545974 , RMSE train = 1.379251251765678
iter = 160 , loss = 0.951356676800518 , RMSE train = 1.3792512518340654
iter = 170 , loss = 0.951253993379777 , RMSE train = 1.3792512518498206
iter = 180 , loss = 0.9512069012302572 , RMSE train = 1.3792512518534474
iter = 190 , loss = 0.9511853039274042 , RMSE train = 1.3792512518542548
iter = 200 , loss = 0.9511753989492309 , RMSE train = 1.3792512518544386
iter = 210 , loss = 0.9511708562840473 , RMSE train = 1.3792512518544602
iter = 220 , loss = 0.9511687728903596 , RMSE train = 1.3792512518544584
iter = 230 , loss = 0.951167817379553 , RMSE train = 1.3792512518544588
iter = 240 , loss = 0.9511673791478755 , RMSE train = 1.3792512518544593
iter = 250 , loss = 0.9511671781571046 , RMSE train = 1.37925125185446
iter = 260 , loss = 0.951167085973653 , RMSE train = 1.3792512518544602
iter = 270 , loss = 0.9511670436936847 , RMSE train = 1.3792512518544602
iter = 280 , loss = 0.9511670243017313 , RMSE train = 1.3792512518544602
iter = 290 , loss = 0.9511670154073838 , RMSE train = 1.3792512518544602
iter = 300 , loss = 0.9511670113278297 , RMSE train = 1.3792512518544602
iter = 310 , loss = 0.9511670094566396 , RMSE train = 1.3792512518544602
iter = 320 , loss = 0.9511670085983571 , RMSE train = 1.3792512518544602
iter = 330 , loss = 0.9511670082046708 , RMSE train = 1.3792512518544602
iter = 340 , loss = 0.951167008024087 , RMSE train = 1.3792512518544602
iter = 350 , loss = 0.9511670079412515 , RMSE train = 1.3792512518544602

User-based MF, RMSE = 1.4469342827320701


User-based MF-5models, RMSE = 1.250198828494739

################################################################################

K = 50, lam = .1, learning_rate = 0.75, max_iter = 350, user_based = 1

iter = 10 , loss = 61.05027155646822 , RMSE train = 3.724850708168153
iter = 20 , loss = 50.979075909628065 , RMSE train = 3.3783499650385
iter = 30 , loss = 43.635548839786864 , RMSE train = 3.0318900884105737
iter = 40 , loss = 37.98338955569427 , RMSE train = 2.7047676834147545
iter = 50 , loss = 33.4567538104113 , RMSE train = 2.4131840911946107
iter = 60 , loss = 29.72089728316378 , RMSE train = 2.168005080592541
iter = 70 , loss = 26.566265528858473 , RMSE train = 1.9669235839389956
iter = 80 , loss = 23.855700573259142 , RMSE train = 1.8065746294910527
iter = 90 , loss = 21.495915162691254 , RMSE train = 1.68370778813675
iter = 100 , loss = 19.42118197908232 , RMSE train = 1.593213467578969
iter = 110 , loss = 17.583614346334212 , RMSE train = 1.5296052889675813
iter = 120 , loss = 15.947190008498577 , RMSE train = 1.4859500216940016
iter = 130 , loss = 14.483983145582783 , RMSE train = 1.4565162538378293
iter = 140 , loss = 13.171737759638658 , RMSE train = 1.4368939867905601
iter = 150 , loss = 11.992274784924442 , RMSE train = 1.4239407450141353
iter = 160 , loss = 10.930427144934717 , RMSE train = 1.4154931603019232
iter = 170 , loss = 9.973314432769365 , RMSE train = 1.4100511954962218
iter = 180 , loss = 9.109839142455947 , RMSE train = 1.4066028388353493
iter = 190 , loss = 8.33032933535472 , RMSE train = 1.4044612967467807
iter = 200 , loss = 7.626279363924211 , RMSE train = 1.4031736459966986
iter = 210 , loss = 6.99015716078761 , RMSE train = 1.4024367757883325
iter = 220 , loss = 6.415257392471112 , RMSE train = 1.402050333560813
iter = 230 , loss = 5.895586743007156 , RMSE train = 1.4018826968890068
iter = 240 , loss = 5.425772127621795 , RMSE train = 1.401847987508874
iter = 250 , loss = 5.000985611637605 , RMSE train = 1.401890664475905
iter = 260 , loss = 4.616881774915092 , RMSE train = 1.4019752215643706
iter = 270 , loss = 4.269544569174233 , RMSE train = 1.4020793342979554
iter = 280 , loss = 3.955441590890977 , RMSE train = 1.402189300090619
iter = 290 , loss = 3.6713842831436323 , RMSE train = 1.4022970137474602
iter = 300 , loss = 3.414492981763204 , RMSE train = 1.4023979664429358
iter = 310 , loss = 3.182165997415803 , RMSE train = 1.4024899257306633
iter = 320 , loss = 2.9720521173429013 , RMSE train = 1.4025720699073145
iter = 330 , loss = 2.7820260458998596 , RMSE train = 1.402644424357334
iter = 340 , loss = 2.6101664000365963 , RMSE train = 1.402707497391699
iter = 350 , loss = 2.4547359466419616 , RMSE train = 1.4027620478559095

User-based MF, RMSE = 1.4619261530214456


iter = 10 , loss = 59.93289085139192 , RMSE train = 3.6101598446509855
iter = 20 , loss = 50.34668220963299 , RMSE train = 3.2719410745154063
iter = 30 , loss = 43.22037808306138 , RMSE train = 2.926542181542098
iter = 40 , loss = 37.673020316892085 , RMSE train = 2.597027949890949
iter = 50 , loss = 33.199565852325144 , RMSE train = 2.302488929351203
iter = 60 , loss = 29.491542355570985 , RMSE train = 2.0563597119517585
iter = 70 , loss = 26.351671770586925 , RMSE train = 1.8534059179605051
iter = 80 , loss = 23.64889558254049 , RMSE train = 1.6914660965947834
iter = 90 , loss = 21.29310432452404 , RMSE train = 1.5646739380920327
iter = 100 , loss = 19.220286429481945 , RMSE train = 1.469594578751878
iter = 110 , loss = 17.38350522909567 , RMSE train = 1.4009726236453142
iter = 120 , loss = 15.747272574806145 , RMSE train = 1.3527891706041204
iter = 130 , loss = 14.28396443028152 , RMSE train = 1.3196158390062405
iter = 140 , loss = 12.971494125429203 , RMSE train = 1.2970407054793078
iter = 150 , loss = 11.791775419864056 , RMSE train = 1.281823102221069
iter = 160 , loss = 10.729689649436576 , RMSE train = 1.2716368454489546
iter = 170 , loss = 9.772379135402046 , RMSE train = 1.2648640019262456
iter = 180 , loss = 8.908754475959395 , RMSE train = 1.2603899106395338
iter = 190 , loss = 8.129143781779721 , RMSE train = 1.2574574525247433
iter = 200 , loss = 7.42503729737877 , RMSE train = 1.2555555327522867
iter = 210 , loss = 6.788896981006075 , RMSE train = 1.2543387434960047
iter = 220 , loss = 6.214010976686291 , RMSE train = 1.2535749721476763
iter = 230 , loss = 5.694379626842052 , RMSE train = 1.2531088452190096
iter = 240 , loss = 5.224624059723915 , RMSE train = 1.2528366743820876
iter = 250 , loss = 4.7999112706336975 , RMSE train = 1.2526894878776775
iter = 260 , loss = 4.4158915259308085 , RMSE train = 1.2526215675375354
iter = 270 , loss = 4.0686451917073105 , RMSE train = 1.2526027606993684
iter = 280 , loss = 3.7546369431228275 , RMSE train = 1.2526133135973179
iter = 290 , loss = 3.4706758877902195 , RMSE train = 1.2526404059558793
iter = 300 , loss = 3.2138805302821276 , RMSE train = 1.2526758260687916
iter = 310 , loss = 2.981647775936045 , RMSE train = 1.252714414766888
iter = 320 , loss = 2.771625361046692 , RMSE train = 1.2527530235590632
iter = 330 , loss = 2.581687230018733 , RMSE train = 1.2527898180621093
iter = 340 , loss = 2.409911475916789 , RMSE train = 1.2528238133663443
iter = 350 , loss = 2.2545605309846817 , RMSE train = 1.2528545650543335

User-based MF, RMSE = 1.317717431770459


iter = 10 , loss = 61.10180223067265 , RMSE train = 3.766602421152606
iter = 20 , loss = 51.121283614996344 , RMSE train = 3.4209326324319167
iter = 30 , loss = 43.81373700225412 , RMSE train = 3.0843021310959218
iter = 40 , loss = 38.17675013054193 , RMSE train = 2.7635373482144168
iter = 50 , loss = 33.65672336459841 , RMSE train = 2.4786082450002946
iter = 60 , loss = 29.923746822373985 , RMSE train = 2.240431073122811
iter = 70 , loss = 26.77030527596924 , RMSE train = 2.0512079068580498
iter = 80 , loss = 24.06014390978728 , RMSE train = 1.903644378907883
iter = 90 , loss = 21.7003901325037 , RMSE train = 1.7915870733608927
iter = 100 , loss = 19.62551217713466 , RMSE train = 1.709096982842801
iter = 110 , loss = 17.787717491145823 , RMSE train = 1.6511087455649678
iter = 120 , loss = 16.151029877624875 , RMSE train = 1.6113158206470857
iter = 130 , loss = 14.687546398528953 , RMSE train = 1.5846289615703375
iter = 140 , loss = 13.375022613028621 , RMSE train = 1.5669471447730252
iter = 150 , loss = 12.195285417734832 , RMSE train = 1.5553847753447672
iter = 160 , loss = 11.13317090222368 , RMSE train = 1.547928455271733
iter = 170 , loss = 10.17580040429146 , RMSE train = 1.5431920006202309
iter = 180 , loss = 9.312077418398152 , RMSE train = 1.5402467778540463
iter = 190 , loss = 8.532330595621577 , RMSE train = 1.5384670245194705
iter = 200 , loss = 7.828054631899045 , RMSE train = 1.5374419044325685
iter = 210 , loss = 7.1917176389374 , RMSE train = 1.5368989723298148
iter = 220 , loss = 6.61661434001614 , RMSE train = 1.536657748688309
iter = 230 , loss = 6.096751377006431 , RMSE train = 1.5366003729205113
iter = 240 , loss = 5.626755539090384 , RMSE train = 1.5366505380471083
iter = 250 , loss = 5.201798693150821 , RMSE train = 1.5367594009149266
iter = 260 , loss = 4.817535158302161 , RMSE train = 1.53689619276746
iter = 270 , loss = 4.470048572744686 , RMSE train = 1.5370419749915467
iter = 280 , loss = 4.155806175867291 , RMSE train = 1.5371854956448345
iter = 290 , loss = 3.8716190189430097 , RMSE train = 1.5373204473390634
iter = 300 , loss = 3.6146070196513707 , RMSE train = 1.5374436588378788
iter = 310 , loss = 3.38216805192737 , RMSE train = 1.5375539080028346
iter = 320 , loss = 3.171950454759847 , RMSE train = 1.537651147569001
iter = 330 , loss = 2.9818284790143545 , RMSE train = 1.5377360046268038
iter = 340 , loss = 2.809880288395889 , RMSE train = 1.5378094610415358
iter = 350 , loss = 2.654368201475464 , RMSE train = 1.5378726530015467

User-based MF, RMSE = 1.6033605746302966


iter = 10 , loss = 61.6862464935181 , RMSE train = 3.7394264686484195
iter = 20 , loss = 51.40470827468238 , RMSE train = 3.4099775657403146
iter = 30 , loss = 43.93787332460978 , RMSE train = 3.080859565785077
iter = 40 , loss = 38.210995854682245 , RMSE train = 2.7683310202719476
iter = 50 , loss = 33.63935764840838 , RMSE train = 2.4867992595014043
iter = 60 , loss = 29.8769014203667 , RMSE train = 2.2429833036950857
iter = 70 , loss = 26.707073321732793 , RMSE train = 2.045763929258386
iter = 80 , loss = 23.988356835431237 , RMSE train = 1.8905778846240568
iter = 90 , loss = 21.624740755118786 , RMSE train = 1.772842362216122
iter = 100 , loss = 19.548790601358863 , RMSE train = 1.686579351887052
iter = 110 , loss = 17.71154853891717 , RMSE train = 1.6256182700314916
iter = 120 , loss = 16.07632123716298 , RMSE train = 1.5839193507603198
iter = 130 , loss = 14.614764706374972 , RMSE train = 1.556052500315703
iter = 140 , loss = 13.304365181836733 , RMSE train = 1.5376938859492395
iter = 150 , loss = 12.126787694609977 , RMSE train = 1.5257496592640378
iter = 160 , loss = 11.066773867084907 , RMSE train = 1.5180887944057158
iter = 170 , loss = 10.111392772367942 , RMSE train = 1.5132640385486085
iter = 180 , loss = 9.24952189641668 , RMSE train = 1.5103016985416755
iter = 190 , loss = 8.47148000966963 , RMSE train = 1.5085468038969612
iter = 200 , loss = 7.76876161716639 , RMSE train = 1.5075656460020932
iter = 210 , loss = 7.133840246341492 , RMSE train = 1.5070745303211945
iter = 220 , loss = 6.5600190686814805 , RMSE train = 1.5068864286104537
iter = 230 , loss = 6.041314601336678 , RMSE train = 1.5068796513140628
iter = 240 , loss = 5.572363952178804 , RMSE train = 1.5069759588249003
iter = 250 , loss = 5.148349164479362 , RMSE train = 1.5071257276530992
iter = 260 , loss = 4.764934258988605 , RMSE train = 1.5072980835811836
iter = 270 , loss = 4.418211927979878 , RMSE train = 1.507474356226323
iter = 280 , loss = 4.104657743679981 , RMSE train = 1.5076437488601964
iter = 290 , loss = 3.8210903554450244 , RMSE train = 1.5078004833042293
iter = 300 , loss = 3.5646365659392494 , RMSE train = 1.5079419255410598
iter = 310 , loss = 3.332700461955798 , RMSE train = 1.5080673622666327
iter = 320 , loss = 3.1229359735592865 , RMSE train = 1.5081772085884386
iter = 330 , loss = 2.933222374518194 , RMSE train = 1.5082725004987745
iter = 340 , loss = 2.7616423365010965 , RMSE train = 1.5083545747368996
iter = 350 , loss = 2.6064622218951574 , RMSE train = 1.508424871325948

User-based MF, RMSE = 1.592608205918047


iter = 10 , loss = 60.92545943819819 , RMSE train = 3.70404026825214
iter = 20 , loss = 50.902366769220876 , RMSE train = 3.3700540643535337
iter = 30 , loss = 43.575552264668275 , RMSE train = 3.028081096927274
iter = 40 , loss = 37.930094377949004 , RMSE train = 2.6997566519922183
iter = 50 , loss = 33.40687757481684 , RMSE train = 2.4017707393324956
iter = 60 , loss = 29.673298448499224 , RMSE train = 2.150304242653678
iter = 70 , loss = 26.520485055266274 , RMSE train = 1.9459330548596783
iter = 80 , loss = 23.811492896560008 , RMSE train = 1.7840544027373852
iter = 90 , loss = 21.45309999479609 , RMSE train = 1.6606751511786988
iter = 100 , loss = 19.379600929155053 , RMSE train = 1.5701769697296617
iter = 110 , loss = 17.54312079111859 , RMSE train = 1.5063077855098923
iter = 120 , loss = 15.907648408032442 , RMSE train = 1.4622231904864889
iter = 130 , loss = 14.44527016273565 , RMSE train = 1.4324561387959307
iter = 140 , loss = 13.133743016424008 , RMSE train = 1.4126442661886942
iter = 150 , loss = 11.954900875504597 , RMSE train = 1.3996403952818925
iter = 160 , loss = 10.89358904052719 , RMSE train = 1.391205098090463
iter = 170 , loss = 9.936938505950446 , RMSE train = 1.3858053826001402
iter = 180 , loss = 9.073862005360816 , RMSE train = 1.3824083794281756
iter = 190 , loss = 8.294696628974057 , RMSE train = 1.3803229600597275
iter = 200 , loss = 7.590944584941066 , RMSE train = 1.3790865908325631
iter = 210 , loss = 6.955080574780304 , RMSE train = 1.3783960978642982
iter = 220 , loss = 6.38040505678337 , RMSE train = 1.3780500421899422
iter = 230 , loss = 5.8609296463223455 , RMSE train = 1.3779166589819747
iter = 240 , loss = 5.3912854435282185 , RMSE train = 1.3779107941764348
iter = 250 , loss = 4.966648058094085 , RMSE train = 1.3779774303675687
iter = 260 , loss = 4.582675068961076 , RMSE train = 1.3780816809653034
iter = 270 , loss = 4.235452965410047 , RMSE train = 1.3782018408827386
iter = 280 , loss = 3.921451492468777 , RMSE train = 1.3783247884603858
iter = 290 , loss = 3.6374839147885947 , RMSE train = 1.3784429395761524
iter = 300 , loss = 3.380672115397124 , RMSE train = 1.37855223855807
iter = 310 , loss = 3.148415722097497 , RMSE train = 1.3786508392928518
iter = 320 , loss = 2.9383646463626647 , RMSE train = 1.3787382445139136
iter = 330 , loss = 2.7483945549148046 , RMSE train = 1.3788147485866067
iter = 340 , loss = 2.576584891076796 , RMSE train = 1.3788810807492533
iter = 350 , loss = 2.4211991336451733 , RMSE train = 1.3789381802379999

User-based MF, RMSE = 1.4467732126402555


User-based MF-5models, RMSE = 1.2500460931787876


################################################################################
user-based = 1 ( ~ user-based)
################################################################################

K = 50, lam = .5, learning_rate = 0.75, max_iter = 350, user_based = 1

iter = 10 , loss = 3.2344358110834825 , RMSE train = 1.4030579250985449
iter = 20 , loss = 1.0047637216357164 , RMSE train = 1.40306700687197
iter = 30 , loss = 0.9844846483226469 , RMSE train = 1.4030670084225203
iter = 40 , loss = 0.9843002079762584 , RMSE train = 1.4030670084227153
iter = 50 , loss = 0.9842985304595933 , RMSE train = 1.4030670084227153
iter = 60 , loss = 0.9842985152021784 , RMSE train = 1.4030670084227153
iter = 70 , loss = 0.9842985150634075 , RMSE train = 1.4030670084227153
iter = 80 , loss = 0.9842985150621453 , RMSE train = 1.4030670084227153
iter = 90 , loss = 0.9842985150621338 , RMSE train = 1.4030670084227153
iter = 100 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 110 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 120 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 130 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 140 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 150 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 160 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 170 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 180 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 190 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 200 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 210 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 220 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 230 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 240 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 250 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 260 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 270 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 280 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 290 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 300 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 310 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 320 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 330 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 340 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153
iter = 350 , loss = 0.9842985150621337 , RMSE train = 1.4030670084227153

User-based MF, RMSE = 1.4621422080122954


iter = 10 , loss = 3.0416826754663715 , RMSE train = 1.2530393729940197
iter = 20 , loss = 0.8055923425307062 , RMSE train = 1.2530508356656245
iter = 30 , loss = 0.7852548698770515 , RMSE train = 1.2530508374393423
iter = 40 , loss = 0.7850698983921601 , RMSE train = 1.2530508374394813
iter = 50 , loss = 0.7850682160456675 , RMSE train = 1.2530508374394813
iter = 60 , loss = 0.7850682007443402 , RMSE train = 1.2530508374394813
iter = 70 , loss = 0.7850682006051701 , RMSE train = 1.2530508374394813
iter = 80 , loss = 0.7850682006039044 , RMSE train = 1.2530508374394813
iter = 90 , loss = 0.7850682006038928 , RMSE train = 1.2530508374394813
iter = 100 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 110 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 120 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 130 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 140 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 150 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 160 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 170 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 180 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 190 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 200 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 210 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 220 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 230 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 240 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 250 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 260 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 270 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 280 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 290 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 300 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 310 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 320 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 330 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 340 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813
iter = 350 , loss = 0.7850682006038927 , RMSE train = 1.2530508374394813

User-based MF, RMSE = 1.3178213441952316


iter = 10 , loss = 3.4394193030443976 , RMSE train = 1.5382111909060658
iter = 20 , loss = 1.2035843122218486 , RMSE train = 1.5382213567747154
iter = 30 , loss = 1.1832491226299788 , RMSE train = 1.538221358435825
iter = 40 , loss = 1.18306417138432 , RMSE train = 1.5382213584360667
iter = 50 , loss = 1.1830624892146906 , RMSE train = 1.5382213584360664
iter = 60 , loss = 1.1830624739148834 , RMSE train = 1.5382213584360664
iter = 70 , loss = 1.1830624737757263 , RMSE train = 1.5382213584360664
iter = 80 , loss = 1.1830624737744604 , RMSE train = 1.5382213584360664
iter = 90 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 100 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 110 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 120 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 130 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 140 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 150 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 160 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 170 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 180 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 190 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 200 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 210 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 220 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 230 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 240 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 250 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 260 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 270 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 280 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 290 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 300 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 310 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 320 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 330 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 340 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664
iter = 350 , loss = 1.1830624737744488 , RMSE train = 1.5382213584360664

User-based MF, RMSE = 1.6035522323941516


iter = 10 , loss = 3.393662705244438 , RMSE train = 1.5087960128528501
iter = 20 , loss = 1.1587613874228295 , RMSE train = 1.5088062274401028
iter = 30 , loss = 1.1384346896027233 , RMSE train = 1.5088062291654412
iter = 40 , loss = 1.1382498154882679 , RMSE train = 1.5088062291656723
iter = 50 , loss = 1.1382481340184119 , RMSE train = 1.5088062291656728
iter = 60 , loss = 1.138248118724945 , RMSE train = 1.5088062291656728
iter = 70 , loss = 1.138248118585845 , RMSE train = 1.5088062291656728
iter = 80 , loss = 1.1382481185845799 , RMSE train = 1.5088062291656728
iter = 90 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 100 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 110 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 120 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 130 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 140 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 150 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 160 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 170 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 180 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 190 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 200 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 210 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 220 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 230 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 240 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 250 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 260 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 270 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 280 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 290 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 300 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 310 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 320 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 330 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 340 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728
iter = 350 , loss = 1.1382481185845683 , RMSE train = 1.5088062291656728

User-based MF, RMSE = 1.5927882830492057


iter = 10 , loss = 3.207986285408148 , RMSE train = 1.3792416972411252
iter = 20 , loss = 0.9716929933670712 , RMSE train = 1.3792512502667271
iter = 30 , loss = 0.9513536939140241 , RMSE train = 1.3792512518543103
iter = 40 , loss = 0.9511687058125973 , RMSE train = 1.3792512518544602
iter = 50 , loss = 0.9511670233142304 , RMSE train = 1.3792512518544602
iter = 60 , loss = 0.9511670080115083 , RMSE train = 1.3792512518544602
iter = 70 , loss = 0.9511670078723253 , RMSE train = 1.3792512518544602
iter = 80 , loss = 0.9511670078710595 , RMSE train = 1.3792512518544602
iter = 90 , loss = 0.9511670078710479 , RMSE train = 1.3792512518544602
iter = 100 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 110 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 120 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 130 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 140 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 150 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 160 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 170 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 180 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 190 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 200 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 210 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 220 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 230 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 240 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 250 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 260 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 270 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 280 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 290 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 300 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 310 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 320 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 330 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 340 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602
iter = 350 , loss = 0.9511670078710478 , RMSE train = 1.3792512518544602

User-based MF, RMSE = 1.4469342827320701


User-based MF-5models, RMSE = 1.250198828494739


################################################################################
user-based = 0 ( ~ item-based)
################################################################################

K = 50, lam = .1, learning_rate = 0.75, max_iter = 350, user_based = 0

iter = 10 , loss = 24.096722517505555 , RMSE train = 1.6130697544813795
iter = 20 , loss = 11.003053654121855 , RMSE train = 1.0838444782373085
iter = 30 , loss = 5.33123505808014 , RMSE train = 1.0536885908819187
iter = 40 , loss = 2.7442525434766476 , RMSE train = 1.0526462219973591
iter = 50 , loss = 1.5584875721461993 , RMSE train = 1.052700268966099
iter = 60 , loss = 1.014723607387887 , RMSE train = 1.052731584755556
iter = 70 , loss = 0.7653566550482747 , RMSE train = 1.0527407123156154
iter = 80 , loss = 0.650998874691344 , RMSE train = 1.0527430937905302
iter = 90 , loss = 0.5985555479506109 , RMSE train = 1.0527436886880126
iter = 100 , loss = 0.574505641296033 , RMSE train = 1.052743833477713
iter = 110 , loss = 0.5634766451923835 , RMSE train = 1.0527438680705767
iter = 120 , loss = 0.5584188819909333 , RMSE train = 1.0527438762191095
iter = 130 , loss = 0.5560994523143941 , RMSE train = 1.0527438781170846
iter = 140 , loss = 0.5550357887201358 , RMSE train = 1.052743878555127
iter = 150 , loss = 0.55454800440012 , RMSE train = 1.0527438786554684
iter = 160 , loss = 0.5543243116656629 , RMSE train = 1.0527438786783019
iter = 170 , loss = 0.5542217284169216 , RMSE train = 1.0527438786834797
iter = 180 , loss = 0.5541746847072335 , RMSE train = 1.0527438786846393
iter = 190 , loss = 0.5541531108786276 , RMSE train = 1.0527438786849108
iter = 200 , loss = 0.5541432172992078 , RMSE train = 1.0527438786849688
iter = 210 , loss = 0.5541386801796322 , RMSE train = 1.0527438786849888
iter = 220 , loss = 0.5541365994887036 , RMSE train = 1.052743878684989
iter = 230 , loss = 0.554135645297321 , RMSE train = 1.0527438786849903
iter = 240 , loss = 0.5541352077107545 , RMSE train = 1.0527438786849905
iter = 250 , loss = 0.5541350070358566 , RMSE train = 1.0527438786849908
iter = 260 , loss = 0.5541349150072805 , RMSE train = 1.0527438786849908
iter = 270 , loss = 0.554134872803346 , RMSE train = 1.0527438786849908
iter = 280 , loss = 0.5541348534487649 , RMSE train = 1.0527438786849908
iter = 290 , loss = 0.5541348445728077 , RMSE train = 1.0527438786849908
iter = 300 , loss = 0.5541348405023127 , RMSE train = 1.0527438786849908
iter = 310 , loss = 0.5541348386355897 , RMSE train = 1.0527438786849908
iter = 320 , loss = 0.5541348377795121 , RMSE train = 1.0527438786849908
iter = 330 , loss = 0.554134837386915 , RMSE train = 1.0527438786849908
iter = 340 , loss = 0.5541348372068698 , RMSE train = 1.0527438786849908
iter = 350 , loss = 0.554134837124301 , RMSE train = 1.0527438786849908

Item-based MF, RMSE = 1.8982906358251148


iter = 10 , loss = 23.974929666496436 , RMSE train = 1.533995209658462
iter = 20 , loss = 10.943144217299281 , RMSE train = 1.0254015615344492
iter = 30 , loss = 5.274191838554711 , RMSE train = 0.9972475385933165
iter = 40 , loss = 2.687173123779929 , RMSE train = 0.9965574361582333
iter = 50 , loss = 1.5012650857779009 , RMSE train = 0.99668082221008
iter = 60 , loss = 0.9574163334295397 , RMSE train = 0.9967260558770191
iter = 70 , loss = 0.7080068599827207 , RMSE train = 0.9967379848162091
iter = 80 , loss = 0.593628815469239 , RMSE train = 0.9967409297741879
iter = 90 , loss = 0.5411760190148103 , RMSE train = 0.996741637853412
iter = 100 , loss = 0.5171217234946791 , RMSE train = 0.9967418053390081
iter = 110 , loss = 0.5060907010091632 , RMSE train = 0.9967418444745169
iter = 120 , loss = 0.5010320040470315 , RMSE train = 0.9967418535304341
iter = 130 , loss = 0.49871214461672425 , RMSE train = 0.9967418556092331
iter = 140 , loss = 0.4976482834134618 , RMSE train = 0.9967418560832342
iter = 150 , loss = 0.49716040830060587 , RMSE train = 0.9967418561906888
iter = 160 , loss = 0.49693667388147084 , RMSE train = 0.996741856214946
iter = 170 , loss = 0.4968340715081721 , RMSE train = 0.9967418562203969
iter = 180 , loss = 0.49678701903050765 , RMSE train = 0.9967418562216062
iter = 190 , loss = 0.4967654411849042 , RMSE train = 0.9967418562218857
iter = 200 , loss = 0.49675554576642045 , RMSE train = 0.9967418562219468
iter = 210 , loss = 0.4967510078054694 , RMSE train = 0.9967418562219581
iter = 220 , loss = 0.49674892672989523 , RMSE train = 0.9967418562219608
iter = 230 , loss = 0.4967479723627896 , RMSE train = 0.9967418562219615
iter = 240 , loss = 0.496747534696004 , RMSE train = 0.9967418562219614
iter = 250 , loss = 0.496747333984513 , RMSE train = 0.9967418562219615
iter = 260 , loss = 0.4967472419392569 , RMSE train = 0.9967418562219615
iter = 270 , loss = 0.49674719972772496 , RMSE train = 0.9967418562219615
iter = 280 , loss = 0.4967471803696864 , RMSE train = 0.9967418562219615
iter = 290 , loss = 0.4967471714921568 , RMSE train = 0.9967418562219615
iter = 300 , loss = 0.4967471674209473 , RMSE train = 0.9967418562219615
iter = 310 , loss = 0.4967471655539 , RMSE train = 0.9967418562219615
iter = 320 , loss = 0.4967471646976753 , RMSE train = 0.9967418562219615
iter = 330 , loss = 0.49674716430501165 , RMSE train = 0.9967418562219615
iter = 340 , loss = 0.4967471641249363 , RMSE train = 0.9967418562219615
iter = 350 , loss = 0.4967471640423537 , RMSE train = 0.9967418562219615

Item-based MF, RMSE = 1.904790213058515


iter = 10 , loss = 24.217903870685298 , RMSE train = 1.6728772398991474
iter = 20 , loss = 11.161158892199149 , RMSE train = 1.2164652573785344
iter = 30 , loss = 5.491062281089196 , RMSE train = 1.193676020098838
iter = 40 , loss = 2.9036068677912503 , RMSE train = 1.1934986784974166
iter = 50 , loss = 1.717407581402903 , RMSE train = 1.1937076157795838
iter = 60 , loss = 1.1733966315770732 , RMSE train = 1.1937712108705305
iter = 70 , loss = 0.9239050762828755 , RMSE train = 1.1937873041774285
iter = 80 , loss = 0.8094872847378801 , RMSE train = 1.1937911992167909
iter = 90 , loss = 0.757015635634877 , RMSE train = 1.1937921233630877
iter = 100 , loss = 0.732952484080103 , RMSE train = 1.193792339775981
iter = 110 , loss = 0.7219173192871394 , RMSE train = 1.1937923899467109
iter = 120 , loss = 0.7168566879734694 , RMSE train = 1.1937924014828338
iter = 130 , loss = 0.7145359255471462 , RMSE train = 1.1937924041173564
iter = 140 , loss = 0.7134716426792002 , RMSE train = 1.1937924047155408
iter = 150 , loss = 0.7129835705481931 , RMSE train = 1.193792404850679
iter = 160 , loss = 0.712759744008543 , RMSE train = 1.1937924048810828
iter = 170 , loss = 0.7126570985295433 , RMSE train = 1.1937924048879007
iter = 180 , loss = 0.7126100258663304 , RMSE train = 1.1937924048894342
iter = 190 , loss = 0.7125884385612529 , RMSE train = 1.1937924048897692
iter = 200 , loss = 0.7125785387067108 , RMSE train = 1.1937924048898494
iter = 210 , loss = 0.7125739986640568 , RMSE train = 1.1937924048898714
iter = 220 , loss = 0.7125719166109766 , RMSE train = 1.193792404889875
iter = 230 , loss = 0.7125709617845887 , RMSE train = 1.1937924048898756
iter = 240 , loss = 0.712570523901887 , RMSE train = 1.1937924048898754
iter = 250 , loss = 0.7125703230888362 , RMSE train = 1.1937924048898754
iter = 260 , loss = 0.7125702309957859 , RMSE train = 1.1937924048898754
iter = 270 , loss = 0.7125701887617513 , RMSE train = 1.1937924048898754
iter = 280 , loss = 0.7125701693931134 , RMSE train = 1.1937924048898754
iter = 290 , loss = 0.7125701605105891 , RMSE train = 1.1937924048898754
iter = 300 , loss = 0.712570156437025 , RMSE train = 1.1937924048898754
iter = 310 , loss = 0.7125701545688674 , RMSE train = 1.1937924048898754
iter = 320 , loss = 0.712570153712119 , RMSE train = 1.1937924048898754
iter = 330 , loss = 0.7125701533192081 , RMSE train = 1.1937924048898754
iter = 340 , loss = 0.7125701531390162 , RMSE train = 1.1937924048898754
iter = 350 , loss = 0.7125701530563785 , RMSE train = 1.1937924048898754

Item-based MF, RMSE = 2.054653760470441


iter = 10 , loss = 24.198588682842654 , RMSE train = 1.6494409318602163
iter = 20 , loss = 11.148760753814923 , RMSE train = 1.1921331665997708
iter = 30 , loss = 5.469425012794867 , RMSE train = 1.1688141496726077
iter = 40 , loss = 2.8774868953490063 , RMSE train = 1.168381545564428
iter = 50 , loss = 1.6892915268286226 , RMSE train = 1.1685295219467855
iter = 60 , loss = 1.1443840128685183 , RMSE train = 1.1685797086954781
iter = 70 , loss = 0.8944859401694492 , RMSE train = 1.1685929009348583
iter = 80 , loss = 0.7798828827809189 , RMSE train = 1.1685961704015977
iter = 90 , loss = 0.7273265901711672 , RMSE train = 1.1685969597277923
iter = 100 , loss = 0.7032247217185791 , RMSE train = 1.1685971470836154
iter = 110 , loss = 0.6921718379865917 , RMSE train = 1.1685971909907553
iter = 120 , loss = 0.6871030957300512 , RMSE train = 1.1685972011764183
iter = 130 , loss = 0.6847786201989372 , RMSE train = 1.1685972035197183
iter = 140 , loss = 0.6837126375055322 , RMSE train = 1.1685972040551005
iter = 150 , loss = 0.6832237872347785 , RMSE train = 1.1685972041766994
iter = 160 , loss = 0.682999604498784 , RMSE train = 1.168597204204175
iter = 170 , loss = 0.6828967959791442 , RMSE train = 1.1685972042103605
iter = 180 , loss = 0.6828496486925942 , RMSE train = 1.1685972042117387
iter = 190 , loss = 0.6828280272349216 , RMSE train = 1.168597204212058
iter = 200 , loss = 0.6828181117509488 , RMSE train = 1.1685972042121227
iter = 210 , loss = 0.6828135645562444 , RMSE train = 1.1685972042121398
iter = 220 , loss = 0.6828114792306227 , RMSE train = 1.1685972042121437
iter = 230 , loss = 0.6828105229069421 , RMSE train = 1.1685972042121449
iter = 240 , loss = 0.6828100843392351 , RMSE train = 1.1685972042121449
iter = 250 , loss = 0.6828098832128227 , RMSE train = 1.1685972042121449
iter = 260 , loss = 0.6828097909764343 , RMSE train = 1.1685972042121449
iter = 270 , loss = 0.68280974867684 , RMSE train = 1.1685972042121449
iter = 280 , loss = 0.6828097292782188 , RMSE train = 1.1685972042121449
iter = 290 , loss = 0.6828097203819833 , RMSE train = 1.1685972042121449
iter = 300 , loss = 0.6828097163021498 , RMSE train = 1.1685972042121449
iter = 310 , loss = 0.6828097144311257 , RMSE train = 1.1685972042121449
iter = 320 , loss = 0.6828097135730667 , RMSE train = 1.1685972042121449
iter = 330 , loss = 0.6828097131795569 , RMSE train = 1.1685972042121449
iter = 340 , loss = 0.6828097129990911 , RMSE train = 1.1685972042121449
iter = 350 , loss = 0.6828097129163283 , RMSE train = 1.1685972042121449

Item-based MF, RMSE = 2.025752779827279


iter = 10 , loss = 24.147888985203046 , RMSE train = 1.6494893274681257
iter = 20 , loss = 11.047098940362398 , RMSE train = 1.1212721978980527
iter = 30 , loss = 5.3728891939558086 , RMSE train = 1.091288001970641
iter = 40 , loss = 2.785024089533101 , RMSE train = 1.0901310225742242
iter = 50 , loss = 1.5988978750834972 , RMSE train = 1.0901556704133986
iter = 60 , loss = 1.0549768133294235 , RMSE train = 1.0901808752033404
iter = 70 , loss = 0.8055395080967462 , RMSE train = 1.0901887846404121
iter = 80 , loss = 0.6911498092335191 , RMSE train = 1.090190926100938
iter = 90 , loss = 0.6386919201754624 , RMSE train = 1.090191473953422
iter = 100 , loss = 0.6146353534603846 , RMSE train = 1.0901916095593878
iter = 110 , loss = 0.6036033078859538 , RMSE train = 1.0901916423668894
iter = 120 , loss = 0.5985441475294134 , RMSE train = 1.0901916501699351
iter = 130 , loss = 0.5962240774542004 , RMSE train = 1.090191652001392
iter = 140 , loss = 0.595160120224962 , RMSE train = 1.090191652426737
iter = 150 , loss = 0.594672201224738 , RMSE train = 1.0901916525246393
iter = 160 , loss = 0.5944484466982468 , RMSE train = 1.0901916525470265
iter = 170 , loss = 0.5943458350904443 , RMSE train = 1.0901916525521045
iter = 180 , loss = 0.5942987783617388 , RMSE train = 1.0901916525532656
iter = 190 , loss = 0.5942771985546231 , RMSE train = 1.0901916525535262
iter = 200 , loss = 0.5942673022290236 , RMSE train = 1.0901916525535906
iter = 210 , loss = 0.5942627638476494 , RMSE train = 1.090191652553611
iter = 220 , loss = 0.5942606825767988 , RMSE train = 1.0901916525536126
iter = 230 , loss = 0.5942597281188106 , RMSE train = 1.0901916525536133
iter = 240 , loss = 0.594259290409646 , RMSE train = 1.0901916525536133
iter = 250 , loss = 0.5942590896783574 , RMSE train = 1.0901916525536133
iter = 260 , loss = 0.5942589976238369 , RMSE train = 1.0901916525536133
iter = 270 , loss = 0.5942589554079628 , RMSE train = 1.0901916525536133
iter = 280 , loss = 0.5942589360478859 , RMSE train = 1.0901916525536133
iter = 290 , loss = 0.5942589271693983 , RMSE train = 1.0901916525536133
iter = 300 , loss = 0.594258923097738 , RMSE train = 1.0901916525536133
iter = 310 , loss = 0.5942589212304783 , RMSE train = 1.0901916525536133
iter = 320 , loss = 0.5942589203741534 , RMSE train = 1.0901916525536133
iter = 330 , loss = 0.5942589199814424 , RMSE train = 1.0901916525536133
iter = 340 , loss = 0.5942589198013447 , RMSE train = 1.0901916525536133
iter = 350 , loss = 0.5942589197187516 , RMSE train = 1.0901916525536133

Item-based MF, RMSE = 1.9360100375142952

Item-based MF-5models, RMSE = 2.0254322911240505


