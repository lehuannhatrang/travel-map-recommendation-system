 
################################################################################
user-based = 1 ( ~ user-based)
################################################################################

K = 50, lam = .1, learning_rate = 0.75, max_iter = 300, user_based = 1

iter = 10 , loss = 24.08304724379804 , RMSE train = 1.6745440841262909
iter = 20 , loss = 11.404329610187379 , RMSE train = 1.4272323869933212
iter = 30 , loss = 5.78289123183696 , RMSE train = 1.4280469550191937
iter = 40 , loss = 3.2108918308564327 , RMSE train = 1.4324631736998177
iter = 50 , loss = 2.030629712602268 , RMSE train = 1.4340427019718966
iter = 60 , loss = 1.4889181005389007 , RMSE train = 1.434535226671419
iter = 70 , loss = 1.240299120470346 , RMSE train = 1.4346857209072583
iter = 80 , loss = 1.1261937183503514 , RMSE train = 1.43473214070844
iter = 90 , loss = 1.0738171605237832 , RMSE train = 1.4347467625285095
iter = 100 , loss = 1.0497681743112013 , RMSE train = 1.4347514869917248
iter = 110 , loss = 1.038720038336624 , RMSE train = 1.4347530535742274
iter = 120 , loss = 1.0336398256126602 , RMSE train = 1.4347535856384745
iter = 130 , loss = 1.0313000947338684 , RMSE train = 1.434753770149821
iter = 140 , loss = 1.030219516954175 , RMSE train = 1.434753835255279
iter = 150 , loss = 1.0297180281887972 , RMSE train = 1.4347538585529507
iter = 160 , loss = 1.0294833271265647 , RMSE train = 1.434753866983617
iter = 170 , loss = 1.029371955156145 , RMSE train = 1.434753870061445
iter = 180 , loss = 1.0293179783653426 , RMSE train = 1.4347538711929104
iter = 190 , loss = 1.029291040405625 , RMSE train = 1.4347538716111803
iter = 200 , loss = 1.0292770979355905 , RMSE train = 1.4347538717664514
iter = 210 , loss = 1.0292695882600162 , RMSE train = 1.4347538718242763
iter = 220 , loss = 1.029265387405157 , RMSE train = 1.4347538718458843
iter = 230 , loss = 1.0292629629228667 , RMSE train = 1.4347538718539492
iter = 240 , loss = 1.0292615312953666 , RMSE train = 1.4347538718569552
iter = 250 , loss = 1.0292606728951597 , RMSE train = 1.4347538718580921
iter = 260 , loss = 1.0292601531980952 , RMSE train = 1.4347538718585926
iter = 270 , loss = 1.0292598366935743 , RMSE train = 1.4347538718587793
iter = 280 , loss = 1.0292596432475283 , RMSE train = 1.4347538718588542
iter = 290 , loss = 1.0292595247590057 , RMSE train = 1.4347538718588642
iter = 300 , loss = 1.0292594520873568 , RMSE train = 1.4347538718588653

User-based MF, RMSE = 1.3436716545329384


iter = 10 , loss = 23.998868598581613 , RMSE train = 1.585896747610936
iter = 20 , loss = 11.229029147291786 , RMSE train = 1.280022908237969
iter = 30 , loss = 5.583877628691546 , RMSE train = 1.2726840333189686
iter = 40 , loss = 3.003428435961127 , RMSE train = 1.2756688854738913
iter = 50 , loss = 1.819875135892071 , RMSE train = 1.2769622596279901
iter = 60 , loss = 1.27681064221837 , RMSE train = 1.2773993398526877
iter = 70 , loss = 1.0276162788832726 , RMSE train = 1.2775406397039752
iter = 80 , loss = 0.9132614177209308 , RMSE train = 1.2775862223564647
iter = 90 , loss = 0.8607756885548357 , RMSE train = 1.2776010967782012
iter = 100 , loss = 0.8366787873164091 , RMSE train = 1.2776060266025748
iter = 110 , loss = 0.8256096934612774 , RMSE train = 1.2776076858896266
iter = 120 , loss = 0.8205204673270999 , RMSE train = 1.2776082519465985
iter = 130 , loss = 0.8181770833741987 , RMSE train = 1.2776084471758642
iter = 140 , loss = 0.8170953250633628 , RMSE train = 1.27760851508137
iter = 150 , loss = 0.816593849758796 , RMSE train = 1.277608538851079
iter = 160 , loss = 0.8163597606180463 , RMSE train = 1.2776085472104342
iter = 170 , loss = 0.8162492824732875 , RMSE train = 1.2776085501602743
iter = 180 , loss = 0.8161962846595073 , RMSE train = 1.2776085512037703
iter = 190 , loss = 0.8161702836431428 , RMSE train = 1.2776085515735491
iter = 200 , loss = 0.8161571619166861 , RMSE train = 1.2776085517047788
iter = 210 , loss = 0.8161503235104496 , RMSE train = 1.2776085517513884
iter = 220 , loss = 0.8161466411504644 , RMSE train = 1.2776085517679854
iter = 230 , loss = 0.8161445987706412 , RMSE train = 1.2776085517738518
iter = 240 , loss = 0.8161434386323487 , RMSE train = 1.2776085517759292
iter = 250 , loss = 0.8161427679972684 , RMSE train = 1.2776085517766658
iter = 260 , loss = 0.8161423756668192 , RMSE train = 1.2776085517769147
iter = 270 , loss = 0.816142144361207 , RMSE train = 1.277608551777002
iter = 280 , loss = 0.8161420073244039 , RMSE train = 1.2776085517770706
iter = 290 , loss = 0.8161419258926719 , RMSE train = 1.2776085517770899
iter = 300 , loss = 0.8161418774144612 , RMSE train = 1.2776085517770936

User-based MF, RMSE = 1.236919300994522


iter = 10 , loss = 24.340578663067685 , RMSE train = 1.7934758706224017
iter = 20 , loss = 11.605292299477975 , RMSE train = 1.5497112322779525
iter = 30 , loss = 5.974624423218817 , RMSE train = 1.5509367864299226
iter = 40 , loss = 3.399275731870249 , RMSE train = 1.5557749491266748
iter = 50 , loss = 2.217463907446004 , RMSE train = 1.5575846939858242
iter = 60 , loss = 1.674991402439509 , RMSE train = 1.5581811880713083
iter = 70 , loss = 1.425991708294655 , RMSE train = 1.5583756899490857
iter = 80 , loss = 1.3116918418517178 , RMSE train = 1.55844028481184
iter = 90 , loss = 1.2592121665911735 , RMSE train = 1.5584623416370307
iter = 100 , loss = 1.2351049627761117 , RMSE train = 1.5584700975015096
iter = 110 , loss = 1.224020797710194 , RMSE train = 1.5584728993107186
iter = 120 , loss = 1.2189155691094178 , RMSE train = 1.5584739350061765
iter = 130 , loss = 1.216556281674994 , RMSE train = 1.5584743250824362
iter = 140 , loss = 1.2154589105554265 , RMSE train = 1.5584744741930434
iter = 150 , loss = 1.2149423043564538 , RMSE train = 1.5584745318586124
iter = 160 , loss = 1.2146940106037887 , RMSE train = 1.5584745543633254
iter = 170 , loss = 1.2145708122690966 , RMSE train = 1.5584745632090309
iter = 180 , loss = 1.214507004595174 , RMSE train = 1.5584745667056106
iter = 190 , loss = 1.2144722810477728 , RMSE train = 1.5584745680940104
iter = 200 , loss = 1.214452459539974 , RMSE train = 1.558474568647268
iter = 210 , loss = 1.2144406975830435 , RMSE train = 1.5584745688684019
iter = 220 , loss = 1.214433526088516 , RMSE train = 1.558474568956979
iter = 230 , loss = 1.2144290778075062 , RMSE train = 1.5584745689925164
iter = 240 , loss = 1.2144262903268417 , RMSE train = 1.5584745690068018
iter = 250 , loss = 1.2144245331983836 , RMSE train = 1.5584745690125468
iter = 260 , loss = 1.214423421766578 , RMSE train = 1.5584745690148643
iter = 270 , loss = 1.214422717338177 , RMSE train = 1.5584745690158237
iter = 280 , loss = 1.214422270323799 , RMSE train = 1.5584745690161808
iter = 290 , loss = 1.2144219864394432 , RMSE train = 1.5584745690163226
iter = 300 , loss = 1.214421806061432 , RMSE train = 1.5584745690164203

User-based MF, RMSE = 1.5339523982158612


iter = 10 , loss = 24.350819253235365 , RMSE train = 1.8043066920165272
iter = 20 , loss = 11.59251530791665 , RMSE train = 1.5415104884152102
iter = 30 , loss = 5.959373995251136 , RMSE train = 1.5410736469716924
iter = 40 , loss = 3.383887990151547 , RMSE train = 1.5457906492281994
iter = 50 , loss = 2.202255055160857 , RMSE train = 1.547682533536145
iter = 60 , loss = 1.659938161624809 , RMSE train = 1.5483523290002368
iter = 70 , loss = 1.4110359827832688 , RMSE train = 1.5485900632295035
iter = 80 , loss = 1.2967916631612117 , RMSE train = 1.548677152530822
iter = 90 , loss = 1.2443417362303406 , RMSE train = 1.5487102854139954
iter = 100 , loss = 1.2202479463008145 , RMSE train = 1.548723341837012
iter = 110 , loss = 1.2091649737677637 , RMSE train = 1.548728639200862
iter = 120 , loss = 1.2040496660865954 , RMSE train = 1.548730837901677
iter = 130 , loss = 1.2016691892690483 , RMSE train = 1.548731766244393
iter = 140 , loss = 1.2005411787559703 , RMSE train = 1.5487321632276547
iter = 150 , loss = 1.1999882913401878 , RMSE train = 1.5487323345945248
iter = 160 , loss = 1.1997026755859723 , RMSE train = 1.5487324090892645
iter = 170 , loss = 1.1995450220508608 , RMSE train = 1.548732441643453
iter = 180 , loss = 1.199452109117685 , RMSE train = 1.5487324559263007
iter = 190 , loss = 1.1993944905305238 , RMSE train = 1.5487324622117151
iter = 200 , loss = 1.199357571707953 , RMSE train = 1.5487324649841778
iter = 210 , loss = 1.199333473118521 , RMSE train = 1.5487324662092716
iter = 220 , loss = 1.199317586887973 , RMSE train = 1.5487324667513633
iter = 230 , loss = 1.1993070606099223 , RMSE train = 1.5487324669914844
iter = 240 , loss = 1.1993000672127938 , RMSE train = 1.5487324670979608
iter = 250 , loss = 1.1992954143446353 , RMSE train = 1.5487324671451992
iter = 260 , loss = 1.1992923162270162 , RMSE train = 1.5487324671661615
iter = 270 , loss = 1.1992902523806261 , RMSE train = 1.5487324671754803
iter = 280 , loss = 1.1992888771265668 , RMSE train = 1.5487324671795966
iter = 290 , loss = 1.1992879605436435 , RMSE train = 1.5487324671814267
iter = 300 , loss = 1.199287349575672 , RMSE train = 1.5487324671822336

User-based MF, RMSE = 1.440247437209668


iter = 10 , loss = 24.178677382243656 , RMSE train = 1.692237019671494
iter = 20 , loss = 11.402696260993848 , RMSE train = 1.4044957277200179
iter = 30 , loss = 5.756798469209335 , RMSE train = 1.3997723659279022
iter = 40 , loss = 3.175571652177375 , RMSE train = 1.4031162208526602
iter = 50 , loss = 1.9914562799265108 , RMSE train = 1.4044496337746128
iter = 60 , loss = 1.448066422261147 , RMSE train = 1.4048792636999934
iter = 70 , loss = 1.1987020173886405 , RMSE train = 1.4050115084796022
iter = 80 , loss = 1.0842635808125063 , RMSE train = 1.4050518539837973
iter = 90 , loss = 1.0317391731888361 , RMSE train = 1.4050642156023179
iter = 100 , loss = 1.0076261200779357 , RMSE train = 1.4050680400100621
iter = 110 , loss = 0.9965519216015081 , RMSE train = 1.4050692368258673
iter = 120 , loss = 0.9914628577383962 , RMSE train = 1.4050696156178266
iter = 130 , loss = 0.989122009523819 , RMSE train = 1.4050697367346772
iter = 140 , loss = 0.9880437125754252 , RMSE train = 1.4050697758002015
iter = 150 , loss = 0.9875458953654868 , RMSE train = 1.4050697884913979
iter = 160 , loss = 0.9873152857520457 , RMSE train = 1.40506979263831
iter = 170 , loss = 0.9872079087191203 , RMSE train = 1.4050697939995527
iter = 180 , loss = 0.9871575325433236 , RMSE train = 1.4050697944480515
iter = 190 , loss = 0.9871336437621391 , RMSE train = 1.405069794596238
iter = 200 , loss = 0.9871221503835902 , RMSE train = 1.405069794645244
iter = 210 , loss = 0.9871165179815206 , RMSE train = 1.4050697946614998
iter = 220 , loss = 0.9871136967444201 , RMSE train = 1.4050697946668955
iter = 230 , loss = 0.9871122490956125 , RMSE train = 1.4050697946686967
iter = 240 , loss = 0.987111487842717 , RMSE train = 1.4050697946692947
iter = 250 , loss = 0.9871110782970491 , RMSE train = 1.4050697946694697
iter = 260 , loss = 0.987110853626834 , RMSE train = 1.4050697946695259
iter = 270 , loss = 0.9871107284572368 , RMSE train = 1.405069794669549
iter = 280 , loss = 0.9871106579149845 , RMSE train = 1.405069794669556
iter = 290 , loss = 0.9871106178326531 , RMSE train = 1.4050697946695565
iter = 300 , loss = 0.9871105949290382 , RMSE train = 1.4050697946695565

User-based MF, RMSE = 1.3521430315611527



################################################################################
user-based = 0 ( ~ item-based)
################################################################################


K = 50, lam = .1, learning_rate = 0.75, max_iter = 300, user_based = 1

iter = 10 , loss = 23.780212550962073 , RMSE train = 1.4453437090032013
iter = 20 , loss = 10.986598434097928 , RMSE train = 1.0809696300873755
iter = 30 , loss = 5.338133205932987 , RMSE train = 1.065861621815687
iter = 40 , loss = 2.756413805547913 , RMSE train = 1.0665803836740804
iter = 50 , loss = 1.5725825615275135 , RMSE train = 1.067032012180871
iter = 60 , loss = 1.0295819815471439 , RMSE train = 1.067168167711233
iter = 70 , loss = 0.7805173670564638 , RMSE train = 1.0672047017121284
iter = 80 , loss = 0.666277421892721 , RMSE train = 1.0672140720210457
iter = 90 , loss = 0.6138782956267393 , RMSE train = 1.0672164141621212
iter = 100 , loss = 0.5898437499296171 , RMSE train = 1.067216989566913
iter = 110 , loss = 0.5788192770805461 , RMSE train = 1.0672171292007115
iter = 120 , loss = 0.573762277737495 , RMSE train = 1.067217162782137
iter = 130 , loss = 0.5714425149130822 , RMSE train = 1.0672171708048548
iter = 140 , loss = 0.5703783422493068 , RMSE train = 1.0672171727121613
iter = 150 , loss = 0.5698901390449951 , RMSE train = 1.0672171731639704
iter = 160 , loss = 0.5696661578832727 , RMSE train = 1.067217173270737
iter = 170 , loss = 0.5695633923964889 , RMSE train = 1.0672171732959246
iter = 180 , loss = 0.5695162392233837 , RMSE train = 1.06721717330187
iter = 190 , loss = 0.5694946017894457 , RMSE train = 1.0672171733032636
iter = 200 , loss = 0.5694846721012049 , RMSE train = 1.0672171733036029
iter = 210 , loss = 0.5694801148296471 , RMSE train = 1.067217173303671
iter = 220 , loss = 0.5694780230366782 , RMSE train = 1.067217173303691
iter = 230 , loss = 0.5694770627900065 , RMSE train = 1.0672171733036935
iter = 240 , loss = 0.5694766219267331 , RMSE train = 1.0672171733036948
iter = 250 , loss = 0.5694764194898203 , RMSE train = 1.067217173303695
iter = 260 , loss = 0.5694763265185083 , RMSE train = 1.067217173303695
iter = 270 , loss = 0.5694762838122264 , RMSE train = 1.067217173303695
iter = 280 , loss = 0.5694762641908415 , RMSE train = 1.067217173303695
iter = 290 , loss = 0.5694762551735559 , RMSE train = 1.067217173303695
iter = 300 , loss = 0.5694762510283576 , RMSE train = 1.067217173303695

User-based MF, RMSE = 3.778805944296767


iter = 10 , loss = 23.690579324095808 , RMSE train = 1.3876078113209145
iter = 20 , loss = 10.915618480818557 , RMSE train = 1.0175843744202309
iter = 30 , loss = 5.272926202813646 , RMSE train = 1.004858119445867
iter = 40 , loss = 2.6930144479594977 , RMSE train = 1.006307390754742
iter = 50 , loss = 1.5098292874071677 , RMSE train = 1.0069253276385355
iter = 60 , loss = 0.9670867080869405 , RMSE train = 1.0070976765121162
iter = 70 , loss = 0.7181325490608329 , RMSE train = 1.0071420257224784
iter = 80 , loss = 0.60394175624786 , RMSE train = 1.007153082651586
iter = 90 , loss = 0.5515649735800798 , RMSE train = 1.0071557891194487
iter = 100 , loss = 0.5275407164004935 , RMSE train = 1.0071564433097986
iter = 110 , loss = 0.5165210246047984 , RMSE train = 1.0071565999988619
iter = 120 , loss = 0.5114662640858545 , RMSE train = 1.0071566372759773
iter = 130 , loss = 0.509147557191108 , RMSE train = 1.0071566461002937
iter = 140 , loss = 0.5080838861125089 , RMSE train = 1.0071566481816283
iter = 150 , loss = 0.5075959228949123 , RMSE train = 1.0071566486712815
iter = 160 , loss = 0.5073720573986807 , RMSE train = 1.0071566487862913
iter = 170 , loss = 0.5072693480696426 , RMSE train = 1.0071566488132555
iter = 180 , loss = 0.5072222223615465 , RMSE train = 1.0071566488195955
iter = 190 , loss = 0.5072005984558365 , RMSE train = 1.0071566488210921
iter = 200 , loss = 0.5071906754767708 , RMSE train = 1.0071566488214385
iter = 210 , loss = 0.5071861215541077 , RMSE train = 1.0071566488215185
iter = 220 , loss = 0.5071840314427369 , RMSE train = 1.007156648821535
iter = 230 , loss = 0.5071830720450444 , RMSE train = 1.007156648821538
iter = 240 , loss = 0.5071826316124494 , RMSE train = 1.0071566488215382
iter = 250 , loss = 0.5071824293949239 , RMSE train = 1.0071566488215384
iter = 260 , loss = 0.507182336535756 , RMSE train = 1.0071566488215384
iter = 270 , loss = 0.5071822938869586 , RMSE train = 1.0071566488215384
iter = 280 , loss = 0.5071822742951023 , RMSE train = 1.0071566488215384
iter = 290 , loss = 0.5071822652930069 , RMSE train = 1.0071566488215384
iter = 300 , loss = 0.5071822611556289 , RMSE train = 1.0071566488215384

User-based MF, RMSE = 3.832124479310478


iter = 10 , loss = 23.899510546838897 , RMSE train = 1.5225288342143537
iter = 20 , loss = 11.127694648246393 , RMSE train = 1.2059886946001037
iter = 30 , loss = 5.484229346408267 , RMSE train = 1.195805618399199
iter = 40 , loss = 2.904035184396865 , RMSE train = 1.1972867678097516
iter = 50 , loss = 1.7206831272784857 , RMSE train = 1.1979091675012274
iter = 60 , loss = 1.1778334772345866 , RMSE train = 1.1980869761494555
iter = 70 , loss = 0.9288148222417255 , RMSE train = 1.1981338065663278
iter = 80 , loss = 0.8145870363485209 , RMSE train = 1.1981457290802262
iter = 90 , loss = 0.7621895656070745 , RMSE train = 1.1981487054987
iter = 100 , loss = 0.7381538635022147 , RMSE train = 1.1981494391050926
iter = 110 , loss = 0.7271278626013719 , RMSE train = 1.1981496183699853
iter = 120 , loss = 0.7220696250928876 , RMSE train = 1.1981496619268701
iter = 130 , loss = 0.7197490000734212 , RMSE train = 1.1981496724728629
iter = 140 , loss = 0.7186842693340774 , RMSE train = 1.1981496750215443
iter = 150 , loss = 0.7181957198297126 , RMSE train = 1.198149675637174
iter = 160 , loss = 0.7179715294671749 , RMSE train = 1.19814967578595
iter = 170 , loss = 0.7178686398578026 , RMSE train = 1.1981496758219672
iter = 180 , loss = 0.7178214139540252 , RMSE train = 1.198149675830697
iter = 190 , loss = 0.7177997342756113 , RMSE train = 1.1981496758328278
iter = 200 , loss = 0.7177897802019366 , RMSE train = 1.1981496758333505
iter = 210 , loss = 0.7177852089150277 , RMSE train = 1.1981496758334815
iter = 220 , loss = 0.7177831090908489 , RMSE train = 1.1981496758335095
iter = 230 , loss = 0.7177821442512365 , RMSE train = 1.1981496758335173
iter = 240 , loss = 0.7177817007646451 , RMSE train = 1.198149675833519
iter = 250 , loss = 0.7177814968305118 , RMSE train = 1.1981496758335193
iter = 260 , loss = 0.71778140300502 , RMSE train = 1.1981496758335188
iter = 270 , loss = 0.7177813598115022 , RMSE train = 1.1981496758335188
iter = 280 , loss = 0.717781339912207 , RMSE train = 1.1981496758335188
iter = 290 , loss = 0.7177813307364123 , RMSE train = 1.1981496758335188
iter = 300 , loss = 0.7177813265008148 , RMSE train = 1.1981496758335188

User-based MF, RMSE = 4.022729732534637


iter = 10 , loss = 23.911922766350376 , RMSE train = 1.5089713726730771
iter = 20 , loss = 11.133099859079714 , RMSE train = 1.1955804838231279
iter = 30 , loss = 5.479550629492902 , RMSE train = 1.1849962258001263
iter = 40 , loss = 2.894620097695951 , RMSE train = 1.1862508257073103
iter = 50 , loss = 1.7091537575471591 , RMSE train = 1.1868111485629738
iter = 60 , loss = 1.1653566309307681 , RMSE train = 1.1869738427129832
iter = 70 , loss = 0.9159107721419875 , RMSE train = 1.1870172058046875
iter = 80 , loss = 0.8014896115494021 , RMSE train = 1.1870283769978183
iter = 90 , loss = 0.7490044058146267 , RMSE train = 1.187031202452521
iter = 100 , loss = 0.7249288287992959 , RMSE train = 1.187031909346988
iter = 110 , loss = 0.713884668443072 , RMSE train = 1.1870320851217202
iter = 120 , loss = 0.708818135409505 , RMSE train = 1.1870321287096712
iter = 130 , loss = 0.7064937021280321 , RMSE train = 1.187032139516742
iter = 140 , loss = 0.705427209707098 , RMSE train = 1.1870321422012655
iter = 150 , loss = 0.7049378360249431 , RMSE train = 1.1870321428704826
iter = 160 , loss = 0.7047132539519361 , RMSE train = 1.187032143038105
iter = 170 , loss = 0.7046101742406812 , RMSE train = 1.1870321430803417
iter = 180 , loss = 0.7045628536425387 , RMSE train = 1.187032143091043
iter = 190 , loss = 0.7045411253409083 , RMSE train = 1.1870321430937874
iter = 200 , loss = 0.7045311454675036 , RMSE train = 1.1870321430944921
iter = 210 , loss = 0.7045265600347923 , RMSE train = 1.1870321430946718
iter = 220 , loss = 0.7045244522163026 , RMSE train = 1.1870321430947128
iter = 230 , loss = 0.704523482740518 , RMSE train = 1.1870321430947264
iter = 240 , loss = 0.704523036509536 , RMSE train = 1.1870321430947326
iter = 250 , loss = 0.7045228309260717 , RMSE train = 1.1870321430947322
iter = 260 , loss = 0.7045227360990891 , RMSE train = 1.1870321430947322
iter = 270 , loss = 0.7045226922936253 , RMSE train = 1.1870321430947322
iter = 280 , loss = 0.7045226720192661 , RMSE train = 1.1870321430947322
iter = 290 , loss = 0.7045226626134737 , RMSE train = 1.1870321430947322
iter = 300 , loss = 0.7045226582370442 , RMSE train = 1.1870321430947322

User-based MF, RMSE = 3.927702360096446


iter = 10 , loss = 23.76093127411125 , RMSE train = 1.4510027910349628
iter = 20 , loss = 11.003565196274085 , RMSE train = 1.1044802894906662
iter = 30 , loss = 5.361896616767782 , RMSE train = 1.091551181627459
iter = 40 , loss = 2.7827021989628347 , RMSE train = 1.092589430258095
iter = 50 , loss = 1.5999418763922106 , RMSE train = 1.09311231814619
iter = 60 , loss = 1.0574093407023712 , RMSE train = 1.093266077111271
iter = 70 , loss = 0.8085517200088599 , RMSE train = 1.0933070066841426
iter = 80 , loss = 0.6944037315782972 , RMSE train = 1.0933174717804124
iter = 90 , loss = 0.6420454436736747 , RMSE train = 1.0933200859747567
iter = 100 , loss = 0.6180289625765925 , RMSE train = 1.093320728937505
iter = 110 , loss = 0.6070124236021042 , RMSE train = 1.0933208853555754
iter = 120 , loss = 0.6019588724722001 , RMSE train = 1.093320923111702
iter = 130 , loss = 0.599640586567935 , RMSE train = 1.0933209321743416
iter = 140 , loss = 0.5985770336801379 , RMSE train = 1.0933209343410846
iter = 150 , loss = 0.5980890829771827 , RMSE train = 1.0933209348577448
iter = 160 , loss = 0.597865200123885 , RMSE train = 1.0933209349807393
iter = 170 , loss = 0.5977624700883174 , RMSE train = 1.0933209350099804
iter = 180 , loss = 0.5977153278711815 , RMSE train = 1.093320935016936
iter = 190 , loss = 0.5976936925455778 , RMSE train = 1.093320935018595
iter = 200 , loss = 0.5976837622213853 , RMSE train = 1.093320935018992
iter = 210 , loss = 0.5976792037777251 , RMSE train = 1.0933209350190878
iter = 220 , loss = 0.5976771109638246 , RMSE train = 1.093320935019109
iter = 230 , loss = 0.5976761499837029 , RMSE train = 1.0933209350191138
iter = 240 , loss = 0.5976757086385944 , RMSE train = 1.0933209350191144
iter = 250 , loss = 0.5976755059009743 , RMSE train = 1.0933209350191144
iter = 260 , loss = 0.5976754127480776 , RMSE train = 1.0933209350191144
iter = 270 , loss = 0.597675369934608 , RMSE train = 1.0933209350191144
iter = 280 , loss = 0.5976753502509852 , RMSE train = 1.0933209350191144
iter = 290 , loss = 0.5976753411980064 , RMSE train = 1.0933209350191144
iter = 300 , loss = 0.5976753370325338 , RMSE train = 1.0933209350191144

User-based MF, RMSE = 3.8351541852602278


User-based MF-5models, RMSE = 2.1597059723581666



