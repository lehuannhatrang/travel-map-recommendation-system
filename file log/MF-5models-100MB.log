
################################################################################
user-based = 1 ( ~ user-based)
################################################################################

K = 50, lam = 0.1, learning_rate = 0.75, max_iter = 250, user_based = 1
 
iter = 10 , loss = 4.339812519435176 , RMSE train = 1.4736979486369186
iter = 20 , loss = 1.1154896468922135 , RMSE train = 1.4736996918858472
iter = 30 , loss = 1.0861645499701944 , RMSE train = 1.4736996921737961
iter = 40 , loss = 1.0858978393452752 , RMSE train = 1.473699692173843
iter = 50 , loss = 1.0858954136210281 , RMSE train = 1.473699692173843
iter = 60 , loss = 1.0858953915591338 , RMSE train = 1.473699692173843
iter = 70 , loss = 1.0858953913584815 , RMSE train = 1.473699692173843
iter = 80 , loss = 1.0858953913566565 , RMSE train = 1.473699692173843
iter = 90 , loss = 1.08589539135664 , RMSE train = 1.473699692173843
iter = 100 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 110 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 120 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 130 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 140 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 150 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 160 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 170 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 180 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 190 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 200 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 210 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 220 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 230 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 240 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843
iter = 250 , loss = 1.0858953913566398 , RMSE train = 1.473699692173843

User-based MF, RMSE = 1.5465412443099478


iter = 10 , loss = 4.109145379612437 , RMSE train = 1.3107684885035258
iter = 20 , loss = 0.8886191606034806 , RMSE train = 1.3107705756120913
iter = 30 , loss = 0.85932859299557 , RMSE train = 1.310770575916177
iter = 40 , loss = 0.8590621964491076 , RMSE train = 1.310770575916202
iter = 50 , loss = 0.859059773582002 , RMSE train = 1.310770575916202
iter = 60 , loss = 0.8590597515461015 , RMSE train = 1.310770575916202
iter = 70 , loss = 0.8590597513456856 , RMSE train = 1.310770575916202
iter = 80 , loss = 0.8590597513438628 , RMSE train = 1.310770575916202
iter = 90 , loss = 0.8590597513438462 , RMSE train = 1.310770575916202
iter = 100 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 110 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 120 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 130 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 140 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 150 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 160 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 170 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 180 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 190 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 200 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 210 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 220 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 230 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 240 , loss = 0.859059751343846 , RMSE train = 1.310770575916202
iter = 250 , loss = 0.859059751343846 , RMSE train = 1.310770575916202

User-based MF, RMSE = 1.3898050884746602


iter = 10 , loss = 4.583274579405198 , RMSE train = 1.6349383582918848
iter = 20 , loss = 1.3660494594490287 , RMSE train = 1.6349435761010294
iter = 30 , loss = 1.3367888169692945 , RMSE train = 1.6349435766903955
iter = 40 , loss = 1.3365226920975855 , RMSE train = 1.6349435766904672
iter = 50 , loss = 1.3365202716962854 , RMSE train = 1.6349435766904672
iter = 60 , loss = 1.3365202496827595 , RMSE train = 1.6349435766904672
iter = 70 , loss = 1.3365202494825466 , RMSE train = 1.6349435766904672
iter = 80 , loss = 1.3365202494807256 , RMSE train = 1.6349435766904672
iter = 90 , loss = 1.3365202494807091 , RMSE train = 1.6349435766904672
iter = 100 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 110 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 120 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 130 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 140 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 150 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 160 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 170 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 180 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 190 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 200 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 210 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 220 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 230 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 240 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672
iter = 250 , loss = 1.336520249480709 , RMSE train = 1.6349435766904672

User-based MF, RMSE = 1.7181366984223432


iter = 10 , loss = 4.532151832941881 , RMSE train = 1.5992248125092083
iter = 20 , loss = 1.3083558471602632 , RMSE train = 1.599228790715242
iter = 30 , loss = 1.2790354788210867 , RMSE train = 1.599228791200668
iter = 40 , loss = 1.2787688109023607 , RMSE train = 1.599228791200725
iter = 50 , loss = 1.278766385563547 , RMSE train = 1.599228791200725
iter = 60 , loss = 1.2787663635051285 , RMSE train = 1.599228791200725
iter = 70 , loss = 1.2787663633045074 , RMSE train = 1.599228791200725
iter = 80 , loss = 1.2787663633026827 , RMSE train = 1.599228791200725
iter = 90 , loss = 1.2787663633026662 , RMSE train = 1.599228791200725
iter = 100 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 110 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 120 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 130 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 140 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 150 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 160 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 170 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 180 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 190 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 200 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 210 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 220 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 230 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 240 , loss = 1.278766363302666 , RMSE train = 1.599228791200725
iter = 250 , loss = 1.278766363302666 , RMSE train = 1.599228791200725

User-based MF, RMSE = 1.6794718118242986


iter = 10 , loss = 4.322444876034302 , RMSE train = 1.4664025127562401
iter = 20 , loss = 1.1047020443221514 , RMSE train = 1.4664025535001015
iter = 30 , loss = 1.0754368334511113 , RMSE train = 1.46640255364629
iter = 40 , loss = 1.0751706676565593 , RMSE train = 1.4664025536462892
iter = 50 , loss = 1.0751682468890584 , RMSE train = 1.4664025536462895
iter = 60 , loss = 1.0751682248722596 , RMSE train = 1.4664025536462895
iter = 70 , loss = 1.0751682246720173 , RMSE train = 1.4664025536462895
iter = 80 , loss = 1.0751682246701961 , RMSE train = 1.4664025536462895
iter = 90 , loss = 1.0751682246701797 , RMSE train = 1.4664025536462895
iter = 100 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 110 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 120 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 130 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 140 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 150 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 160 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 170 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 180 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 190 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 200 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 210 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 220 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 230 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 240 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895
iter = 250 , loss = 1.0751682246701795 , RMSE train = 1.4664025536462895

User-based MF, RMSE = 1.5479305840092241

User-based MF-5models, RMSE = 1.2769175047531125

################################################################################

K = 50, lam = .1, learning_rate = 0.1, max_iter = 50, user_based = 1

iter = 10 , loss = 82.25591115363142 , RMSE train = 3.7924406681151255
iter = 20 , loss = 70.53528914453184 , RMSE train = 3.527846257938324
iter = 30 , loss = 61.26778692264622 , RMSE train = 3.240869182894181
iter = 40 , loss = 53.767585338529415 , RMSE train = 2.94265617221585
iter = 50 , loss = 47.568556068216296 , RMSE train = 2.6562670202592833
iter = 60 , loss = 42.350042161796765 , RMSE train = 2.3979129567289768
iter = 70 , loss = 37.88837093064494 , RMSE train = 2.176620123648601
iter = 80 , loss = 34.024994011140265 , RMSE train = 1.9942061484430496
iter = 90 , loss = 30.64544254267921 , RMSE train = 1.8489990094483097
iter = 100 , loss = 27.665356608667967 , RMSE train = 1.7381256888163132
iter = 110 , loss = 25.02116442652119 , RMSE train = 1.6567654728324148
iter = 120 , loss = 22.663830738318225 , RMSE train = 1.598934429760728
iter = 130 , loss = 20.554638970616306 , RMSE train = 1.5586830395104427
iter = 140 , loss = 18.662326010915894 , RMSE train = 1.5310556873285857
iter = 150 , loss = 16.961119898205748 , RMSE train = 1.512260104198255
iter = 160 , loss = 15.429382591455969 , RMSE train = 1.499537431021974
iter = 170 , loss = 14.048659993999577 , RMSE train = 1.4909644396103943
iter = 180 , loss = 12.803007483189095 , RMSE train = 1.485203451198226
iter = 190 , loss = 11.678502960076756 , RMSE train = 1.4813408512198314
iter = 200 , loss = 10.662888492132945 , RMSE train = 1.4787562998546138
iter = 210 , loss = 9.745300956621668 , RMSE train = 1.4770304228727151
iter = 220 , loss = 8.916064982945905 , RMSE train = 1.4758802209778314
iter = 230 , loss = 8.16653010397531 , RMSE train = 1.4751154509560478
iter = 240 , loss = 7.488939791647248 , RMSE train = 1.4746083217248582
iter = 250 , loss = 6.8763239209353095 , RMSE train = 1.4742735750915155
iter = 260 , loss = 6.322408809625879 , RMSE train = 1.4740538071354845
iter = 270 , loss = 5.821540739039951 , RMSE train = 1.4739105755791204
iter = 280 , loss = 5.368620052535522 , RMSE train = 1.473818156956631
iter = 290 , loss = 4.959043740843366 , RMSE train = 1.4737593565539882
iter = 300 , loss = 4.588654980559773 , RMSE train = 1.473722694309024

User-based MF, RMSE = 1.54673231138361


iter = 10 , loss = 82.18500640026363 , RMSE train = 3.7188030507751155
iter = 20 , loss = 70.39610238972622 , RMSE train = 3.4415853425198795
iter = 30 , loss = 61.08677997273985 , RMSE train = 3.141412341770517
iter = 40 , loss = 53.561450208592824 , RMSE train = 2.8333637942163308
iter = 50 , loss = 47.34776949215775 , RMSE train = 2.53751435018122
iter = 60 , loss = 42.12112136080438 , RMSE train = 2.272392822624063
iter = 70 , loss = 37.65531311624291 , RMSE train = 2.0456385308699017
iter = 80 , loss = 33.79019823180017 , RMSE train = 1.8587862554884858
iter = 90 , loss = 30.41030096721763 , RMSE train = 1.708728558753467
iter = 100 , loss = 27.43063364890633 , RMSE train = 1.5921318762501597
iter = 110 , loss = 24.78723923403969 , RMSE train = 1.5053930397836175
iter = 120 , loss = 22.43085156515958 , RMSE train = 1.4432547296632936
iter = 130 , loss = 20.32262066782743 , RMSE train = 1.3997958148883882
iter = 140 , loss = 18.431210900048825 , RMSE train = 1.3699485567139429
iter = 150 , loss = 16.73081513238873 , RMSE train = 1.3497592589087575
iter = 160 , loss = 15.19978251224402 , RMSE train = 1.3362156893532073
iter = 170 , loss = 13.819658995065469 , RMSE train = 1.327200280648072
iter = 180 , loss = 12.5745069424706 , RMSE train = 1.3212391227491485
iter = 190 , loss = 11.450414532271433 , RMSE train = 1.3173297503391306
iter = 200 , loss = 10.4351352268234 , RMSE train = 1.3147852028420473
iter = 210 , loss = 9.51781717012361 , RMSE train = 1.313143868588874
iter = 220 , loss = 8.68879546397646 , RMSE train = 1.312098193421233
iter = 230 , loss = 7.939429008909814 , RMSE train = 1.3114428104380054
iter = 240 , loss = 7.261969441538829 , RMSE train = 1.3110405624742085
iter = 250 , loss = 6.649453621462254 , RMSE train = 1.3108017449687341
iter = 260 , loss = 6.095613758245756 , RMSE train = 1.310667207117354
iter = 270 , loss = 5.59480104876621 , RMSE train = 1.310598360578145
iter = 280 , loss = 5.141919900997661 , RMSE train = 1.310569877307973
iter = 290 , loss = 4.732370641481889 , RMSE train = 1.310565414807389
iter = 300 , loss = 4.361999166586316 , RMSE train = 1.31057454252503

User-based MF, RMSE = 1.3900092397082493


iter = 10 , loss = 82.56990712463467 , RMSE train = 3.856064846759046
iter = 20 , loss = 70.81852166377664 , RMSE train = 3.587335894942335
iter = 30 , loss = 61.533692743298054 , RMSE train = 3.29522425439545
iter = 40 , loss = 54.02419076110455 , RMSE train = 2.9965869819862903
iter = 50 , loss = 47.82058993594233 , RMSE train = 2.715068972370871
iter = 60 , loss = 42.600223046218524 , RMSE train = 2.4674974003990413
iter = 70 , loss = 38.13819733276557 , RMSE train = 2.260797476350451
iter = 80 , loss = 34.27523168576765 , RMSE train = 2.09449151471687
iter = 90 , loss = 30.896425389129732 , RMSE train = 1.9640459986946386
iter = 100 , loss = 27.917171951908912 , RMSE train = 1.865179210420742
iter = 110 , loss = 25.273766266764103 , RMSE train = 1.7925890326317744
iter = 120 , loss = 22.917108034142814 , RMSE train = 1.7412630157928939
iter = 130 , loss = 20.808455885097867 , RMSE train = 1.7057880597399526
iter = 140 , loss = 18.916544761924477 , RMSE train = 1.6816622910799308
iter = 150 , loss = 17.21561291823518 , RMSE train = 1.6654635734136913
iter = 160 , loss = 15.68403822198156 , RMSE train = 1.6546675048508845
iter = 170 , loss = 14.30338436853712 , RMSE train = 1.6475217226135208
iter = 180 , loss = 13.057724255516964 , RMSE train = 1.6428273406433629
iter = 190 , loss = 11.93315189989644 , RMSE train = 1.6397691135800732
iter = 200 , loss = 10.917423567968175 , RMSE train = 1.6377950989997991
iter = 210 , loss = 9.999688271446637 , RMSE train = 1.6365362042450844
iter = 220 , loss = 9.17028076859228 , RMSE train = 1.6357461363869814
iter = 230 , loss = 8.420558881639588 , RMSE train = 1.6352613162502723
iter = 240 , loss = 7.742772745836001 , RMSE train = 1.6349735519002244
iter = 250 , loss = 7.129957498793475 , RMSE train = 1.6348115834936958
iter = 260 , loss = 6.57584353767565 , RMSE train = 1.6347287088578228
iter = 270 , loss = 6.074780239045676 , RMSE train = 1.6346945111050801
iter = 280 , loss = 5.621670233747938 , RMSE train = 1.6346893374930094
iter = 290 , loss = 5.211912144865323 , RMSE train = 1.6347006247487343
iter = 300 , loss = 4.841350256006716 , RMSE train = 1.634720458172561

User-based MF, RMSE = 1.7181453809767753


iter = 10 , loss = 82.35407991168586 , RMSE train = 3.824445885055548
iter = 20 , loss = 70.64593012732858 , RMSE train = 3.562731543256987
iter = 30 , loss = 61.3893926446528 , RMSE train = 3.2792269935334355
iter = 40 , loss = 53.89869894891476 , RMSE train = 2.9895075312200556
iter = 50 , loss = 47.707858524576906 , RMSE train = 2.7125693284710604
iter = 60 , loss = 42.49637714791616 , RMSE train = 2.4641421845504476
iter = 70 , loss = 38.040741626715025 , RMSE train = 2.252843109296081
iter = 80 , loss = 34.18254840756598 , RMSE train = 2.0798843438285535
iter = 90 , loss = 30.807455265596793 , RMSE train = 1.943654550731127
iter = 100 , loss = 27.83121088137987 , RMSE train = 1.8402617371678056
iter = 110 , loss = 25.190335657082127 , RMSE train = 1.764881215797409
iter = 120 , loss = 22.835872202274693 , RMSE train = 1.711620939105146
iter = 130 , loss = 20.72916959263163 , RMSE train = 1.67481556137041
iter = 140 , loss = 18.839020057228506 , RMSE train = 1.6497262663765486
iter = 150 , loss = 17.13969833090218 , RMSE train = 1.6327522805619272
iter = 160 , loss = 15.609605834017119 , RMSE train = 1.6213452687193421
iter = 170 , loss = 14.230321880436328 , RMSE train = 1.61372329794426
iter = 180 , loss = 12.98593019360032 , RMSE train = 1.6086535621776816
iter = 190 , loss = 11.862532776097305 , RMSE train = 1.6052958539654436
iter = 200 , loss = 10.847892233440605 , RMSE train = 1.6030833464161476
iter = 210 , loss = 9.931162983513923 , RMSE train = 1.6016334623460313
iter = 220 , loss = 9.102684670004372 , RMSE train = 1.6006899261104688
iter = 230 , loss = 8.353819706405124 , RMSE train = 1.6000815149088279
iter = 240 , loss = 7.676822639514872 , RMSE train = 1.5996939632891187
iter = 250 , loss = 7.0647328877217355 , RMSE train = 1.59945124294004
iter = 260 , loss = 6.511285010699273 , RMSE train = 1.5993028922077925
iter = 270 , loss = 6.010832423197895 , RMSE train = 1.5992155141365876
iter = 280 , loss = 5.558281655976945 , RMSE train = 1.5991670300863763
iter = 290 , loss = 5.149035078047026 , RMSE train = 1.599143050524114
iter = 300 , loss = 4.778940550774237 , RMSE train = 1.5991341387035303

User-based MF, RMSE = 1.6796378382536892


iter = 10 , loss = 82.12700511389906 , RMSE train = 3.792192316657103
iter = 20 , loss = 70.43660279089688 , RMSE train = 3.526226915882281
iter = 30 , loss = 61.190785940940266 , RMSE train = 3.2382394456131975
iter = 40 , loss = 53.70619041301384 , RMSE train = 2.9418045531260937
iter = 50 , loss = 47.5184640162367 , RMSE train = 2.654429831424526
iter = 60 , loss = 42.30821330612983 , RMSE train = 2.3932479430925144
iter = 70 , loss = 37.85265540945121 , RMSE train = 2.1697960758569423
iter = 80 , loss = 33.993865103909854 , RMSE train = 1.986047035473345
iter = 90 , loss = 30.617808885937364 , RMSE train = 1.840116196865661
iter = 100 , loss = 27.640431063301573 , RMSE train = 1.7292830182784245
iter = 110 , loss = 24.998372957817 , RMSE train = 1.6481287391843606
iter = 120 , loss = 22.642749253128024 , RMSE train = 1.5904702046237906
iter = 130 , loss = 20.53494955173544 , RMSE train = 1.5504009920436492
iter = 140 , loss = 18.643786551082503 , RMSE train = 1.5229441997200348
iter = 150 , loss = 16.943542968783035 , RMSE train = 1.504288893085582
iter = 160 , loss = 15.412620670738367 , RMSE train = 1.4916945144815228
iter = 170 , loss = 14.032595078112939 , RMSE train = 1.4832262005099357
iter = 180 , loss = 12.787543723103417 , RMSE train = 1.4775482938650113
iter = 190 , loss = 11.66356139485815 , RMSE train = 1.4737534778946186
iter = 200 , loss = 10.648403240844763 , RMSE train = 1.4712222056889157
iter = 210 , loss = 9.731216431121208 , RMSE train = 1.4695391152299748
iter = 220 , loss = 8.902333820322768 , RMSE train = 1.4684241066057298
iter = 230 , loss = 8.153111610611662 , RMSE train = 1.4676883155329241
iter = 240 , loss = 7.475798754619047 , RMSE train = 1.467205177591556
iter = 250 , loss = 6.863429686122656 , RMSE train = 1.4668900049188602
iter = 260 , loss = 6.309734555869636 , RMSE train = 1.4666861970253546
iter = 270 , loss = 5.809062898329374 , RMSE train = 1.4665559758129127
iter = 280 , loss = 5.356317840500731 , RMSE train = 1.466474163485437
iter = 290 , loss = 4.94689877176779 , RMSE train = 1.46642401005722
iter = 300 , loss = 4.576650948090104 , RMSE train = 1.4663943978481502

User-based MF, RMSE = 1.5479719718207203


User-based MF-5models, RMSE = 1.2769292724637498

################################################################################
user-based = 0 ( ~ item-based)
################################################################################

K = 50, lam = .1, learning_rate = 0.75, max_iter = 200, user_based = 0

iter = 10 , loss = 34.764955441187674 , RMSE train = 1.9474753875733575
iter = 20 , loss = 15.99408238535548 , RMSE train = 1.3898361798978238
iter = 30 , loss = 7.813164540706116 , RMSE train = 1.357737199688776
iter = 40 , loss = 4.0803083390041195 , RMSE train = 1.356488714793856
iter = 50 , loss = 2.369281422251005 , RMSE train = 1.3564907146766465
iter = 60 , loss = 1.5846555078661149 , RMSE train = 1.356506572699766
iter = 70 , loss = 1.2248361970643629 , RMSE train = 1.3565113759242249
iter = 80 , loss = 1.0598278370192145 , RMSE train = 1.3565126161959997
iter = 90 , loss = 0.9841574045483159 , RMSE train = 1.3565129219514174
iter = 100 , loss = 0.9494561061436566 , RMSE train = 1.3565129955466344
iter = 110 , loss = 0.9335426371999074 , RMSE train = 1.3565130129718306
iter = 120 , loss = 0.9262449732309914 , RMSE train = 1.3565130170457627
iter = 130 , loss = 0.9228983805089163 , RMSE train = 1.3565130179886067
iter = 140 , loss = 0.9213636860129643 , RMSE train = 1.3565130182049967
iter = 150 , loss = 0.9206598992072488 , RMSE train = 1.3565130182543135
iter = 160 , loss = 0.9203371535274137 , RMSE train = 1.3565130182654859
iter = 170 , loss = 0.9201891473312686 , RMSE train = 1.3565130182679963
iter = 180 , loss = 0.9201212739470264 , RMSE train = 1.3565130182685687
iter = 190 , loss = 0.9200901482373884 , RMSE train = 1.3565130182686913
iter = 200 , loss = 0.9200758744517015 , RMSE train = 1.356513018268696

Item-based MF, RMSE = 2.724872324812332


iter = 10 , loss = 34.65881465378679 , RMSE train = 1.8864784302072257
iter = 20 , loss = 15.892562450285922 , RMSE train = 1.3181621988226009
iter = 30 , loss = 7.713521423193528 , RMSE train = 1.2838662062293786
iter = 40 , loss = 3.9816374540010697 , RMSE train = 1.282430120111898
iter = 50 , loss = 2.271090454182138 , RMSE train = 1.282404417936206
iter = 60 , loss = 1.486693089563795 , RMSE train = 1.2824147512008661
iter = 70 , loss = 1.1269806510067277 , RMSE train = 1.2824183594741132
iter = 80 , loss = 0.9620218187286207 , RMSE train = 1.2824193372244623
iter = 90 , loss = 0.886374238492483 , RMSE train = 1.2824195852230018
iter = 100 , loss = 0.8516834619662 , RMSE train = 1.282419646127661
iter = 110 , loss = 0.8357748328831589 , RMSE train = 1.282419660769199
iter = 120 , loss = 0.8284793942226922 , RMSE train = 1.2824196642335703
iter = 130 , loss = 0.8251338245227935 , RMSE train = 1.2824196650431339
iter = 140 , loss = 0.8235996003280146 , RMSE train = 1.2824196652304254
iter = 150 , loss = 0.8228960297402864 , RMSE train = 1.2824196652734068
iter = 160 , loss = 0.8225733834740916 , RMSE train = 1.2824196652831974
iter = 170 , loss = 0.8224254229915061 , RMSE train = 1.2824196652854063
iter = 180 , loss = 0.822357570630133 , RMSE train = 1.2824196652859166
iter = 190 , loss = 0.8223264545896247 , RMSE train = 1.282419665286018
iter = 200 , loss = 0.8223121852516216 , RMSE train = 1.282419665286027

Item-based MF, RMSE = 2.742791952863389


iter = 10 , loss = 35.053617641677874 , RMSE train = 2.075461815157119
iter = 20 , loss = 16.270524800802285 , RMSE train = 1.577356868052387
iter = 30 , loss = 8.090002086973294 , RMSE train = 1.5488742958521509
iter = 40 , loss = 4.3576475171663285 , RMSE train = 1.5477776934348224
iter = 50 , loss = 2.6468681647356807 , RMSE train = 1.5477861889647722
iter = 60 , loss = 1.8623553665756631 , RMSE train = 1.5478028117648772
iter = 70 , loss = 1.502587223078847 , RMSE train = 1.547807859770696
iter = 80 , loss = 1.3376020367407742 , RMSE train = 1.5478091784873012
iter = 90 , loss = 1.2619421270921172 , RMSE train = 1.5478095068249411
iter = 100 , loss = 1.2272456159267897 , RMSE train = 1.547809586467769
iter = 110 , loss = 1.2113343269275025 , RMSE train = 1.5478096054388046
iter = 120 , loss = 1.2040376558526178 , RMSE train = 1.5478096098955259
iter = 130 , loss = 1.2006915152535336 , RMSE train = 1.5478096109310306
iter = 140 , loss = 1.199157026528816 , RMSE train = 1.5478096111694475
iter = 150 , loss = 1.1984533333099847 , RMSE train = 1.547809611223936
iter = 160 , loss = 1.1981306301609587 , RMSE train = 1.5478096112363187
iter = 170 , loss = 1.1979826432765655 , RMSE train = 1.5478096112391124
iter = 180 , loss = 1.1979147786531936 , RMSE train = 1.5478096112397404
iter = 190 , loss = 1.1978836569142326 , RMSE train = 1.5478096112398774
iter = 200 , loss = 1.1978693849264241 , RMSE train = 1.5478096112399025

Item-based MF, RMSE = 2.9627407338940217


iter = 10 , loss = 34.929769566407174 , RMSE train = 2.0504454561641974
iter = 20 , loss = 16.19164785550846 , RMSE train = 1.5382866634282883
iter = 30 , loss = 8.022939606385547 , RMSE train = 1.5103710723006731
iter = 40 , loss = 4.295327790233918 , RMSE train = 1.509455520984013
iter = 50 , loss = 2.5866343079105913 , RMSE train = 1.5095039487259594
iter = 60 , loss = 1.8030618372270006 , RMSE train = 1.509528916570469
iter = 70 , loss = 1.4437215919975375 , RMSE train = 1.5095356890895701
iter = 80 , loss = 1.2789319081048165 , RMSE train = 1.509537363215114
iter = 90 , loss = 1.2033614813268538 , RMSE train = 1.5095377647321213
iter = 100 , loss = 1.1687059599361014 , RMSE train = 1.509537859434307
iter = 110 , loss = 1.1528134541409623 , RMSE train = 1.509537881502851
iter = 120 , loss = 1.1455253917878003 , RMSE train = 1.509537886596372
iter = 130 , loss = 1.142183197084393 , RMSE train = 1.5095378877627026
iter = 140 , loss = 1.140650517091772 , RMSE train = 1.5095378880279915
iter = 150 , loss = 1.1399476529930919 , RMSE train = 1.5095378880879926
iter = 160 , loss = 1.13962532992157 , RMSE train = 1.5095378881015054
iter = 170 , loss = 1.139477517272798 , RMSE train = 1.5095378881045243
iter = 180 , loss = 1.1394097325245092 , RMSE train = 1.5095378881052017
iter = 190 , loss = 1.1393786474035859 , RMSE train = 1.5095378881053572
iter = 200 , loss = 1.139364392203337 , RMSE train = 1.509537888105381

Item-based MF, RMSE = 2.8935697520770445


iter = 10 , loss = 34.829010075672564 , RMSE train = 1.9904250449444107
iter = 20 , loss = 16.06892416036905 , RMSE train = 1.4459167118946097
iter = 30 , loss = 7.889885921051815 , RMSE train = 1.414659222895168
iter = 40 , loss = 4.157997610180701 , RMSE train = 1.4133117695067872
iter = 50 , loss = 2.447467836924371 , RMSE train = 1.4132810365985442
iter = 60 , loss = 1.6630821966994405 , RMSE train = 1.4132896289936587
iter = 70 , loss = 1.3033756885362726 , RMSE train = 1.4132929177448725
iter = 80 , loss = 1.13841962519058 , RMSE train = 1.413293846503582
iter = 90 , loss = 1.0627733065892695 , RMSE train = 1.41329408839865
iter = 100 , loss = 1.0280831004700959 , RMSE train = 1.4132941489192121
iter = 110 , loss = 1.0121747284206069 , RMSE train = 1.4132941636698344
iter = 120 , loss = 1.0048794052598393 , RMSE train = 1.413294167197099
iter = 130 , loss = 1.0015338872872652 , RMSE train = 1.4132941680282953
iter = 140 , loss = 0.9999996861643031 , RMSE train = 1.4132941682219031
iter = 150 , loss = 0.9992961258176414 , RMSE train = 1.4132941682665776
iter = 160 , loss = 0.998973484072027 , RMSE train = 1.4132941682768039
iter = 170 , loss = 0.9988255255722892 , RMSE train = 1.4132941682791271
iter = 180 , loss = 0.9987576740743376 , RMSE train = 1.4132941682796532
iter = 190 , loss = 0.998726558406683 , RMSE train = 1.413294168279767
iter = 200 , loss = 0.9987122892281076 , RMSE train = 1.4132941682797817

Item-based MF, RMSE = 2.833421280601401

Item-based MF-5models, RMSE = 2.7153218666218324


################################################################################

K = 50, lam = .5, learning_rate = 0.1, max_iter = 100, user_based = 0

iter = 10 , loss = 218.26685256320053 , RMSE train = 2.582230430911834
iter = 20 , loss = 129.5405559113464 , RMSE train = 1.6282545234646175
iter = 30 , loss = 77.7345499405718 , RMSE train = 1.3960634976867003
iter = 40 , loss = 46.88623430739425 , RMSE train = 1.36199272520104
iter = 50 , loss = 28.43829856573488 , RMSE train = 1.3573296013134564
iter = 60 , loss = 17.395765434914114 , RMSE train = 1.3566552479259824
iter = 70 , loss = 10.784598313300652 , RMSE train = 1.3565437192923524
iter = 80 , loss = 6.826310136632325 , RMSE train = 1.356521080868243
iter = 90 , loss = 4.45634799962154 , RMSE train = 1.3565154098601693
iter = 100 , loss = 3.0373666840219102 , RMSE train = 1.3565137683344504

User-based MF, RMSE = 2.7248708855061023


iter = 10 , loss = 217.97035493033266 , RMSE train = 2.509362976555351
iter = 20 , loss = 129.33772397670407 , RMSE train = 1.5596342375491656
iter = 30 , loss = 77.57691743775723 , RMSE train = 1.3217227244127807
iter = 40 , loss = 46.753468904609726 , RMSE train = 1.2873738249429132
iter = 50 , loss = 28.3198498884578 , RMSE train = 1.2829904804972547
iter = 60 , loss = 17.28571091987404 , RMSE train = 1.2824665413730225
iter = 70 , loss = 10.679509298753013 , RMSE train = 1.2824152921195506
iter = 80 , loss = 6.724172798845491 , RMSE train = 1.282415053442783
iter = 90 , loss = 4.355970233705602 , RMSE train = 1.2824175051760693
iter = 100 , loss = 2.9380396114655376 , RMSE train = 1.2824187843335795

User-based MF, RMSE = 2.742789356701632


iter = 10 , loss = 218.50954933849226 , RMSE train = 2.6420392120437333
iter = 20 , loss = 129.8161606956826 , RMSE train = 1.7826128439115239
iter = 30 , loss = 78.01430594037328 , RMSE train = 1.5808428359709403
iter = 40 , loss = 47.165936644443825 , RMSE train = 1.5520773375393784
iter = 50 , loss = 28.717444401952886 , RMSE train = 1.5483428484547617
iter = 60 , loss = 17.674438088878375 , RMSE train = 1.5478695033397816
iter = 70 , loss = 11.062942976070607 , RMSE train = 1.5478131458069075
iter = 80 , loss = 7.104442812790056 , RMSE train = 1.5478081920347537
iter = 90 , loss = 4.734348068582866 , RMSE train = 1.5478086205659742
iter = 100 , loss = 3.3152852358108413 , RMSE train = 1.547809145165378

User-based MF, RMSE = 2.9627392689423635


iter = 10 , loss = 218.38516195441755 , RMSE train = 2.6512398829710015
iter = 20 , loss = 129.70184838695033 , RMSE train = 1.7534161580280279
iter = 30 , loss = 77.92057378673009 , RMSE train = 1.5434496003218452
iter = 40 , loss = 47.08621060329179 , RMSE train = 1.5138218533771504
iter = 50 , loss = 28.64625414406867 , RMSE train = 1.510037739774668
iter = 60 , loss = 17.60835396261113 , RMSE train = 1.5095808406901294
iter = 70 , loss = 10.999906972762245 , RMSE train = 1.5095347403588928
iter = 80 , loss = 7.0432276003204 , RMSE train = 1.5095340054290998
iter = 90 , loss = 4.674221377259166 , RMSE train = 1.5095360093015846
iter = 100 , loss = 3.255809662411681 , RMSE train = 1.5095371043393067

User-based MF, RMSE = 2.8935686892379335


iter = 10 , loss = 218.23280963878742 , RMSE train = 2.611470612985962
iter = 20 , loss = 129.56292935083394 , RMSE train = 1.66846527270759
iter = 30 , loss = 77.78286674057355 , RMSE train = 1.4481070989131075
iter = 40 , loss = 46.94784878690939 , RMSE train = 1.4174012554835504
iter = 50 , loss = 28.50714196844977 , RMSE train = 1.4136659171128512
iter = 60 , loss = 17.468681826399937 , RMSE train = 1.4132830528061944
iter = 70 , loss = 10.85986220561471 , RMSE train = 1.413270656485559
iter = 80 , loss = 6.902946601814146 , RMSE train = 1.4132828823937331
iter = 90 , loss = 4.533794232870443 , RMSE train = 1.4132896305692044
iter = 100 , loss = 3.1152933001456695 , RMSE train = 1.413292433660745

User-based MF, RMSE = 2.83342092424115


User-based MF-5models, RMSE = 2.7153213376893723

