{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Matrix-Factorization-5models_RS","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSd2Q2vLsD7F2DYxALIwZg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"colab_type":"code","executionInfo":{"elapsed":27005,"status":"ok","timestamp":1583287634568,"user":{"displayName":"Ý Nguyễn Thị Như","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCwe9q5eGcI5GmOWVyzNvPeymwsZvqdGyTl7SB=s64","userId":"03745038202951496647"},"user_tz":-420},"id":"w8HYaaZEclLx","outputId":"970d3cc0-4474-4552-c0ec-905e8d5c663a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /Thesis\n"]}],"source":["from google.colab import drive\n","drive.mount('/Thesis', force_remount= True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":2293,"status":"ok","timestamp":1583287651606,"user":{"displayName":"Ý Nguyễn Thị Như","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCwe9q5eGcI5GmOWVyzNvPeymwsZvqdGyTl7SB=s64","userId":"03745038202951496647"},"user_tz":-420},"id":"sLmRF9oZVaHm","outputId":"0a252f1e-bd65-4fc6-adb3-cf62f3532a94"},"outputs":[{"name":"stdout","output_type":"stream","text":["/Thesis/My Drive/Oh_Our_Thesis/recommender-system\n"]}],"source":["cd \"/Thesis/My Drive/Oh_Our_Thesis/recommender-system\""]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"fMvs7Z2t5Ep5"},"outputs":[],"source":["import pandas as pd \n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy import sparse \n","\n","from sklearn.decomposition import NMF\n","\n","\n","class MF(object):\n","    \"\"\"docstring for CF\"\"\"\n","    def __init__(self, n_users, n_items, Y_data, K, lam = 0.1, Xinit = None, Winit = None, \n","            learning_rate = 0.5, max_iter = 100, print_every = 100, user_based = 1):\n","        self.Y_raw_data = Y_data\n","        self.K = K\n","        # regularization parameter\n","        self.lam = lam\n","        # learning rate for gradient descent\n","        self.learning_rate = learning_rate\n","        # maximum number of iterations\n","        self.max_iter = max_iter\n","        # print results after print_every iterations\n","        self.print_every = print_every\n","        # user-based or item-based\n","        self.user_based = user_based\n","        # number of users, items, and ratings. Remember to add 1 since id starts from 0\n","\n","        # self.n_users = int(np.max(Y_data[:, 0])) + 1 \n","        # self.n_items = int(np.max(Y_data[:, 1])) + 1\n","\n","        # (+1) is added before pass the param\n","        self.n_users = int(n_users) # +1\n","        self.n_items = int(n_items) # +1\n","\n","        self.n_ratings = Y_data.shape[0]\n","        \n","        if Xinit is None: # new\n","            self.X = np.random.randn(self.n_items, K)\n","        else: # or from saved data\n","            self.X = Xinit \n","        \n","        if Winit is None: \n","            self.W = np.random.randn(K, self.n_users)\n","        else: # from saved data\n","            self.W = Winit\n","            \n","        # normalized data, update later in normalized_Y function\n","        self.Y_data_n = self.Y_raw_data.copy()\n","\n","    def normalize_Y(self):\n","        if self.user_based:\n","            user_col = 0\n","            item_col = 1\n","            n_objects = self.n_users\n","\n","        # if we want to normalize based on item, just switch first two columns of data\n","        else: # item base\n","            user_col = 1\n","            item_col = 0 \n","            n_objects = self.n_items\n","\n","        users = (self.Y_raw_data[:, user_col] ).astype(np.int32)\n","        self.mu = np.zeros((n_objects,))\n","        for n in range(n_objects):\n","            # row indices of rating done by user n\n","            # since indices need to be integers, we need to convert\n","            ids = np.where(users == n)[0].astype(np.int32)\n","            # indices of all ratings associated with user n\n","            item_ids = (self.Y_data_n[ids, item_col]).astype(np.int32)\n","            # and the corresponding ratings \n","            ratings = self.Y_data_n[ids, 2]\n","            # take mean\n","            m = np.mean(ratings) \n","            if np.isnan(m):\n","                m = 0 # to avoid empty array and nan value\n","            self.mu[n] = m\n","            # normalize\n","            self.Y_data_n[ids, 2] = ratings - self.mu[n]\n","\n","    def loss(self):\n","        L = 0 \n","        for i in range(self.n_ratings):\n","            # user, item, rating\n","            n, m, rate = int(self.Y_data_n[i, 0]), int(self.Y_data_n[i, 1]), self.Y_data_n[i, 2]\n","            L += 0.5*(rate - self.X[m, :].dot(self.W[:, n]))**2\n","\n","    # take average\n","        L /= self.n_ratings\n","        # regularization, don't ever forget this \n","        L += 0.5*self.lam*(np.linalg.norm(self.X, 'fro') + np.linalg.norm(self.W, 'fro'))\n","        return L \n","\n","\n","    def get_items_rated_by_user(self, user_id):\n","        \"\"\"\n","        get all items which are rated by user user_id, and the corresponding ratings\n","        \"\"\"\n","        ids = np.where((self.Y_data_n[:,0]).astype(np.int32) == user_id)[0] \n","        item_ids = self.Y_data_n[ids, 1].astype(np.int32) # indices need to be integers\n","        ratings = self.Y_data_n[ids, 2]\n","        return (item_ids, ratings)\n","\n","\n","    def get_users_who_rate_item(self, item_id):\n","        \"\"\"\n","        get all users who rated item item_id and get the corresponding ratings\n","        \"\"\"\n","        ids = np.where((self.Y_data_n[:,1]).astype(np.int32) == item_id)[0] \n","        user_ids = self.Y_data_n[ids, 0].astype(np.int32)\n","        ratings = self.Y_data_n[ids, 2]\n","        return (user_ids, ratings)\n","\n","\n","    def updateX(self):\n","        for m in range(self.n_items):\n","            user_ids, ratings = self.get_users_who_rate_item(m)\n","            Wm = self.W[:, user_ids]\n","            # gradient\n","            grad_xm = -(ratings - self.X[m, :].dot(Wm)).dot(Wm.T)/self.n_ratings + \\\n","                                               self.lam*self.X[m, :]\n","            self.X[m, :] -= self.learning_rate*grad_xm.reshape((self.K,))\n","\n","    def updateW(self):\n","        for n in range(self.n_users):\n","            item_ids, ratings = self.get_items_rated_by_user(n)\n","            Xn = self.X[item_ids, :]\n","            # gradient\n","            grad_wn = -Xn.T.dot(ratings - Xn.dot(self.W[:, n]))/self.n_ratings + \\\n","                        self.lam*self.W[:, n]\n","            self.W[:, n] -= self.learning_rate*grad_wn.reshape((self.K,))\n","            \n","    def fit(self):\n","        self.normalize_Y()\n","        for it in range(self.max_iter):\n","            self.updateX()\n","            self.updateW()\n","            if (it + 1) % self.print_every == 0:\n","                rmse_train = self.evaluate_RMSE(self.Y_raw_data)\n","                print ('iter =', it + 1, ', loss =', self.loss(), ', RMSE train =', rmse_train)\n","\n","\n","        # model = NMF(n_components=10, init='random', random_state=0)\n","        # self.W = model.fit_transform(X)\n","        # self.H = model.components_\n","\n","    def pred(self, u, i):\n","        \"\"\" \n","        predict the rating of user u for item i \n","        if you need the un\n","        \"\"\"\n","        u = int(u)\n","        i = int(i)\n","        if self.user_based:\n","            bias = self.mu[u]\n","        else: \n","            bias = self.mu[i]\n","        pred = self.X[i, :].dot(self.W[:, u]) + bias \n","        # truncate if results are out of range [0, 10]\n","        if pred < 1:\n","            return 1 \n","        if pred > 10: \n","            return 10 \n","        return pred \n","\n","\n","    def pred_for_user(self, user_id):\n","        \"\"\"\n","        predict ratings one user give all unrated items\n","        \"\"\"\n","        ids = np.where((self.Y_data_n[:, 0]).astype(np.int32) == user_id)[0]\n","        items_rated_by_u = self.Y_data_n[ids, 1].tolist()              \n","\n","        y_pred = self.X.dot(self.W[:, user_id]) + self.mu[user_id]\n","        predicted_ratings= []\n","\n","        for i in range(self.n_items):\n","            if i not in items_rated_by_u:\n","                predicted_ratings.append((i, y_pred[i]))\n","\n","        return predicted_ratings\n","\n","\n","    def pred_for_all_user(self):\n","        all_pre_rating = []\n","\n","        for user_id in range (self.n_users):\n","            # predicted_ratings= []\n","            # ids = np.where((self.Y_data_n[:, 0]).astype(np.int32) == user_id)[0]\n","            # items_rated_by_u = self.Y_data_n[ids, 1].tolist()           \n","            y_pred = self.X.dot(self.W[:, user_id]) + self.mu[user_id] \n","            all_pre_rating.append(y_pred.tolist())\n","\n","        return all_pre_rating \n","\n","    def pred_all_usePred(self):\n","        pred = []\n","        for i in range(self.n_users):\n","            tmp = []\n","            for j in range(self.n_items):\n","                tmp += [self.pred(i, j)]\n","            pred += [tmp]\n","        return pred\n","\n","    def evaluate_RMSE(self, rate_test):\n","        n_tests = rate_test.shape[0]\n","        SE = 0 # squared error\n","        for n in range(n_tests):\n","            pred = self.pred(rate_test[n, 0], rate_test[n, 1])\n","            SE += (pred - rate_test[n, 2])**2 \n","\n","        RMSE = np.sqrt(SE/n_tests)\n","        return RMSE"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"ExhNNyp5WjsC"},"outputs":[],"source":["tmp = pd.read_json('../Crawl_Data/user_rates_place-ver2.json')\n","\n","# ratings_x = tmp[['User_Id','Place_Id','Rating','TimeStamp']]\n","ratings_x = tmp[['User_Id','Place_Id', 'Rating', 'Rating_Space',\t'Rating_Location',\t'Rating_Quality',\t'Rating_Service',\t'Rating_Price']]\n","\n","userId = ratings_x.User_Id.unique()\n","userId.sort()\n","mapUserId  = {}\n","for i, value in enumerate(userId):\n","  mapUserId[value] = i\n","\n","placeId = ratings_x.Place_Id.unique()\n","placeId.sort()\n","mapPlaceId  = {}\n","for i, value in enumerate(placeId):\n","  mapPlaceId[value] = i\n","\n","tmp_based = ratings_x.values\n","\n","for i in range(len(tmp_based[:,0])):\n","  tmp_based[i][0] = mapUserId[tmp_based [i][0]]\n","for i in range(len(tmp_based[:,1])):\n","  tmp_based[i][1] = mapPlaceId[tmp_based [i][1]]\n","\n","\n","# indices start from 0\n","# tmp_based[:, :2] -= 1\n","# rate_test[:, :2] -= 1\n","\n","# tmp_based = tmp_based[:,[0,1]].astype(int)\n","# tmp_based4\n","tmp_based_copy = tmp_based"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"97IXgh1YWmhk"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","tmp_based_copy = tmp_based_copy[tmp_based_copy[:,0].argsort()] # sort theo  user_id\n","based_train = []\n","based_test = []\n","_usersId = (tmp_based_copy[:, 0]).astype(np.int32)\n","\n","for i in range((max(tmp_based_copy[:,0])).astype(int)):  \n","  _ids = np.where(_usersId == i)[0].astype(np.int32)\n","  if (len(_ids) > 4):\n","    X_train, X_test= train_test_split(tmp_based_copy[_ids], test_size=.2, random_state=42)\n","    based_train += X_train.tolist()\n","    based_test += X_test.tolist()\n","    \n","based_train = np.array(based_train)\n","based_test = np.array(based_test)\n","# X_test = np.array(X_test)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"colab_type":"code","executionInfo":{"elapsed":103330,"status":"ok","timestamp":1583289257029,"user":{"displayName":"Ý Nguyễn Thị Như","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCwe9q5eGcI5GmOWVyzNvPeymwsZvqdGyTl7SB=s64","userId":"03745038202951496647"},"user_tz":-420},"id":"kZTUDNfUWoy5","outputId":"925ae650-fbe0-447d-852b-81fc78d8f940"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"name":"stdout","output_type":"stream","text":["iter = 10 , loss = 1.440643391094046 , RMSE train = 0.936062860297831\n","iter = 20 , loss = 0.44722870584701246 , RMSE train = 0.9360668675068299\n","iter = 30 , loss = 0.43819352038188675 , RMSE train = 0.936066867889704\n","iter = 40 , loss = 0.4381113448330287 , RMSE train = 0.9360668678897408\n","iter = 50 , loss = 0.438110597440447 , RMSE train = 0.9360668678897408\n","\n","User-based MF, RMSE = 1.831055750803039\n"]}],"source":["rs = MF(int(max(tmp_based_copy[:,0])), int(max(tmp_based_copy[:,1])), based_train, K = 10, lam = .5, print_every = 10, learning_rate = 0.75, max_iter = 50, user_based = 0)\n","rs.fit()\n","\n","# evaluate on test data\n","RMSE = rs.evaluate_RMSE(based_test)\n","print ('\\nUser-based MF, RMSE =', RMSE)\n","print ('\\n\\n')"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","executionInfo":{"elapsed":3148232,"status":"ok","timestamp":1583318040336,"user":{"displayName":"Ý Nguyễn Thị Như","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCwe9q5eGcI5GmOWVyzNvPeymwsZvqdGyTl7SB=s64","userId":"03745038202951496647"},"user_tz":-420},"id":"BUcHatdoj3sK","outputId":"026c1fbf-a0a8-4f9b-bacc-400c9b26da9a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"name":"stdout","output_type":"stream","text":["iter = 10 , loss = 24.096722517505555 , RMSE train = 1.6130697544813795\n","iter = 20 , loss = 11.003053654121855 , RMSE train = 1.0838444782373085\n","iter = 30 , loss = 5.33123505808014 , RMSE train = 1.0536885908819187\n","iter = 40 , loss = 2.7442525434766476 , RMSE train = 1.0526462219973591\n","iter = 50 , loss = 1.5584875721461993 , RMSE train = 1.052700268966099\n","iter = 60 , loss = 1.014723607387887 , RMSE train = 1.052731584755556\n","iter = 70 , loss = 0.7653566550482747 , RMSE train = 1.0527407123156154\n","iter = 80 , loss = 0.650998874691344 , RMSE train = 1.0527430937905302\n","iter = 90 , loss = 0.5985555479506109 , RMSE train = 1.0527436886880126\n","iter = 100 , loss = 0.574505641296033 , RMSE train = 1.052743833477713\n","iter = 110 , loss = 0.5634766451923835 , RMSE train = 1.0527438680705767\n","iter = 120 , loss = 0.5584188819909333 , RMSE train = 1.0527438762191095\n","iter = 130 , loss = 0.5560994523143941 , RMSE train = 1.0527438781170846\n","iter = 140 , loss = 0.5550357887201358 , RMSE train = 1.052743878555127\n","iter = 150 , loss = 0.55454800440012 , RMSE train = 1.0527438786554684\n","iter = 160 , loss = 0.5543243116656629 , RMSE train = 1.0527438786783019\n","iter = 170 , loss = 0.5542217284169216 , RMSE train = 1.0527438786834797\n","iter = 180 , loss = 0.5541746847072335 , RMSE train = 1.0527438786846393\n","iter = 190 , loss = 0.5541531108786276 , RMSE train = 1.0527438786849108\n","iter = 200 , loss = 0.5541432172992078 , RMSE train = 1.0527438786849688\n","iter = 210 , loss = 0.5541386801796322 , RMSE train = 1.0527438786849888\n","iter = 220 , loss = 0.5541365994887036 , RMSE train = 1.052743878684989\n","iter = 230 , loss = 0.554135645297321 , RMSE train = 1.0527438786849903\n","iter = 240 , loss = 0.5541352077107545 , RMSE train = 1.0527438786849905\n","iter = 250 , loss = 0.5541350070358566 , RMSE train = 1.0527438786849908\n","iter = 260 , loss = 0.5541349150072805 , RMSE train = 1.0527438786849908\n","iter = 270 , loss = 0.554134872803346 , RMSE train = 1.0527438786849908\n","iter = 280 , loss = 0.5541348534487649 , RMSE train = 1.0527438786849908\n","iter = 290 , loss = 0.5541348445728077 , RMSE train = 1.0527438786849908\n","iter = 300 , loss = 0.5541348405023127 , RMSE train = 1.0527438786849908\n","iter = 310 , loss = 0.5541348386355897 , RMSE train = 1.0527438786849908\n","iter = 320 , loss = 0.5541348377795121 , RMSE train = 1.0527438786849908\n","iter = 330 , loss = 0.554134837386915 , RMSE train = 1.0527438786849908\n","iter = 340 , loss = 0.5541348372068698 , RMSE train = 1.0527438786849908\n","iter = 350 , loss = 0.554134837124301 , RMSE train = 1.0527438786849908\n","\n","User-based MF, RMSE = 1.8982906358251148\n","\n","\n","iter = 10 , loss = 23.974929666496436 , RMSE train = 1.533995209658462\n","iter = 20 , loss = 10.943144217299281 , RMSE train = 1.0254015615344492\n","iter = 30 , loss = 5.274191838554711 , RMSE train = 0.9972475385933165\n","iter = 40 , loss = 2.687173123779929 , RMSE train = 0.9965574361582333\n","iter = 50 , loss = 1.5012650857779009 , RMSE train = 0.99668082221008\n","iter = 60 , loss = 0.9574163334295397 , RMSE train = 0.9967260558770191\n","iter = 70 , loss = 0.7080068599827207 , RMSE train = 0.9967379848162091\n","iter = 80 , loss = 0.593628815469239 , RMSE train = 0.9967409297741879\n","iter = 90 , loss = 0.5411760190148103 , RMSE train = 0.996741637853412\n","iter = 100 , loss = 0.5171217234946791 , RMSE train = 0.9967418053390081\n","iter = 110 , loss = 0.5060907010091632 , RMSE train = 0.9967418444745169\n","iter = 120 , loss = 0.5010320040470315 , RMSE train = 0.9967418535304341\n","iter = 130 , loss = 0.49871214461672425 , RMSE train = 0.9967418556092331\n","iter = 140 , loss = 0.4976482834134618 , RMSE train = 0.9967418560832342\n","iter = 150 , loss = 0.49716040830060587 , RMSE train = 0.9967418561906888\n","iter = 160 , loss = 0.49693667388147084 , RMSE train = 0.996741856214946\n","iter = 170 , loss = 0.4968340715081721 , RMSE train = 0.9967418562203969\n","iter = 180 , loss = 0.49678701903050765 , RMSE train = 0.9967418562216062\n","iter = 190 , loss = 0.4967654411849042 , RMSE train = 0.9967418562218857\n","iter = 200 , loss = 0.49675554576642045 , RMSE train = 0.9967418562219468\n","iter = 210 , loss = 0.4967510078054694 , RMSE train = 0.9967418562219581\n","iter = 220 , loss = 0.49674892672989523 , RMSE train = 0.9967418562219608\n","iter = 230 , loss = 0.4967479723627896 , RMSE train = 0.9967418562219615\n","iter = 240 , loss = 0.496747534696004 , RMSE train = 0.9967418562219614\n","iter = 250 , loss = 0.496747333984513 , RMSE train = 0.9967418562219615\n","iter = 260 , loss = 0.4967472419392569 , RMSE train = 0.9967418562219615\n","iter = 270 , loss = 0.49674719972772496 , RMSE train = 0.9967418562219615\n","iter = 280 , loss = 0.4967471803696864 , RMSE train = 0.9967418562219615\n","iter = 290 , loss = 0.4967471714921568 , RMSE train = 0.9967418562219615\n","iter = 300 , loss = 0.4967471674209473 , RMSE train = 0.9967418562219615\n","iter = 310 , loss = 0.4967471655539 , RMSE train = 0.9967418562219615\n","iter = 320 , loss = 0.4967471646976753 , RMSE train = 0.9967418562219615\n","iter = 330 , loss = 0.49674716430501165 , RMSE train = 0.9967418562219615\n","iter = 340 , loss = 0.4967471641249363 , RMSE train = 0.9967418562219615\n","iter = 350 , loss = 0.4967471640423537 , RMSE train = 0.9967418562219615\n","\n","User-based MF, RMSE = 1.904790213058515\n","\n","\n","iter = 10 , loss = 24.217903870685298 , RMSE train = 1.6728772398991474\n","iter = 20 , loss = 11.161158892199149 , RMSE train = 1.2164652573785344\n","iter = 30 , loss = 5.491062281089196 , RMSE train = 1.193676020098838\n","iter = 40 , loss = 2.9036068677912503 , RMSE train = 1.1934986784974166\n","iter = 50 , loss = 1.717407581402903 , RMSE train = 1.1937076157795838\n","iter = 60 , loss = 1.1733966315770732 , RMSE train = 1.1937712108705305\n","iter = 70 , loss = 0.9239050762828755 , RMSE train = 1.1937873041774285\n","iter = 80 , loss = 0.8094872847378801 , RMSE train = 1.1937911992167909\n","iter = 90 , loss = 0.757015635634877 , RMSE train = 1.1937921233630877\n","iter = 100 , loss = 0.732952484080103 , RMSE train = 1.193792339775981\n","iter = 110 , loss = 0.7219173192871394 , RMSE train = 1.1937923899467109\n","iter = 120 , loss = 0.7168566879734694 , RMSE train = 1.1937924014828338\n","iter = 130 , loss = 0.7145359255471462 , RMSE train = 1.1937924041173564\n","iter = 140 , loss = 0.7134716426792002 , RMSE train = 1.1937924047155408\n","iter = 150 , loss = 0.7129835705481931 , RMSE train = 1.193792404850679\n","iter = 160 , loss = 0.712759744008543 , RMSE train = 1.1937924048810828\n","iter = 170 , loss = 0.7126570985295433 , RMSE train = 1.1937924048879007\n","iter = 180 , loss = 0.7126100258663304 , RMSE train = 1.1937924048894342\n","iter = 190 , loss = 0.7125884385612529 , RMSE train = 1.1937924048897692\n","iter = 200 , loss = 0.7125785387067108 , RMSE train = 1.1937924048898494\n","iter = 210 , loss = 0.7125739986640568 , RMSE train = 1.1937924048898714\n","iter = 220 , loss = 0.7125719166109766 , RMSE train = 1.193792404889875\n","iter = 230 , loss = 0.7125709617845887 , RMSE train = 1.1937924048898756\n","iter = 240 , loss = 0.712570523901887 , RMSE train = 1.1937924048898754\n","iter = 250 , loss = 0.7125703230888362 , RMSE train = 1.1937924048898754\n","iter = 260 , loss = 0.7125702309957859 , RMSE train = 1.1937924048898754\n","iter = 270 , loss = 0.7125701887617513 , RMSE train = 1.1937924048898754\n","iter = 280 , loss = 0.7125701693931134 , RMSE train = 1.1937924048898754\n","iter = 290 , loss = 0.7125701605105891 , RMSE train = 1.1937924048898754\n","iter = 300 , loss = 0.712570156437025 , RMSE train = 1.1937924048898754\n","iter = 310 , loss = 0.7125701545688674 , RMSE train = 1.1937924048898754\n","iter = 320 , loss = 0.712570153712119 , RMSE train = 1.1937924048898754\n","iter = 330 , loss = 0.7125701533192081 , RMSE train = 1.1937924048898754\n","iter = 340 , loss = 0.7125701531390162 , RMSE train = 1.1937924048898754\n","iter = 350 , loss = 0.7125701530563785 , RMSE train = 1.1937924048898754\n","\n","User-based MF, RMSE = 2.054653760470441\n","\n","\n","iter = 10 , loss = 24.198588682842654 , RMSE train = 1.6494409318602163\n","iter = 20 , loss = 11.148760753814923 , RMSE train = 1.1921331665997708\n","iter = 30 , loss = 5.469425012794867 , RMSE train = 1.1688141496726077\n","iter = 40 , loss = 2.8774868953490063 , RMSE train = 1.168381545564428\n","iter = 50 , loss = 1.6892915268286226 , RMSE train = 1.1685295219467855\n","iter = 60 , loss = 1.1443840128685183 , RMSE train = 1.1685797086954781\n","iter = 70 , loss = 0.8944859401694492 , RMSE train = 1.1685929009348583\n","iter = 80 , loss = 0.7798828827809189 , RMSE train = 1.1685961704015977\n","iter = 90 , loss = 0.7273265901711672 , RMSE train = 1.1685969597277923\n","iter = 100 , loss = 0.7032247217185791 , RMSE train = 1.1685971470836154\n","iter = 110 , loss = 0.6921718379865917 , RMSE train = 1.1685971909907553\n","iter = 120 , loss = 0.6871030957300512 , RMSE train = 1.1685972011764183\n","iter = 130 , loss = 0.6847786201989372 , RMSE train = 1.1685972035197183\n","iter = 140 , loss = 0.6837126375055322 , RMSE train = 1.1685972040551005\n","iter = 150 , loss = 0.6832237872347785 , RMSE train = 1.1685972041766994\n","iter = 160 , loss = 0.682999604498784 , RMSE train = 1.168597204204175\n","iter = 170 , loss = 0.6828967959791442 , RMSE train = 1.1685972042103605\n","iter = 180 , loss = 0.6828496486925942 , RMSE train = 1.1685972042117387\n","iter = 190 , loss = 0.6828280272349216 , RMSE train = 1.168597204212058\n","iter = 200 , loss = 0.6828181117509488 , RMSE train = 1.1685972042121227\n","iter = 210 , loss = 0.6828135645562444 , RMSE train = 1.1685972042121398\n","iter = 220 , loss = 0.6828114792306227 , RMSE train = 1.1685972042121437\n","iter = 230 , loss = 0.6828105229069421 , RMSE train = 1.1685972042121449\n","iter = 240 , loss = 0.6828100843392351 , RMSE train = 1.1685972042121449\n","iter = 250 , loss = 0.6828098832128227 , RMSE train = 1.1685972042121449\n","iter = 260 , loss = 0.6828097909764343 , RMSE train = 1.1685972042121449\n","iter = 270 , loss = 0.68280974867684 , RMSE train = 1.1685972042121449\n","iter = 280 , loss = 0.6828097292782188 , RMSE train = 1.1685972042121449\n","iter = 290 , loss = 0.6828097203819833 , RMSE train = 1.1685972042121449\n","iter = 300 , loss = 0.6828097163021498 , RMSE train = 1.1685972042121449\n","iter = 310 , loss = 0.6828097144311257 , RMSE train = 1.1685972042121449\n","iter = 320 , loss = 0.6828097135730667 , RMSE train = 1.1685972042121449\n","iter = 330 , loss = 0.6828097131795569 , RMSE train = 1.1685972042121449\n","iter = 340 , loss = 0.6828097129990911 , RMSE train = 1.1685972042121449\n","iter = 350 , loss = 0.6828097129163283 , RMSE train = 1.1685972042121449\n","\n","User-based MF, RMSE = 2.025752779827279\n","\n","\n","iter = 10 , loss = 24.147888985203046 , RMSE train = 1.6494893274681257\n","iter = 20 , loss = 11.047098940362398 , RMSE train = 1.1212721978980527\n","iter = 30 , loss = 5.3728891939558086 , RMSE train = 1.091288001970641\n","iter = 40 , loss = 2.785024089533101 , RMSE train = 1.0901310225742242\n","iter = 50 , loss = 1.5988978750834972 , RMSE train = 1.0901556704133986\n","iter = 60 , loss = 1.0549768133294235 , RMSE train = 1.0901808752033404\n","iter = 70 , loss = 0.8055395080967462 , RMSE train = 1.0901887846404121\n","iter = 80 , loss = 0.6911498092335191 , RMSE train = 1.090190926100938\n","iter = 90 , loss = 0.6386919201754624 , RMSE train = 1.090191473953422\n","iter = 100 , loss = 0.6146353534603846 , RMSE train = 1.0901916095593878\n","iter = 110 , loss = 0.6036033078859538 , RMSE train = 1.0901916423668894\n","iter = 120 , loss = 0.5985441475294134 , RMSE train = 1.0901916501699351\n","iter = 130 , loss = 0.5962240774542004 , RMSE train = 1.090191652001392\n","iter = 140 , loss = 0.595160120224962 , RMSE train = 1.090191652426737\n","iter = 150 , loss = 0.594672201224738 , RMSE train = 1.0901916525246393\n","iter = 160 , loss = 0.5944484466982468 , RMSE train = 1.0901916525470265\n","iter = 170 , loss = 0.5943458350904443 , RMSE train = 1.0901916525521045\n","iter = 180 , loss = 0.5942987783617388 , RMSE train = 1.0901916525532656\n","iter = 190 , loss = 0.5942771985546231 , RMSE train = 1.0901916525535262\n","iter = 200 , loss = 0.5942673022290236 , RMSE train = 1.0901916525535906\n","iter = 210 , loss = 0.5942627638476494 , RMSE train = 1.090191652553611\n","iter = 220 , loss = 0.5942606825767988 , RMSE train = 1.0901916525536126\n","iter = 230 , loss = 0.5942597281188106 , RMSE train = 1.0901916525536133\n","iter = 240 , loss = 0.594259290409646 , RMSE train = 1.0901916525536133\n","iter = 250 , loss = 0.5942590896783574 , RMSE train = 1.0901916525536133\n","iter = 260 , loss = 0.5942589976238369 , RMSE train = 1.0901916525536133\n","iter = 270 , loss = 0.5942589554079628 , RMSE train = 1.0901916525536133\n","iter = 280 , loss = 0.5942589360478859 , RMSE train = 1.0901916525536133\n","iter = 290 , loss = 0.5942589271693983 , RMSE train = 1.0901916525536133\n","iter = 300 , loss = 0.594258923097738 , RMSE train = 1.0901916525536133\n","iter = 310 , loss = 0.5942589212304783 , RMSE train = 1.0901916525536133\n","iter = 320 , loss = 0.5942589203741534 , RMSE train = 1.0901916525536133\n","iter = 330 , loss = 0.5942589199814424 , RMSE train = 1.0901916525536133\n","iter = 340 , loss = 0.5942589198013447 , RMSE train = 1.0901916525536133\n","iter = 350 , loss = 0.5942589197187516 , RMSE train = 1.0901916525536133\n","\n","User-based MF, RMSE = 1.9360100375142952\n","\n","\n"]}],"source":["M = int(max(tmp_based_copy[:,0])) + 1\n","N = int(max(tmp_based_copy[:,1])) + 1\n","pred_allUser = np.matrix(np.zeros((M,N)))\n","evaluate = np.matrix(np.zeros((M,N)))\n","for i in range(5):\n","    rs = MF(M, N, based_train[:,[0,1,i+3]], K = 50, lam = .1, print_every = 10, learning_rate = 0.75, max_iter = 350, user_based = 0)\n","    rs.fit()\n","    evaluate += np.matrix(rs.pred_all_usePred())\n","    pred_allUser += np.matrix(rs.pred_for_all_user())\n","    RMSE = rs.evaluate_RMSE(based_test[:,[0,1,i+3]])\n","    print ('\\nUser-based MF, RMSE =', RMSE)\n","    print('\\n')\n","\n","evaluate /= 5\n","pred_allUser /= 5\n","\n","pred_allUser = np.array(pred_allUser)\n","evaluate = np.array(evaluate)\n","# evaluate on test data\n","# RMSE = rs.evaluate_RMSE(based_test)\n","# print ('\\nUser-based MF, RMSE =', RMSE)"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":718,"status":"ok","timestamp":1583318041082,"user":{"displayName":"Ý Nguyễn Thị Như","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCwe9q5eGcI5GmOWVyzNvPeymwsZvqdGyTl7SB=s64","userId":"03745038202951496647"},"user_tz":-420},"id":"i2qeXCJsmsU6","outputId":"84ba8d42-268d-43a4-8873-5c0913cb2305"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","User-based MF-5models, RMSE = 2.0254322911240505\n"]}],"source":["# evalute \n","SE = 0\n","for i in range(len(based_test)):\n","    SE += (pred_allUser[int(based_test[i][0])][int(based_test[i][1])] - based_test[i, 2])**2 \n","\n","RMSE = np.sqrt(SE/len(based_test))\n","print ('\\nUser-based MF-5models, RMSE =', RMSE)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"NPQ1HgCJbZVd"},"outputs":[],"source":[""]}]}